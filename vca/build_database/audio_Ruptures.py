"""
éŸ³é¢‘å˜åŒ–ç‚¹æ£€æµ‹è„šæœ¬
ä½¿ç”¨Rupturesåº“æ£€æµ‹éŸ³é¢‘ä¸­çš„æ‰€æœ‰å˜åŒ–ç‚¹ï¼ˆchange pointsï¼‰
ä¸ä½¿ç”¨librosaï¼Œä½¿ç”¨audioreadå’Œsoundfile
"""

import sys
import os
import numpy as np
import ruptures as rpt
import argparse
from typing import List, Tuple
from scipy import signal as scipy_signal
from scipy.fft import rfft, rfftfreq
import matplotlib
matplotlib.use('Agg')  # ä½¿ç”¨éäº¤äº’å¼åç«¯
import matplotlib.pyplot as plt
from pathlib import Path

# é…ç½®ä¸­æ–‡å­—ä½“
def setup_chinese_font():
    """é…ç½®matplotlibçš„ä¸­æ–‡å­—ä½“"""
    import matplotlib.font_manager as fm

    # å¸¸è§çš„ä¸­æ–‡å­—ä½“åˆ—è¡¨
    chinese_fonts = [
        'SimHei',           # é»‘ä½“
        'Microsoft YaHei',  # å¾®è½¯é›…é»‘
        'WenQuanYi Micro Hei',  # æ–‡æ³‰é©¿å¾®ç±³é»‘ (Linuxå¸¸è§)
        'Noto Sans CJK SC',     # æ€æºé»‘ä½“ç®€ä½“
        'AR PL UMing CN',       # æ–‡é¼PLç®€ä¸­æ˜ä½“
        'STSong',               # åæ–‡å®‹ä½“
        'STHeiti',              # åæ–‡é»‘ä½“
    ]

    # è·å–ç³»ç»Ÿæ‰€æœ‰å­—ä½“
    available_fonts = [f.name for f in fm.fontManager.ttflist]

    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨çš„ä¸­æ–‡å­—ä½“
    for font in chinese_fonts:
        if font in available_fonts:
            plt.rcParams['font.sans-serif'] = [font, 'DejaVu Sans']
            plt.rcParams['axes.unicode_minus'] = False
            return font

    # å¦‚æœæ‰¾ä¸åˆ°ä¸­æ–‡å­—ä½“ï¼Œä½¿ç”¨è‹±æ–‡æ ‡ç­¾
    print("âš ï¸  æœªæ‰¾åˆ°ä¸­æ–‡å­—ä½“ï¼Œå°†ä½¿ç”¨è‹±æ–‡æ ‡ç­¾")
    return None

setup_chinese_font()

# æ·»åŠ vcaæ¨¡å—åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))
from vca.audio_utils import load_audio_no_librosa


def load_audio(audio_path: str, sr: int = 16000) -> Tuple[np.ndarray, int]:
    """
    åŠ è½½éŸ³é¢‘æ–‡ä»¶ï¼ˆä½¿ç”¨audioread/soundfileï¼Œä¸ä½¿ç”¨librosaï¼‰

    Args:
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        sr: é‡‡æ ·ç‡ï¼Œé»˜è®¤16000Hz

    Returns:
        audio: éŸ³é¢‘æ•°æ®
        sr: é‡‡æ ·ç‡
    """
    audio = load_audio_no_librosa(audio_path, sr=sr)
    return audio, sr


def compute_rms(audio: np.ndarray, frame_length: int = 2048, hop_length: int = 512) -> np.ndarray:
    """
    è®¡ç®—RMSèƒ½é‡

    Args:
        audio: éŸ³é¢‘æ•°æ®
        frame_length: å¸§é•¿åº¦
        hop_length: å¸§ç§»

    Returns:
        rms: RMSèƒ½é‡ (1, n_frames)
    """
    # åˆ†å¸§
    n_frames = 1 + (len(audio) - frame_length) // hop_length
    frames = np.lib.stride_tricks.as_strided(
        audio,
        shape=(frame_length, n_frames),
        strides=(audio.itemsize, hop_length * audio.itemsize)
    )

    # è®¡ç®—RMS
    rms = np.sqrt(np.mean(frames ** 2, axis=0))
    return rms.reshape(1, -1)


def compute_zcr(audio: np.ndarray, frame_length: int = 2048, hop_length: int = 512) -> np.ndarray:
    """
    è®¡ç®—è¿‡é›¶ç‡ï¼ˆZero Crossing Rateï¼‰

    Args:
        audio: éŸ³é¢‘æ•°æ®
        frame_length: å¸§é•¿åº¦
        hop_length: å¸§ç§»

    Returns:
        zcr: è¿‡é›¶ç‡ (1, n_frames)
    """
    # åˆ†å¸§
    n_frames = 1 + (len(audio) - frame_length) // hop_length
    frames = np.lib.stride_tricks.as_strided(
        audio,
        shape=(frame_length, n_frames),
        strides=(audio.itemsize, hop_length * audio.itemsize)
    )

    # è®¡ç®—è¿‡é›¶ç‡
    zcr = np.mean(np.abs(np.diff(np.sign(frames), axis=0)), axis=0) / 2
    return zcr.reshape(1, -1)


def compute_spectral_features(audio: np.ndarray, sr: int = 16000,
                              frame_length: int = 2048, hop_length: int = 512) -> np.ndarray:
    """
    è®¡ç®—é¢‘è°±ç‰¹å¾ï¼ˆé¢‘è°±è´¨å¿ƒã€é¢‘è°±æ»šé™ï¼‰

    Args:
        audio: éŸ³é¢‘æ•°æ®
        sr: é‡‡æ ·ç‡
        frame_length: å¸§é•¿åº¦
        hop_length: å¸§ç§»

    Returns:
        features: é¢‘è°±ç‰¹å¾ (2, n_frames)
    """
    # åˆ†å¸§
    n_frames = 1 + (len(audio) - frame_length) // hop_length
    frames = np.lib.stride_tricks.as_strided(
        audio,
        shape=(frame_length, n_frames),
        strides=(audio.itemsize, hop_length * audio.itemsize)
    )

    # åŠ çª—
    window = scipy_signal.get_window('hann', frame_length)
    frames = frames * window[:, np.newaxis]

    # è®¡ç®—FFT
    fft = np.abs(rfft(frames, axis=0))
    freqs = rfftfreq(frame_length, 1/sr)

    # é¢‘è°±è´¨å¿ƒ
    spectral_centroid = np.sum(freqs[:, np.newaxis] * fft, axis=0) / (np.sum(fft, axis=0) + 1e-10)

    # é¢‘è°±æ»šé™ï¼ˆ85%èƒ½é‡ç‚¹ï¼‰
    cumsum = np.cumsum(fft, axis=0)
    total = cumsum[-1, :]
    threshold = 0.85 * total
    spectral_rolloff = np.zeros(n_frames)
    for i in range(n_frames):
        idx = np.where(cumsum[:, i] >= threshold[i])[0]
        if len(idx) > 0:
            spectral_rolloff[i] = freqs[idx[0]]
        else:
            spectral_rolloff[i] = freqs[-1]

    return np.vstack([spectral_centroid, spectral_rolloff])


def compute_mfcc_simple(audio: np.ndarray, sr: int = 16000, n_mfcc: int = 5,
                       frame_length: int = 2048, hop_length: int = 512) -> np.ndarray:
    """
    è®¡ç®—ç®€åŒ–çš„MFCCç‰¹å¾ï¼ˆä½¿ç”¨melæ»¤æ³¢å™¨ç»„ï¼‰

    Args:
        audio: éŸ³é¢‘æ•°æ®
        sr: é‡‡æ ·ç‡
        n_mfcc: MFCCç³»æ•°æ•°é‡
        frame_length: å¸§é•¿åº¦
        hop_length: å¸§ç§»

    Returns:
        mfcc: MFCCç‰¹å¾ (n_mfcc, n_frames)
    """
    # åˆ†å¸§
    n_frames = 1 + (len(audio) - frame_length) // hop_length
    frames = np.lib.stride_tricks.as_strided(
        audio,
        shape=(frame_length, n_frames),
        strides=(audio.itemsize, hop_length * audio.itemsize)
    )

    # åŠ çª—
    window = scipy_signal.get_window('hann', frame_length)
    frames = frames * window[:, np.newaxis]

    # è®¡ç®—åŠŸç‡è°±
    fft = np.abs(rfft(frames, axis=0)) ** 2

    # åˆ›å»ºMelæ»¤æ³¢å™¨ç»„ï¼ˆç®€åŒ–ç‰ˆï¼‰
    n_fft = frame_length // 2 + 1
    n_mels = 40
    mel_filters = create_mel_filterbank(n_fft, n_mels, sr)

    # åº”ç”¨Melæ»¤æ³¢å™¨
    mel_spec = np.dot(mel_filters, fft)
    mel_spec = np.where(mel_spec == 0, np.finfo(float).eps, mel_spec)

    # å¯¹æ•°
    log_mel_spec = np.log(mel_spec)

    # DCTå˜æ¢å¾—åˆ°MFCC
    from scipy.fftpack import dct
    mfcc = dct(log_mel_spec, axis=0, norm='ortho')[:n_mfcc, :]

    return mfcc


def create_mel_filterbank(n_fft: int, n_mels: int = 40, sr: int = 16000) -> np.ndarray:
    """
    åˆ›å»ºMelæ»¤æ³¢å™¨ç»„

    Args:
        n_fft: FFTç‚¹æ•°
        n_mels: Melæ»¤æ³¢å™¨æ•°é‡
        sr: é‡‡æ ·ç‡

    Returns:
        mel_filters: Melæ»¤æ³¢å™¨ç»„ (n_mels, n_fft)
    """
    def hz_to_mel(hz):
        return 2595 * np.log10(1 + hz / 700)

    def mel_to_hz(mel):
        return 700 * (10 ** (mel / 2595) - 1)

    # Melé¢‘ç‡èŒƒå›´
    mel_min = hz_to_mel(0)
    mel_max = hz_to_mel(sr / 2)
    mel_points = np.linspace(mel_min, mel_max, n_mels + 2)
    hz_points = mel_to_hz(mel_points)

    # è½¬æ¢ä¸ºFFT binç´¢å¼•
    bin_points = np.floor((n_fft + 1) * hz_points / sr).astype(int)

    # åˆ›å»ºæ»¤æ³¢å™¨
    filters = np.zeros((n_mels, n_fft))
    for i in range(n_mels):
        left = bin_points[i]
        center = bin_points[i + 1]
        right = bin_points[i + 2]

        # ä¸Šå‡æ–œå¡
        for j in range(left, center):
            filters[i, j] = (j - left) / (center - left)

        # ä¸‹é™æ–œå¡
        for j in range(center, right):
            filters[i, j] = (right - j) / (right - center)

    return filters


def extract_features(audio: np.ndarray, sr: int, feature_type: str = 'rms',
                     hop_length: int = 512) -> np.ndarray:
    """
    æå–éŸ³é¢‘ç‰¹å¾ï¼ˆä¸ä½¿ç”¨librosaï¼‰

    Args:
        audio: éŸ³é¢‘æ•°æ®
        sr: é‡‡æ ·ç‡
        feature_type: ç‰¹å¾ç±»å‹ï¼Œå¯é€‰ 'rms', 'zcr', 'spectral', 'mfcc'
        hop_length: å¸§ç§»ï¼Œå€¼è¶Šå¤§é€Ÿåº¦è¶Šå¿«

    Returns:
        features: ç‰¹å¾çŸ©é˜µ (n_features, n_frames)
    """
    frame_length = hop_length * 2  # é»˜è®¤å¸§é•¿åº¦ä¸ºhop_lengthçš„2å€

    if feature_type == 'rms':
        # RMSèƒ½é‡ï¼ˆæœ€å¿«ï¼‰
        features = compute_rms(audio, frame_length=frame_length, hop_length=hop_length)
    elif feature_type == 'zcr':
        # è¿‡é›¶ç‡
        features = compute_zcr(audio, frame_length=frame_length, hop_length=hop_length)
    elif feature_type == 'spectral':
        # é¢‘è°±ç‰¹å¾
        features = compute_spectral_features(audio, sr=sr, frame_length=frame_length, hop_length=hop_length)
    elif feature_type == 'mfcc':
        # MFCCç‰¹å¾
        features = compute_mfcc_simple(audio, sr=sr, n_mfcc=5, frame_length=frame_length, hop_length=hop_length)
    elif feature_type == 'combined':
        # ç»„åˆç‰¹å¾ï¼ˆRMS + ZCRï¼‰
        rms = compute_rms(audio, frame_length=frame_length, hop_length=hop_length)
        zcr = compute_zcr(audio, frame_length=frame_length, hop_length=hop_length)
        features = np.vstack([rms, zcr])
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç‰¹å¾ç±»å‹: {feature_type}")

    return features


def detect_change_points(
    features: np.ndarray,
    method: str = 'Pelt',
    model: str = 'l2',
    min_size: int = 10,
    jump: int = 10,
    pen: float = None
) -> Tuple[List[int], float]:
    """
    ä½¿ç”¨Rupturesæ£€æµ‹å˜åŒ–ç‚¹

    Args:
        features: ç‰¹å¾çŸ©é˜µ (n_features, n_frames)
        method: æ£€æµ‹æ–¹æ³•ï¼Œå¯é€‰ 'Pelt', 'Binseg', 'BottomUp', 'Window'
        model: æŸå¤±æ¨¡å‹ï¼Œå¯é€‰ 'l1', 'l2', 'rbf', 'linear', 'normal', 'ar'
               æ³¨æ„ï¼š'l2'æœ€å¿«ï¼Œ'rbf'æœ€æ…¢ä½†å¯èƒ½æ›´å‡†ç¡®
        min_size: ä¸¤ä¸ªå˜åŒ–ç‚¹ä¹‹é—´çš„æœ€å°è·ç¦»
        jump: è·³è·ƒæ­¥é•¿ï¼Œç”¨äºåŠ é€Ÿè®¡ç®—ï¼ˆå€¼è¶Šå¤§è¶Šå¿«ï¼Œä½†ç²¾åº¦ç•¥é™ï¼‰
        pen: æƒ©ç½šå€¼ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ä¼°è®¡

    Returns:
        change_points: å˜åŒ–ç‚¹åˆ—è¡¨ï¼ˆå¸§ç´¢å¼•ï¼‰
        pen: å®é™…ä½¿ç”¨çš„æƒ©ç½šå€¼
    """
    # è½¬ç½®ç‰¹å¾çŸ©é˜µï¼Œruptureséœ€è¦ (n_frames, n_features) æ ¼å¼
    signal = features.T

    print(f"  ä¿¡å·å½¢çŠ¶: {signal.shape} (å¸§æ•° x ç‰¹å¾ç»´åº¦)")

    # é€‰æ‹©æ£€æµ‹ç®—æ³•
    if method == 'Pelt':
        algo = rpt.Pelt(model=model, min_size=min_size, jump=jump)
    elif method == 'Binseg':
        algo = rpt.Binseg(model=model, min_size=min_size, jump=jump)
    elif method == 'BottomUp':
        algo = rpt.BottomUp(model=model, min_size=min_size, jump=jump)
    elif method == 'Window':
        algo = rpt.Window(width=40, model=model, min_size=min_size, jump=jump)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ£€æµ‹æ–¹æ³•: {method}")

    # æ‹Ÿåˆæ•°æ®
    print(f"  æ­£åœ¨æ‹Ÿåˆæ•°æ®...")
    algo.fit(signal)

    # å¦‚æœæ²¡æœ‰æŒ‡å®šæƒ©ç½šå€¼ï¼Œè‡ªåŠ¨ä¼°è®¡
    if pen is None:
        # ä½¿ç”¨æ›´å°çš„æƒ©ç½šå€¼ä»¥æ£€æµ‹æ›´å¤šå˜åŒ–ç‚¹
        # åŸå§‹å…¬å¼: np.log(n_samples) * n_features
        # è°ƒæ•´ä¸ºæ›´æ•æ„Ÿçš„ç‰ˆæœ¬
        base_pen = np.log(signal.shape[0]) * signal.shape[1]
        pen = base_pen * 0.3  # é™ä½åˆ°30%ï¼Œå¢åŠ æ•æ„Ÿåº¦

    print(f"  ä½¿ç”¨æƒ©ç½šå€¼: {pen:.2f} (å€¼è¶Šå°æ£€æµ‹åˆ°çš„å˜åŒ–ç‚¹è¶Šå¤š)")

    # é¢„æµ‹å˜åŒ–ç‚¹
    print(f"  æ­£åœ¨é¢„æµ‹å˜åŒ–ç‚¹...")
    change_points = algo.predict(pen=pen)

    # ç§»é™¤æœ€åä¸€ä¸ªç‚¹ï¼ˆæ€»æ˜¯æ•°æ®çš„æœ«å°¾ï¼‰
    if change_points and change_points[-1] == signal.shape[0]:
        change_points = change_points[:-1]

    return change_points, pen


def frames_to_time(frames: List[int], sr: int, hop_length: int = 512) -> List[float]:
    """
    å°†å¸§ç´¢å¼•è½¬æ¢ä¸ºæ—¶é—´ï¼ˆç§’ï¼‰

    Args:
        frames: å¸§ç´¢å¼•åˆ—è¡¨
        sr: é‡‡æ ·ç‡
        hop_length: å¸§ç§»

    Returns:
        times: æ—¶é—´åˆ—è¡¨ï¼ˆç§’ï¼‰
    """
    times = [frame * hop_length / sr for frame in frames]
    return times


def detect_audio_change_points(
    audio_path: str,
    sr: int = 16000,
    feature_type: str = 'combined',
    method: str = 'Pelt',
    model: str = 'l2',
    min_size: int = 10,
    jump: int = 10,
    pen: float = None,
    hop_length: int = 1024
) -> Tuple[List[int], List[float], float]:
    """
    æ£€æµ‹éŸ³é¢‘æ–‡ä»¶ä¸­çš„æ‰€æœ‰å˜åŒ–ç‚¹

    Args:
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        sr: é‡‡æ ·ç‡ï¼Œé™ä½å¯åŠ é€Ÿï¼ˆå¦‚8000ï¼‰
        feature_type: ç‰¹å¾ç±»å‹
            - 'combined': RMS+ZCRç»„åˆï¼ˆé»˜è®¤ï¼Œå¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼‰
            - 'rms': ä»…RMSèƒ½é‡ï¼ˆæœ€å¿«ä½†å¯èƒ½ä¸å¤Ÿæ•æ„Ÿï¼‰
            - 'mfcc': MFCCç‰¹å¾ï¼ˆæ›´å‡†ç¡®ä½†è¾ƒæ…¢ï¼‰
            - 'spectral': é¢‘è°±ç‰¹å¾
            - 'zcr': è¿‡é›¶ç‡
        method: æ£€æµ‹æ–¹æ³•
        model: æŸå¤±æ¨¡å‹ï¼Œ'l2'æœ€å¿«ï¼Œ'rbf'æœ€æ…¢ä½†å¯èƒ½æ›´å‡†ç¡®
        min_size: æœ€å°æ®µé•¿åº¦ï¼ˆå¸§æ•°ï¼‰
        jump: è·³è·ƒæ­¥é•¿ï¼Œå€¼è¶Šå¤§è¶Šå¿«
        pen: æƒ©ç½šå€¼ï¼Œå€¼è¶Šå¤§æ£€æµ‹åˆ°çš„å˜åŒ–ç‚¹è¶Šå°‘ï¼ˆé»˜è®¤è‡ªåŠ¨è®¡ç®—ï¼‰
        hop_length: å¸§ç§»ï¼Œå€¼è¶Šå¤§é€Ÿåº¦è¶Šå¿«ä½†æ—¶é—´ç²¾åº¦é™ä½

    Returns:
        change_points_frames: å˜åŒ–ç‚¹å¸§ç´¢å¼•åˆ—è¡¨
        change_points_times: å˜åŒ–ç‚¹æ—¶é—´åˆ—è¡¨ï¼ˆç§’ï¼‰
        actual_pen: å®é™…ä½¿ç”¨çš„æƒ©ç½šå€¼
    """
    print(f"æ­£åœ¨åŠ è½½éŸ³é¢‘: {audio_path}")
    audio, sr = load_audio(audio_path, sr)
    print(f"  éŸ³é¢‘æ—¶é•¿: {len(audio)/sr:.2f}ç§’")

    print(f"æ­£åœ¨æå–ç‰¹å¾: {feature_type} (hop_length={hop_length})")
    features = extract_features(audio, sr, feature_type, hop_length=hop_length)
    print(f"  ç‰¹å¾å½¢çŠ¶: {features.shape}")

    print(f"æ­£åœ¨æ£€æµ‹å˜åŒ–ç‚¹: method={method}, model={model}, jump={jump}")
    change_points_frames, actual_pen = detect_change_points(
        features, method=method, model=model,
        min_size=min_size, jump=jump, pen=pen
    )

    print(f"æ­£åœ¨è½¬æ¢ä¸ºæ—¶é—´...")
    change_points_times = frames_to_time(change_points_frames, sr, hop_length)

    return change_points_frames, change_points_times, actual_pen


def generate_color_video(
    audio_path: str,
    change_points_times: List[float],
    output_path: str = None,
    fps: int = 30,
    resolution: Tuple[int, int] = (1920, 1080),
    color_palette: List[Tuple[int, int, int]] = None,
    params_info: dict = None
):
    """
    ç”Ÿæˆä¸€ä¸ªå¸¦æœ‰çº¯è‰²ç”»é¢çš„è§†é¢‘ï¼Œå½“é‡åˆ°åˆ†å‰²ç‚¹æ—¶åˆ‡æ¢é¢œè‰²

    Args:
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
        change_points_times: å˜åŒ–ç‚¹æ—¶é—´åˆ—è¡¨ï¼ˆç§’ï¼‰
        output_path: è¾“å‡ºè§†é¢‘è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
        fps: è§†é¢‘å¸§ç‡ï¼Œé»˜è®¤30
        resolution: è§†é¢‘åˆ†è¾¨ç‡ (å®½, é«˜)ï¼Œé»˜è®¤1920x1080
        color_palette: é¢œè‰²åˆ—è¡¨ï¼Œæ¯ä¸ªé¢œè‰²ä¸ºRGBå…ƒç»„ã€‚å¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è°ƒè‰²æ¿
        params_info: å‚æ•°ä¿¡æ¯å­—å…¸ï¼Œç”¨äºåœ¨è§†é¢‘ä¸­æ˜¾ç¤º
    """
    try:
        import subprocess
        import tempfile
        import shutil
    except ImportError as e:
        print(f"âŒ ç¼ºå°‘å¿…è¦çš„åº“: {e}")
        return

    # æ£€æŸ¥ffmpegæ˜¯å¦å¯ç”¨
    if shutil.which('ffmpeg') is None:
        print("âŒ æœªæ‰¾åˆ°ffmpegï¼Œè¯·å®‰è£…ffmpegåé‡è¯•")
        return

    # é»˜è®¤é¢œè‰²è°ƒè‰²æ¿ï¼ˆæŸ”å’Œçš„é¢œè‰²ï¼‰
    if color_palette is None:
        color_palette = [
            (70, 130, 180),   # Steel Blue
            (255, 182, 193),  # Light Pink
            (144, 238, 144),  # Light Green
            (255, 218, 185),  # Peach
            (221, 160, 221),  # Plum
            (135, 206, 250),  # Light Sky Blue
            (255, 255, 224),  # Light Yellow
            (176, 224, 230),  # Powder Blue
            (255, 228, 225),  # Misty Rose
            (240, 230, 140),  # Khaki
            (173, 216, 230),  # Light Blue
            (255, 160, 122),  # Light Salmon
            (152, 251, 152),  # Pale Green
            (238, 130, 238),  # Violet
            (250, 250, 210),  # Light Goldenrod Yellow
        ]

    # è·å–éŸ³é¢‘æ—¶é•¿
    audio, sr = load_audio(audio_path, sr=16000)
    duration = len(audio) / sr

    # æ„å»ºæ—¶é—´æ®µåˆ—è¡¨
    segments = []
    prev_time = 0.0
    for cp_time in change_points_times:
        if cp_time > prev_time:
            segments.append((prev_time, cp_time))
        prev_time = cp_time
    # æ·»åŠ æœ€åä¸€ä¸ªæ®µ
    if prev_time < duration:
        segments.append((prev_time, duration))

    # å¦‚æœæ²¡æœ‰åˆ†å‰²ç‚¹ï¼Œæ•´ä¸ªè§†é¢‘ä½¿ç”¨ä¸€ä¸ªé¢œè‰²
    if not segments:
        segments = [(0.0, duration)]

    print(f"\nğŸ“Š è§†é¢‘æ®µè½ä¿¡æ¯:")
    print(f"  æ€»æ—¶é•¿: {duration:.2f}ç§’")
    print(f"  æ®µè½æ•°: {len(segments)}")

    # ç¡®å®šè¾“å‡ºè·¯å¾„
    if output_path is None:
        audio_file = Path(audio_path)
        output_path = audio_file.parent / f"{audio_file.stem}_color_video.mp4"
    output_path = str(output_path)

    # åˆ›å»ºä¸´æ—¶ç›®å½•
    with tempfile.TemporaryDirectory() as temp_dir:
        segment_files = []

        # ä¸ºæ¯ä¸ªæ®µè½ç”Ÿæˆè§†é¢‘ç‰‡æ®µ
        for i, (start_time, end_time) in enumerate(segments):
            color = color_palette[i % len(color_palette)]
            segment_duration = end_time - start_time

            # è½¬æ¢RGBä¸ºåå…­è¿›åˆ¶
            color_hex = '#{:02x}{:02x}{:02x}'.format(*color)

            segment_file = os.path.join(temp_dir, f"segment_{i:04d}.mp4")
            segment_files.append(segment_file)

            print(f"  æ®µè½ {i+1}: {start_time:.2f}s - {end_time:.2f}s (é¢œè‰²: {color_hex})")

            # æ„å»ºå‚æ•°æ˜¾ç¤ºæ–‡æœ¬
            if params_info:
                # æ ¼å¼åŒ–å‚æ•°ä¿¡æ¯
                param_lines = []
                param_lines.append(f"Feature: {params_info.get('feature', 'N/A')}")
                param_lines.append(f"Method: {params_info.get('method', 'N/A')}")
                param_lines.append(f"Model: {params_info.get('model', 'N/A')}")
                param_lines.append(f"Hop Length: {params_info.get('hop_length', 'N/A')}")
                param_lines.append(f"Jump: {params_info.get('jump', 'N/A')}")
                param_lines.append(f"Penalty: {params_info.get('pen', 'auto')}")
                param_lines.append(f"Min Size: {params_info.get('min_size', 'N/A')}")
                param_lines.append(f"Change Points: {params_info.get('n_change_points', 'N/A')}")
                param_lines.append(f"Segment: {i+1}/{len(segments)}")
                param_lines.append(f"Time: {start_time:.2f}s - {end_time:.2f}s")

                # æ ¹æ®èƒŒæ™¯é¢œè‰²é€‰æ‹©æ–‡å­—é¢œè‰²ï¼ˆæ·±è‰²èƒŒæ™¯ç”¨ç™½è‰²ï¼Œæµ…è‰²èƒŒæ™¯ç”¨é»‘è‰²ï¼‰
                brightness = (color[0] * 299 + color[1] * 587 + color[2] * 114) / 1000
                text_color = 'white' if brightness < 128 else 'black'
                shadow_color = 'black@0.5' if brightness < 128 else 'white@0.5'

                # è®¡ç®—å­—ä½“å¤§å°ï¼ˆæ ¹æ®åˆ†è¾¨ç‡è°ƒæ•´ï¼‰
                font_size = max(16, resolution[1] // 30)
                line_height = font_size + 8

                # æŸ¥æ‰¾å¯ç”¨çš„å­—ä½“æ–‡ä»¶
                font_paths = [
                    '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',
                    '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf',
                    '/usr/share/fonts/TTF/DejaVuSans.ttf',
                    '/usr/share/fonts/dejavu/DejaVuSans.ttf',
                    '/usr/share/fonts/truetype/freefont/FreeSans.ttf',
                    '/System/Library/Fonts/Helvetica.ttc',  # macOS
                    'C:/Windows/Fonts/arial.ttf',  # Windows
                ]

                font_file = None
                for fp in font_paths:
                    if os.path.exists(fp):
                        font_file = fp
                        break

                if font_file:
                    # æ„å»ºå¤šè¡Œæ–‡å­—æ»¤é•œï¼ˆä½¿ç”¨å­—ä½“æ–‡ä»¶ï¼‰
                    drawtext_filters = []
                    for idx, line in enumerate(param_lines):
                        # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
                        escaped_line = line.replace("'", "\\'").replace(":", "\\:")
                        y_pos = 20 + idx * line_height
                        drawtext_filters.append(
                            f"drawtext=text='{escaped_line}':fontfile='{font_file}':fontsize={font_size}:"
                            f"fontcolor={text_color}:x=20:y={y_pos}:shadowcolor={shadow_color}:shadowx=2:shadowy=2"
                        )

                    filter_chain = ','.join(drawtext_filters)

                    # ä½¿ç”¨ffmpegç”Ÿæˆçº¯è‰²è§†é¢‘æ®µï¼ˆå¸¦æ–‡å­—ï¼‰
                    cmd = [
                        'ffmpeg', '-y',
                        '-f', 'lavfi',
                        '-i', f'color=c={color_hex}:s={resolution[0]}x{resolution[1]}:r={fps}:d={segment_duration}',
                        '-vf', filter_chain,
                        '-c:v', 'libx264',
                        '-pix_fmt', 'yuv420p',
                        '-t', str(segment_duration),
                        segment_file
                    ]
                else:
                    # æ²¡æœ‰æ‰¾åˆ°å­—ä½“æ–‡ä»¶ï¼Œä¸æ·»åŠ æ–‡å­—
                    print(f"  âš ï¸  æœªæ‰¾åˆ°å­—ä½“æ–‡ä»¶ï¼Œè§†é¢‘å°†ä¸æ˜¾ç¤ºæ–‡å­—")
                    cmd = [
                        'ffmpeg', '-y',
                        '-f', 'lavfi',
                        '-i', f'color=c={color_hex}:s={resolution[0]}x{resolution[1]}:r={fps}:d={segment_duration}',
                        '-c:v', 'libx264',
                        '-pix_fmt', 'yuv420p',
                        '-t', str(segment_duration),
                        segment_file
                    ]
            else:
                # ä½¿ç”¨ffmpegç”Ÿæˆçº¯è‰²è§†é¢‘æ®µï¼ˆä¸å¸¦æ–‡å­—ï¼‰
                cmd = [
                    'ffmpeg', '-y',
                    '-f', 'lavfi',
                    '-i', f'color=c={color_hex}:s={resolution[0]}x{resolution[1]}:r={fps}:d={segment_duration}',
                    '-c:v', 'libx264',
                    '-pix_fmt', 'yuv420p',
                    '-t', str(segment_duration),
                    segment_file
                ]

            result = subprocess.run(cmd, capture_output=True, text=True)
            if result.returncode != 0:
                print(f"âŒ ç”Ÿæˆæ®µè½ {i+1} å¤±è´¥: {result.stderr}")
                return

        # åˆ›å»ºæ–‡ä»¶åˆ—è¡¨ç”¨äºconcat
        concat_list_file = os.path.join(temp_dir, 'concat_list.txt')
        with open(concat_list_file, 'w') as f:
            for segment_file in segment_files:
                f.write(f"file '{segment_file}'\n")

        # åˆå¹¶æ‰€æœ‰è§†é¢‘æ®µï¼ˆæ— éŸ³é¢‘ï¼‰
        temp_video = os.path.join(temp_dir, 'temp_video.mp4')
        cmd = [
            'ffmpeg', '-y',
            '-f', 'concat',
            '-safe', '0',
            '-i', concat_list_file,
            '-c', 'copy',
            temp_video
        ]

        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"âŒ åˆå¹¶è§†é¢‘æ®µå¤±è´¥: {result.stderr}")
            return

        # æ·»åŠ éŸ³é¢‘
        print(f"\nğŸµ æ­£åœ¨æ·»åŠ éŸ³é¢‘...")
        cmd = [
            'ffmpeg', '-y',
            '-i', temp_video,
            '-i', audio_path,
            '-c:v', 'copy',
            '-c:a', 'aac',
            '-b:a', '192k',
            '-shortest',
            output_path
        ]

        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            print(f"âŒ æ·»åŠ éŸ³é¢‘å¤±è´¥: {result.stderr}")
            return

    print(f"\nâœ… è§†é¢‘å·²ä¿å­˜åˆ°: {output_path}")


def visualize_change_points(
    audio: np.ndarray,
    sr: int,
    change_points_times: List[float],
    features: np.ndarray = None,
    feature_type: str = 'rms',
    hop_length: int = 1024,
    output_path: str = None
):
    """
    å¯è§†åŒ–éŸ³é¢‘å’Œæ£€æµ‹åˆ°çš„å˜åŒ–ç‚¹

    Args:
        audio: éŸ³é¢‘æ•°æ®
        sr: é‡‡æ ·ç‡
        change_points_times: å˜åŒ–ç‚¹æ—¶é—´åˆ—è¡¨
        features: æå–çš„ç‰¹å¾ï¼ˆå¯é€‰ï¼‰
        feature_type: ç‰¹å¾ç±»å‹
        hop_length: å¸§ç§»
        output_path: è¾“å‡ºå›¾ç‰‡è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
    """
    # æ ‡ç­¾ï¼ˆä¸­è‹±æ–‡ï¼‰
    labels = {
        'waveform_title': 'Audio Waveform and Change Points',
        'amplitude': 'Amplitude',
        'spectrogram_title': 'Spectrogram',
        'frequency': 'Frequency (Hz)',
        'power': 'Power (dB)',
        'features_title': f'Audio Features ({feature_type})',
        'normalized_feature': 'Normalized Feature Value',
        'time': 'Time (s)',
        'change_point': 'Change Point',
        'no_features': 'No features provided',
        'feature': 'Feature',
    }

    # åˆ›å»ºå›¾å½¢
    fig = plt.figure(figsize=(16, 10))

    # æ—¶é—´è½´
    duration = len(audio) / sr
    time_audio = np.linspace(0, duration, len(audio))

    # 1. ç»˜åˆ¶éŸ³é¢‘æ³¢å½¢
    ax1 = plt.subplot(3, 1, 1)
    ax1.plot(time_audio, audio, color='steelblue', linewidth=0.5, alpha=0.7)
    ax1.set_ylabel(labels['amplitude'], fontsize=12)
    ax1.set_title(labels['waveform_title'], fontsize=14, fontweight='bold')
    ax1.grid(True, alpha=0.3)
    ax1.set_xlim(0, duration)

    # æ ‡è®°å˜åŒ–ç‚¹
    for cp_time in change_points_times:
        ax1.axvline(x=cp_time, color='red', linestyle='--', linewidth=2, alpha=0.8,
                   label=labels['change_point'] if cp_time == change_points_times[0] else '')

    if change_points_times:
        ax1.legend(loc='upper right', fontsize=10)

    # 2. ç»˜åˆ¶æ—¶é¢‘å›¾ï¼ˆSpectrogramï¼‰
    ax2 = plt.subplot(3, 1, 2)

    # è®¡ç®—STFT
    nperseg = 2048
    noverlap = nperseg // 2
    f, t, Sxx = scipy_signal.spectrogram(audio, sr, nperseg=nperseg, noverlap=noverlap)

    # è½¬æ¢ä¸ºdB
    Sxx_db = 10 * np.log10(Sxx + 1e-10)

    # ç»˜åˆ¶æ—¶é¢‘å›¾
    im = ax2.pcolormesh(t, f, Sxx_db, shading='gouraud', cmap='viridis')
    ax2.set_ylabel(labels['frequency'], fontsize=12)
    ax2.set_title(labels['spectrogram_title'], fontsize=14, fontweight='bold')
    ax2.set_ylim(0, min(8000, sr/2))  # åªæ˜¾ç¤ºåˆ°8kHz

    # æ·»åŠ colorbar
    cbar = plt.colorbar(im, ax=ax2)
    cbar.set_label(labels['power'], fontsize=10)

    # æ ‡è®°å˜åŒ–ç‚¹
    for cp_time in change_points_times:
        ax2.axvline(x=cp_time, color='red', linestyle='--', linewidth=2, alpha=0.8)

    # 3. ç»˜åˆ¶ç‰¹å¾å›¾
    ax3 = plt.subplot(3, 1, 3)

    if features is not None:
        # ç‰¹å¾çš„æ—¶é—´è½´
        n_frames = features.shape[1]
        time_features = np.arange(n_frames) * hop_length / sr

        # ç»˜åˆ¶æ¯ä¸ªç‰¹å¾ç»´åº¦ï¼ˆä½¿ç”¨è‹±æ–‡æ ‡ç­¾ï¼‰
        feature_names = {
            'rms': ['RMS Energy'],
            'zcr': ['Zero Crossing Rate'],
            'combined': ['RMS Energy', 'Zero Crossing Rate'],
            'spectral': ['Spectral Centroid', 'Spectral Rolloff'],
            'mfcc': [f'MFCC-{i+1}' for i in range(features.shape[0])]
        }

        names = feature_names.get(feature_type, [f'Feature-{i+1}' for i in range(features.shape[0])])
        colors = plt.cm.tab10(np.linspace(0, 1, features.shape[0]))

        for i in range(features.shape[0]):
            # å½’ä¸€åŒ–ç‰¹å¾ä»¥ä¾¿æ˜¾ç¤º
            feat_norm = (features[i] - features[i].min()) / (features[i].max() - features[i].min() + 1e-10)
            label = names[i] if i < len(names) else f'Feature-{i+1}'
            ax3.plot(time_features, feat_norm, label=label, color=colors[i], linewidth=1.5, alpha=0.8)

        ax3.set_ylabel(labels['normalized_feature'], fontsize=12)
        ax3.set_title(labels['features_title'], fontsize=14, fontweight='bold')
        ax3.legend(loc='upper right', fontsize=9)
        ax3.grid(True, alpha=0.3)
    else:
        ax3.text(0.5, 0.5, labels['no_features'], ha='center', va='center', fontsize=14, transform=ax3.transAxes)
        ax3.set_ylabel(labels['feature'], fontsize=12)
        ax3.set_title('Audio Features', fontsize=14, fontweight='bold')

    # æ ‡è®°å˜åŒ–ç‚¹
    for cp_time in change_points_times:
        ax3.axvline(x=cp_time, color='red', linestyle='--', linewidth=2, alpha=0.8)

    ax3.set_xlabel(labels['time'], fontsize=12)
    ax3.set_xlim(0, duration)

    # è°ƒæ•´å¸ƒå±€
    plt.tight_layout()

    # ä¿å­˜å›¾ç‰‡
    if output_path is None:
        output_path = 'audio_change_points_visualization.png'

    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    print(f"\nâœ… Visualization saved to: {output_path}")

    plt.close()


def main():
    parser = argparse.ArgumentParser(
        description='ä½¿ç”¨Rupturesæ£€æµ‹éŸ³é¢‘å˜åŒ–ç‚¹ï¼ˆä¸ä½¿ç”¨librosaï¼‰',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ€§èƒ½ä¼˜åŒ–å»ºè®®:
  - å¿«é€Ÿæ¨¡å¼: --feature rms --jump 20 --hop-length 2048 --pen 1.0
  - å¹³è¡¡æ¨¡å¼: --feature combined --jump 10 --hop-length 1024 (é»˜è®¤)
  - ç²¾ç¡®æ¨¡å¼: --feature mfcc --jump 5 --hop-length 512 --pen 1.0
  - æ•æ„Ÿæ¨¡å¼: --pen 0.5 (æ£€æµ‹æ›´å¤šå˜åŒ–ç‚¹)

ç¤ºä¾‹:
  # é»˜è®¤æ£€æµ‹ï¼ˆå¹³è¡¡æ¨¡å¼ï¼Œä½¿ç”¨RMS+ZCRç‰¹å¾ï¼‰
  python audio_Ruptures.py audio.wav

  # å¿«é€Ÿæ£€æµ‹
  python audio_Ruptures.py audio.wav --feature rms --jump 20

  # ç²¾ç¡®æ£€æµ‹ï¼ˆä½¿ç”¨MFCCç‰¹å¾ï¼‰
  python audio_Ruptures.py audio.wav --feature mfcc --jump 5

  # æ£€æµ‹æ›´å¤šå˜åŒ–ç‚¹ï¼ˆé™ä½æƒ©ç½šå€¼ï¼‰
  python audio_Ruptures.py audio.wav --pen 1.0

  # æ£€æµ‹å¹¶ç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡
  python audio_Ruptures.py audio.wav --visualize

  # æŒ‡å®šå¯è§†åŒ–è¾“å‡ºè·¯å¾„
  python audio_Ruptures.py audio.wav -v -o result.png

  # ç”Ÿæˆé¢œè‰²å˜åŒ–è§†é¢‘ï¼ˆçº¯è‰²ç”»é¢ï¼Œåˆ†å‰²ç‚¹å¤„åˆ‡æ¢é¢œè‰²ï¼‰
  python audio_Ruptures.py audio.wav --video

  # ç”Ÿæˆè‡ªå®šä¹‰åˆ†è¾¨ç‡å’Œå¸§ç‡çš„è§†é¢‘
  python audio_Ruptures.py audio.wav --video --resolution 1280x720 --fps 24

  # åŒæ—¶ç”Ÿæˆå›¾ç‰‡å’Œè§†é¢‘
  python audio_Ruptures.py audio.wav --visualize --video

  # å®Œæ•´ç¤ºä¾‹ï¼šä½¿ç”¨MFCCç‰¹å¾ï¼Œé™ä½æƒ©ç½šå€¼ï¼Œå¹¶ç”Ÿæˆå¯è§†åŒ–
  python audio_Ruptures.py audio.wav --feature mfcc --pen 1.0 --visualize
        """
    )
    parser.add_argument('audio_path', type=str, help='éŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--sr', type=int, default=16000,
                        help='é‡‡æ ·ç‡ï¼Œé»˜è®¤16000ï¼ˆé™ä½å¦‚8000å¯åŠ é€Ÿï¼‰')
    parser.add_argument('--feature', type=str, default='combined',
                        choices=['rms', 'zcr', 'spectral', 'mfcc', 'combined'],
                        help='ç‰¹å¾ç±»å‹ï¼Œé»˜è®¤combinedï¼ˆRMS+ZCRï¼Œå¹³è¡¡é€Ÿåº¦å’Œå‡†ç¡®æ€§ï¼‰')
    parser.add_argument('--method', type=str, default='Pelt',
                        choices=['Pelt', 'Binseg', 'BottomUp', 'Window'],
                        help='æ£€æµ‹æ–¹æ³•ï¼Œé»˜è®¤Pelt')
    parser.add_argument('--model', type=str, default='l2',
                        choices=['l1', 'l2', 'rbf', 'linear', 'normal', 'ar'],
                        help='æŸå¤±æ¨¡å‹ï¼Œé»˜è®¤l2ï¼ˆæœ€å¿«ï¼‰ï¼Œrbfæ›´å‡†ç¡®ä½†æ…¢å¾—å¤š')
    parser.add_argument('--min-size', type=int, default=10,
                        help='æœ€å°æ®µé•¿åº¦ï¼ˆå¸§æ•°ï¼‰ï¼Œé»˜è®¤10')
    parser.add_argument('--jump', type=int, default=10,
                        help='è·³è·ƒæ­¥é•¿ï¼Œé»˜è®¤10ï¼ˆå€¼è¶Šå¤§é€Ÿåº¦è¶Šå¿«ä½†ç²¾åº¦ç•¥é™ï¼‰')
    parser.add_argument('--pen', type=float, default=None,
                        help='æƒ©ç½šå€¼ï¼Œé»˜è®¤è‡ªåŠ¨è®¡ç®—ï¼ˆå€¼è¶Šå¤§æ£€æµ‹åˆ°çš„å˜åŒ–ç‚¹è¶Šå°‘ï¼‰')
    parser.add_argument('--hop-length', type=int, default=1024,
                        help='å¸§ç§»ï¼Œé»˜è®¤1024ï¼ˆå€¼è¶Šå¤§é€Ÿåº¦è¶Šå¿«ä½†æ—¶é—´ç²¾åº¦é™ä½ï¼‰')
    parser.add_argument('--visualize', '-v', action='store_true',
                        help='ç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡ï¼ˆåŒ…å«æ³¢å½¢ã€æ—¶é¢‘å›¾å’Œç‰¹å¾ï¼‰')
    parser.add_argument('--video', action='store_true',
                        help='ç”Ÿæˆé¢œè‰²å˜åŒ–è§†é¢‘ï¼ˆçº¯è‰²ç”»é¢é…éŸ³é¢‘ï¼Œåˆ†å‰²ç‚¹å¤„åˆ‡æ¢é¢œè‰²ï¼‰')
    parser.add_argument('--output', '-o', type=str, default=None,
                        help='å¯è§†åŒ–å›¾ç‰‡æˆ–è§†é¢‘è¾“å‡ºè·¯å¾„')
    parser.add_argument('--fps', type=int, default=30,
                        help='ç”Ÿæˆè§†é¢‘çš„å¸§ç‡ï¼Œé»˜è®¤30')
    parser.add_argument('--resolution', type=str, default='1920x1080',
                        help='ç”Ÿæˆè§†é¢‘çš„åˆ†è¾¨ç‡ï¼Œé»˜è®¤1920x1080')

    args = parser.parse_args()

    import time
    start_time = time.time()

    # å…ˆåŠ è½½éŸ³é¢‘å’Œæå–ç‰¹å¾ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
    print(f"æ­£åœ¨åŠ è½½éŸ³é¢‘: {args.audio_path}")
    audio, sr = load_audio(args.audio_path, args.sr)
    audio_duration = len(audio) / sr
    print(f"  éŸ³é¢‘æ—¶é•¿: {audio_duration:.2f}ç§’")

    print(f"æ­£åœ¨æå–ç‰¹å¾: {args.feature} (hop_length={args.hop_length})")
    features = extract_features(audio, sr, args.feature, hop_length=args.hop_length)
    print(f"  ç‰¹å¾å½¢çŠ¶: {features.shape}")

    print(f"æ­£åœ¨æ£€æµ‹å˜åŒ–ç‚¹: method={args.method}, model={args.model}, jump={args.jump}")
    change_points_frames, actual_pen = detect_change_points(
        features, method=args.method, model=args.model,
        min_size=args.min_size, jump=args.jump, pen=args.pen
    )

    print(f"æ­£åœ¨è½¬æ¢ä¸ºæ—¶é—´...")
    change_points_times = frames_to_time(change_points_frames, sr, args.hop_length)

    elapsed_time = time.time() - start_time

    # è¾“å‡ºç»“æœ
    print(f"\n{'='*50}")
    print(f"æ£€æµ‹å®Œæˆ! è€—æ—¶: {elapsed_time:.2f}ç§’")
    print(f"æ£€æµ‹åˆ° {len(change_points_frames)} ä¸ªå˜åŒ–ç‚¹:")
    print(f"{'='*50}")

    if len(change_points_frames) == 0:
        print("\nâš ï¸  æœªæ£€æµ‹åˆ°ä»»ä½•å˜åŒ–ç‚¹ï¼")
        print("å»ºè®®:")
        print("  1. é™ä½æƒ©ç½šå€¼: --pen 1.0 æˆ– --pen 0.5")
        print("  2. ä½¿ç”¨æ›´æ•æ„Ÿçš„ç‰¹å¾: --feature mfcc")
        print("  3. å‡å°æœ€å°æ®µé•¿åº¦: --min-size 5")
        print("  4. å‡å°è·³è·ƒæ­¥é•¿: --jump 5")
    else:
        print(f"\n{'å¸§ç´¢å¼•':>8} | {'æ—¶é—´(ç§’)':>10}")
        print("-" * 30)
        for frame, time in zip(change_points_frames, change_points_times):
            print(f"{frame:8d} | {time:10.3f}")

    # å¯è§†åŒ–
    if args.visualize:
        print(f"\næ­£åœ¨ç”Ÿæˆå¯è§†åŒ–å›¾ç‰‡...")

        # æ„å»ºåŒ…å«å‚æ•°çš„æ–‡ä»¶åï¼ˆä½¿ç”¨å®é™…çš„penå€¼ï¼‰
        pen_str = f"pen{actual_pen:.2f}"
        param_suffix = f"_{args.feature}_{args.method}_{args.model}_hop{args.hop_length}_jump{args.jump}_{pen_str}_cp{len(change_points_frames)}"

        # ç¡®å®šè¾“å‡ºè·¯å¾„
        if args.output is None:
            audio_path = Path(args.audio_path)
            output_path = audio_path.parent / f"{audio_path.stem}{param_suffix}.png"
        else:
            # å¦‚æœç”¨æˆ·æŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œåœ¨æ–‡ä»¶åä¸­æ’å…¥å‚æ•°
            output_p = Path(args.output)
            output_path = output_p.parent / f"{output_p.stem}{param_suffix}{output_p.suffix}"

        visualize_change_points(
            audio=audio,
            sr=sr,
            change_points_times=change_points_times,
            features=features,
            feature_type=args.feature,
            hop_length=args.hop_length,
            output_path=str(output_path)
        )

    # ç”Ÿæˆé¢œè‰²å˜åŒ–è§†é¢‘
    if args.video:
        print(f"\næ­£åœ¨ç”Ÿæˆé¢œè‰²å˜åŒ–è§†é¢‘...")

        # è§£æåˆ†è¾¨ç‡
        try:
            width, height = map(int, args.resolution.split('x'))
            resolution = (width, height)
        except ValueError:
            print(f"âš ï¸  æ— æ•ˆçš„åˆ†è¾¨ç‡æ ¼å¼ '{args.resolution}'ï¼Œä½¿ç”¨é»˜è®¤å€¼ 1920x1080")
            resolution = (1920, 1080)

        # æ„å»ºåŒ…å«å‚æ•°çš„æ–‡ä»¶åï¼ˆä½¿ç”¨å®é™…çš„penå€¼ï¼‰
        pen_str = f"pen{actual_pen:.2f}"
        param_suffix = f"_{args.feature}_{args.method}_{args.model}_hop{args.hop_length}_jump{args.jump}_{pen_str}_cp{len(change_points_frames)}"

        # ç¡®å®šè¾“å‡ºè·¯å¾„
        if args.output is None:
            audio_file = Path(args.audio_path)
            video_output_path = audio_file.parent / f"{audio_file.stem}{param_suffix}.mp4"
        else:
            # å¦‚æœç”¨æˆ·æŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼Œåœ¨æ–‡ä»¶åä¸­æ’å…¥å‚æ•°
            output_p = Path(args.output)
            video_output_path = output_p.parent / f"{output_p.stem}{param_suffix}{output_p.suffix}"

        # æ„å»ºå‚æ•°ä¿¡æ¯å­—å…¸
        params_info = {
            'feature': args.feature,
            'method': args.method,
            'model': args.model,
            'hop_length': args.hop_length,
            'jump': args.jump,
            'pen': f"{actual_pen:.2f}",
            'min_size': args.min_size,
            'n_change_points': len(change_points_frames),
            'sr': sr,
        }

        generate_color_video(
            audio_path=args.audio_path,
            change_points_times=change_points_times,
            output_path=str(video_output_path),
            fps=args.fps,
            resolution=resolution,
            params_info=params_info
        )

    return change_points_frames, change_points_times


if __name__ == '__main__':
    main()
