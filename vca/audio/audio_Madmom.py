"""
éŸ³é¢‘å…³é”®ç‚¹æ£€æµ‹è„šæœ¬
ä½¿ç”¨Madmomåº“æ£€æµ‹éŸ³é¢‘ä¸­çš„èŠ‚æ‹ã€å†²å‡»ç‚¹ç­‰å…³é”®æ„Ÿå®˜ç‚¹
æ”¯æŒå¯è§†åŒ–å’Œè§†é¢‘ç”Ÿæˆ
"""
# ============ Python 3.10+ å’Œ NumPy 1.24+ å…¼å®¹æ€§ä¿®å¤ ============
# å¿…é¡»åœ¨å¯¼å…¥ madmom ä¹‹å‰æ‰§è¡Œ

# ä¿®å¤ collections æ¨¡å—ï¼ˆPython 3.10+ ç§»é™¤äº†ç›´æ¥ä» collections å¯¼å…¥æŠ½è±¡åŸºç±»ï¼‰
import collections
import collections.abc
for attr in ('MutableSequence', 'Iterable', 'Mapping', 'MutableMapping', 'Callable'):
    if not hasattr(collections, attr):
        setattr(collections, attr, getattr(collections.abc, attr))

# ä¿®å¤ numpy æ¨¡å—ï¼ˆNumPy 1.24+ ç§»é™¤äº† np.float, np.int ç­‰åˆ«åï¼‰
import numpy as np
import warnings
with warnings.catch_warnings():
    warnings.simplefilter("ignore", FutureWarning)
    if not hasattr(np, 'float'):
        np.float = np.float64
    if not hasattr(np, 'int'):
        np.int = np.int64
    if not hasattr(np, 'complex'):
        np.complex = np.complex128
    if not hasattr(np, 'object'):
        np.object = np.object_
    if not hasattr(np, 'bool'):
        np.bool = np.bool_
    if not hasattr(np, 'str'):
        np.str = np.str_

# ============ å…¼å®¹æ€§ä¿®å¤ç»“æŸ ============

import os
import sys
import time
import argparse
import json
import subprocess
import tempfile
from typing import List, Tuple
from pathlib import Path

from madmom.features.downbeats import RNNDownBeatProcessor, DBNDownBeatTrackingProcessor
from madmom.audio.signal import Signal
import madmom.features.downbeats as _downbeats_module
import itertools as _it

# ============ Lightweight caches for interactive usage ============
# Madmom's RNN/CNN processors are expensive; in the UI users often tweak
# thresholds and re-run. Cache intermediate activations per audio file.
_VCA_ACT_CACHE_MAX = 8
_vca_cache_beat_act = {}


def _vca_cache_key(audio_path: str) -> tuple:
    try:
        return (audio_path, os.path.getmtime(audio_path))
    except Exception:
        return (audio_path, None)


def _vca_cache_put(cache: dict, key: tuple, value):
    cache[key] = value
    if len(cache) > _VCA_ACT_CACHE_MAX:
        # Keep it simple: clear to bound memory.
        cache.clear()

# ============ End caches ============

# ============ NumPy 2.x å…¼å®¹æ€§ä¿®å¤ for DBNDownBeatTrackingProcessor ============
# madmom 0.16.1 ä¸­çš„ np.asarray(results)[:, 1] åœ¨ NumPy 2.x ä¸­ä¼šå¤±è´¥
# å› ä¸º results ä¸­çš„å…ƒç´  (path, log_prob) å½¢çŠ¶ä¸ä¸€è‡´

def _patched_dbn_process(self, activations, **kwargs):
    """ä¿®å¤ NumPy 2.x å…¼å®¹æ€§çš„ DBNDownBeatTrackingProcessor.process æ–¹æ³•"""
    first = 0
    if self.threshold:
        idx = np.nonzero(activations >= self.threshold)[0]
        if idx.any():
            first = max(first, np.min(idx))
            last = min(len(activations), np.max(idx) + 1)
        else:
            last = first
        activations = activations[first:last]
    
    if not activations.any():
        return np.empty((0, 2))
    
    results = list(self.map(_downbeats_module._process_dbn, 
                            zip(self.hmms, _it.repeat(activations))))
    
    # ä¿®å¤: ä½¿ç”¨åˆ—è¡¨æ¨å¯¼å¼è·å– log probabilitiesï¼Œè€Œä¸æ˜¯ np.asarray(results)[:, 1]
    log_probs = [r[1] for r in results]
    best = np.argmax(log_probs)
    
    path, _ = results[best]
    st = self.hmms[best].transition_model.state_space
    om = self.hmms[best].observation_model
    positions = st.state_positions[path]
    beat_numbers = positions.astype(int) + 1
    
    if self.correct:
        beats = np.empty(0, dtype=np.int64)  # ä¿®å¤: np.int -> np.int64
        beat_range = om.pointers[path] >= 1
        idx = np.nonzero(np.diff(beat_range.astype(np.int64)))[0] + 1  # ä¿®å¤
        if beat_range[0]:
            idx = np.r_[0, idx]
        if beat_range[-1]:
            idx = np.r_[idx, beat_range.size]
        if idx.any():
            for left, right in idx.reshape((-1, 2)):
                peak = np.argmax(activations[left:right]) // 2 + left
                beats = np.hstack((beats, peak))
    else:
        beats = np.nonzero(np.diff(beat_numbers))[0] + 1
    
    return np.vstack(((beats + first) / float(self.fps), beat_numbers[beats])).T

# åº”ç”¨ monkey-patch
DBNDownBeatTrackingProcessor.process = _patched_dbn_process
# ============ NumPy 2.x å…¼å®¹æ€§ä¿®å¤ç»“æŸ ============

# æ·»åŠ vcaæ¨¡å—åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))
from vca.audio.audio_utils import load_audio_no_librosa

# ============ NMSï¼ˆéæå¤§å€¼æŠ‘åˆ¶ï¼‰å‡½æ•° ============

def nms_1d(values, timestamps, min_distance, threshold=None, max_points=None,
           sort_by_values=None):
    """
    ä¸€ç»´éæå¤§å€¼æŠ‘åˆ¶

    Args:
        values: æ£€æµ‹å€¼ï¼ˆç”¨äºé˜ˆå€¼è¿‡æ»¤ï¼Œå¦‚confidenceï¼‰
        timestamps: å¯¹åº”çš„æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
        min_distance: æœ€å°é—´éš”ï¼ˆç§’ï¼‰ï¼Œç›¸é‚»ç‚¹ä¹‹é—´è‡³å°‘é—´éš”è¿™ä¹ˆå¤šç§’
        threshold: å¯é€‰çš„é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„ç‚¹è¢«å¿½ç•¥
        max_points: å¯é€‰çš„æœ€å¤§ç‚¹æ•°é™åˆ¶
        sort_by_values: ç”¨äºæ’åºçš„å€¼ï¼ˆå¦‚pitchï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨valuesæ’åº

    Returns:
        selected_timestamps: ç­›é€‰åçš„æ—¶é—´æˆ³
        selected_values: ç­›é€‰åçš„å€¼
        selected_indices: ç­›é€‰åçš„åŸå§‹ç´¢å¼•
    """
    values = np.array(values, dtype=np.float64)
    timestamps = np.array(timestamps, dtype=np.float64)

    if sort_by_values is not None:
        sort_by_values = np.array(sort_by_values, dtype=np.float64)
    else:
        sort_by_values = values.copy()

    if len(values) == 0:
        return np.array([]), np.array([]), np.array([], dtype=np.int64)

    original_indices = np.arange(len(values))

    # é˜ˆå€¼è¿‡æ»¤ï¼ˆåŸºäºvaluesï¼Œå¦‚confidenceï¼‰
    if threshold is not None:
        mask = values >= threshold
        values = values[mask]
        timestamps = timestamps[mask]
        sort_by_values = sort_by_values[mask]
        original_indices = original_indices[mask]

    if len(values) == 0:
        return np.array([]), np.array([]), np.array([], dtype=np.int64)

    # æŒ‰sort_by_valuesä»å¤§åˆ°å°æ’åºï¼ˆä¼˜å…ˆä¿ç•™å€¼æœ€å¤§çš„ç‚¹ï¼‰
    sorted_order = np.argsort(sort_by_values)[::-1]
    selected_mask = np.zeros(len(values), dtype=bool)

    # è´ªå¿ƒé€‰æ‹©
    for idx in sorted_order:
        t = timestamps[idx]
        already_selected_times = timestamps[selected_mask]

        if len(already_selected_times) == 0 or np.all(np.abs(already_selected_times - t) >= min_distance):
            selected_mask[idx] = True
            if max_points is not None and np.sum(selected_mask) >= max_points:
                break

    # æŒ‰æ—¶é—´æ’åºè¿”å›
    selected_timestamps = timestamps[selected_mask]
    selected_values = values[selected_mask]
    selected_indices = original_indices[selected_mask]

    time_order = np.argsort(selected_timestamps)
    return (
        selected_timestamps[time_order],
        selected_values[time_order],
        selected_indices[time_order]
    )


def nms_adaptive(values, timestamps, min_distance, adaptive_ratio=0.5):
    """è‡ªé€‚åº”é˜ˆå€¼çš„NMSï¼Œé˜ˆå€¼ = æœ€å¤§å€¼ * adaptive_ratio"""
    values = np.array(values, dtype=np.float64)
    if len(values) == 0:
        return np.array([]), np.array([]), np.array([], dtype=np.int64)
    threshold = np.max(values) * adaptive_ratio
    return nms_1d(values, timestamps, min_distance, threshold=threshold)


def nms_window(values, timestamps, window_size, top_k=1):
    """çª—å£NMSï¼šæ¯ä¸ªæ—¶é—´çª—å£å†…ä¿ç•™top-kä¸ªç‚¹"""
    values = np.array(values, dtype=np.float64)
    timestamps = np.array(timestamps, dtype=np.float64)
    original_indices = np.arange(len(values))

    if len(values) == 0:
        return np.array([]), np.array([]), np.array([], dtype=np.int64)

    t_min, t_max = timestamps.min(), timestamps.max()
    selected_mask = np.zeros(len(values), dtype=bool)

    window_start = t_min
    while window_start <= t_max:
        window_end = window_start + window_size
        window_mask = (timestamps >= window_start) & (timestamps < window_end)
        window_indices = np.where(window_mask)[0]

        if len(window_indices) > 0:
            window_values = values[window_indices]
            top_k_indices = window_indices[np.argsort(window_values)[::-1][:top_k]]
            selected_mask[top_k_indices] = True

        window_start = window_end

    selected_timestamps = timestamps[selected_mask]
    selected_values = values[selected_mask]
    selected_indices = original_indices[selected_mask]

    time_order = np.argsort(selected_timestamps)
    return (
        selected_timestamps[time_order],
        selected_values[time_order],
        selected_indices[time_order]
    )

# ============ Pitchæ£€æµ‹å‡½æ•° ============

def detect_pitch(audio_path, samplerate=0, tolerance=0.8):
    """
    æ£€æµ‹éŸ³é¢‘çš„Pitch

    Returns:
        pitches: pitchæ•°ç»„
        confidences: ç½®ä¿¡åº¦æ•°ç»„
        timestamps: æ—¶é—´æˆ³æ•°ç»„
        actual_samplerate: å®é™…é‡‡æ ·ç‡
    """
    from aubio import source, pitch
    from pathlib import Path

    # Convert MP3 to WAV if needed (aubio's source_wavread requires WAV format)
    p = Path(audio_path)
    if p.suffix.lower() not in {".wav", ".wave"}:
        # Create a temporary WAV file
        wav_path = p.with_name(f"{p.stem}__vca_pitch.wav")

        # Check if WAV already exists and is newer than source
        if not wav_path.exists() or wav_path.stat().st_mtime < p.stat().st_mtime:
            # Convert using ffmpeg
            import shutil
            ffmpeg = shutil.which("ffmpeg")
            if ffmpeg:
                cmd = [
                    ffmpeg, "-y", "-i", str(p),
                    "-vn", "-ac", "1", "-acodec", "pcm_s16le",
                    str(wav_path)
                ]
                result = subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                if result.returncode != 0:
                    raise RuntimeError(f"ffmpeg failed to convert {audio_path} to WAV")
            else:
                raise RuntimeError("ffmpeg not found. Cannot convert MP3 to WAV for pitch detection.")

        audio_path = str(wav_path)

    downsample = 1
    win_s = 4096 // downsample
    hop_s = 512 // downsample

    s = source(audio_path, samplerate, hop_s)
    actual_samplerate = s.samplerate

    pitch_o = pitch("yin", win_s, hop_s, actual_samplerate)
    pitch_o.set_unit("midi")
    pitch_o.set_tolerance(tolerance)

    pitches = []
    confidences = []
    timestamps = []
    total_frames = 0

    while True:
        samples, read = s()
        p = pitch_o(samples)[0]
        c = pitch_o.get_confidence()

        timestamps.append(total_frames / float(actual_samplerate))
        pitches.append(p)
        confidences.append(c)

        total_frames += read
        if read < hop_s:
            break

    return np.array(pitches), np.array(confidences), np.array(timestamps), actual_samplerate


# ============ Melèƒ½é‡æ£€æµ‹å‡½æ•° ============

def compute_mel_energies(audio_path, samplerate=0, win_s=512, n_filters=40):
    """
    è®¡ç®—melèƒ½é‡

    Returns:
        timestamps: æ—¶é—´æˆ³æ•°ç»„
        energies: (n_frames, n_filters) çš„èƒ½é‡çŸ©é˜µ
        total_energies: æ¯å¸§çš„æ€»èƒ½é‡
        samplerate: å®é™…é‡‡æ ·ç‡
    """
    from aubio import source, pvoc, filterbank
    from pathlib import Path

    # Convert MP3 to WAV if needed (aubio's source_wavread requires WAV format)
    p = Path(audio_path)
    if p.suffix.lower() not in {".wav", ".wave"}:
        # Create a temporary WAV file
        wav_path = p.with_name(f"{p.stem}__vca_mel.wav")

        # Check if WAV already exists and is newer than source
        if not wav_path.exists() or wav_path.stat().st_mtime < p.stat().st_mtime:
            # Convert using ffmpeg
            import shutil
            ffmpeg = shutil.which("ffmpeg")
            if ffmpeg:
                cmd = [
                    ffmpeg, "-y", "-i", str(p),
                    "-vn", "-ac", "1", "-acodec", "pcm_s16le",
                    str(wav_path)
                ]
                result = subprocess.run(cmd, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
                if result.returncode != 0:
                    raise RuntimeError(f"ffmpeg failed to convert {audio_path} to WAV")
            else:
                raise RuntimeError("ffmpeg not found. Cannot convert MP3 to WAV for mel energy computation.")

        audio_path = str(wav_path)

    hop_s = win_s // 4

    s = source(audio_path, samplerate, hop_s)
    actual_samplerate = s.samplerate

    pv = pvoc(win_s, hop_s)
    f = filterbank(n_filters, win_s)
    f.set_mel_coeffs_slaney(actual_samplerate)

    energies_list = []
    timestamps = []
    total_frames = 0

    while True:
        samples, read = s()
        fftgrain = pv(samples)
        new_energies = f(fftgrain)

        timestamps.append(total_frames / float(actual_samplerate))
        energies_list.append(new_energies.copy())

        total_frames += read
        if read < hop_s:
            break

    energies = np.vstack(energies_list)
    timestamps = np.array(timestamps)
    total_energies = np.sum(energies, axis=1)

    return timestamps, energies, total_energies, actual_samplerate





class SensoryKeypointDetector:
    def __init__(
        self,
        # æ£€æµ‹æ–¹æ³•é€‰æ‹©
        detection_method: str = "downbeat",  # "downbeat", "pitch", "mel_energy"
        # DBN èŠ‚æ‹æ£€æµ‹å‚æ•°
        beats_per_bar: list = None,
        min_bpm: float = 55.0,
        max_bpm: float = 215.0,
        num_tempi: int = 60,
        transition_lambda: float = 100,
        observation_lambda: int = 16,
        dbn_threshold: float = 0.05,
        correct_beats: bool = True,
        fps: int = 100,
        # Pitchæ£€æµ‹å‚æ•°
        pitch_tolerance: float = 0.8,
        pitch_threshold: float = 0.8,
        pitch_min_distance: float = 0.5,
        pitch_nms_method: str = "basic",  # "basic", "adaptive", "window"
        pitch_max_points: int = None,
        # Melèƒ½é‡æ£€æµ‹å‚æ•°
        mel_win_s: int = 512,
        mel_n_filters: int = 40,
        mel_threshold_ratio: float = 0.3,
        mel_min_distance: float = 0.5,
        mel_nms_method: str = "basic",
        mel_max_points: int = None,
    ):
        """
        éŸ³é¢‘æ„Ÿå®˜å…³é”®ç‚¹æ£€æµ‹å™¨ - æ”¯æŒå¤šç§æ£€æµ‹æ–¹æ³•
        
        Args:
            detection_method: æ£€æµ‹æ–¹æ³• ("downbeat", "pitch", "mel_energy")
            
            DBN èŠ‚æ‹æ£€æµ‹å‚æ•°:
                beats_per_bar: æ¯å°èŠ‚çš„æ‹æ•°ï¼Œå¦‚[4]è¡¨ç¤º4/4æ‹ (é»˜è®¤[4])
                min_bpm: æœ€å°BPM (é»˜è®¤55.0)
                max_bpm: æœ€å¤§BPM (é»˜è®¤215.0)
                ...
            
            Pitchæ£€æµ‹å‚æ•°:
                pitch_tolerance: pitchæ£€æµ‹å®¹å·® (é»˜è®¤0.8)
                pitch_threshold: pitchç½®ä¿¡åº¦é˜ˆå€¼ (é»˜è®¤0.8)
                pitch_min_distance: pitchæ£€æµ‹çš„æœ€å°é—´éš”(ç§’) (é»˜è®¤0.5)
                pitch_nms_method: NMSæ–¹æ³• (é»˜è®¤"basic")
                pitch_max_points: æœ€å¤§ä¿ç•™ç‚¹æ•°
            
            Melèƒ½é‡æ£€æµ‹å‚æ•°:
                mel_win_s: FFTçª—å£å¤§å° (é»˜è®¤512)
                mel_n_filters: Melæ»¤æ³¢å™¨æ•°é‡ (é»˜è®¤40)
                mel_threshold_ratio: èƒ½é‡é˜ˆå€¼æ¯”ä¾‹ (é»˜è®¤0.3)
                mel_min_distance: æœ€å°é—´éš”(ç§’) (é»˜è®¤0.5)
                mel_nms_method: NMSæ–¹æ³• (é»˜è®¤"basic")
                mel_max_points: æœ€å¤§ä¿ç•™ç‚¹æ•°
        """
        self.detection_method = detection_method
        
        # DBN å‚æ•°
        self.beats_per_bar = beats_per_bar if beats_per_bar is not None else [4]
        self.min_bpm = min_bpm
        self.max_bpm = max_bpm
        self.num_tempi = num_tempi
        self.transition_lambda = transition_lambda
        self.observation_lambda = observation_lambda
        self.dbn_threshold = dbn_threshold
        self.correct_beats = correct_beats
        self.fps = fps
        
        # Pitchå‚æ•°
        self.pitch_tolerance = pitch_tolerance
        self.pitch_threshold = pitch_threshold
        self.pitch_min_distance = pitch_min_distance
        self.pitch_nms_method = pitch_nms_method
        self.pitch_max_points = pitch_max_points
        
        # Melå‚æ•°
        self.mel_win_s = mel_win_s
        self.mel_n_filters = mel_n_filters
        self.mel_threshold_ratio = mel_threshold_ratio
        self.mel_min_distance = mel_min_distance
        self.mel_nms_method = mel_nms_method
        self.mel_max_points = mel_max_points

    def analyze_pitch(self, audio_path):
        """Pitchæ£€æµ‹ + NMSï¼Œé€»è¾‘ä¸demo_pitch_nms.pyå®Œå…¨ä¸€è‡´"""
        print(f"æ­£åœ¨è¿›è¡ŒPitchæ£€æµ‹: {audio_path} ...")

        pitches, confidences, timestamps, samplerate = detect_pitch(
            audio_path, tolerance=self.pitch_tolerance
        )

        print(f"æ£€æµ‹å®Œæˆ: {len(pitches)} ä¸ªç‚¹, æ—¶é•¿ {timestamps[-1]:.2f}s")

        # ============== NMSè¿‡æ»¤ï¼ˆä¸demo_pitch_nms.pyå®Œå…¨ä¸€è‡´ï¼‰==============
        # ä½¿ç”¨ç»å¯¹é˜ˆå€¼
        threshold = self.pitch_threshold
        min_distance = self.pitch_min_distance
        max_points = self.pitch_max_points
        method = self.pitch_nms_method

        print(f"\nåº”ç”¨NMSè¿‡æ»¤ (method={method}, min_distance={min_distance}s, threshold={threshold})")
        if max_points is not None:
            print(f"  é™åˆ¶ç‚¹æ•°: {max_points}")

        if method == "adaptive":
            sel_t, sel_c, sel_i = nms_adaptive(confidences, timestamps, min_distance,
                                               adaptive_ratio=threshold)
        elif method == "window":
            sel_t, sel_c, sel_i = nms_window(confidences, timestamps, 1.0, top_k=1)
        else:  # basic
            sel_t, sel_c, sel_i = nms_1d(
                confidences, timestamps, min_distance,
                threshold=threshold, max_points=max_points,
                sort_by_values=pitches  # æŒ‰pitchæ’åº
            )

        sel_p = pitches[sel_i]

        print(f"è¿‡æ»¤å: {len(sel_t)} ä¸ªæ˜¾è‘—ç‚¹")
        
        # è½¬æ¢ä¸ºå…³é”®ç‚¹æ ¼å¼
        timeline = []
        for t, p, c in zip(sel_t, sel_p, sel_c):
            timeline.append({
                'time': float(t),
                'type': 'Pitch',
                'pitch': float(p),
                'confidence': float(c),
                'intensity': float(c)
            })
        
        return {
            "meta": {"n_pitch_points": len(timeline)},
            "keypoints": timeline,
            "pitches": sel_p,
            "confidences": sel_c,
            "timestamps": sel_t,
            "sample_rate": samplerate
        }

    def analyze_mel_energy(self, audio_path):
        """Melèƒ½é‡æ£€æµ‹ + NMS"""
        print(f"æ­£åœ¨è¿›è¡ŒMelèƒ½é‡æ£€æµ‹: {audio_path} ...")
        
        timestamps, energies, total_energies, samplerate = compute_mel_energies(
            audio_path, win_s=self.mel_win_s, n_filters=self.mel_n_filters
        )
        
        print(f"  è®¡ç®—å®Œæˆ: {len(timestamps)} å¸§, æ—¶é•¿ {timestamps[-1]:.2f}s")
        print(f"  é‡‡æ ·ç‡: {samplerate} Hz, Melæ»¤æ³¢å™¨: {self.mel_n_filters} ä¸ª")
        
        # ä½¿ç”¨å…¨éƒ¨é¢‘å¸¦
        selected_energies = total_energies
        
        # è®¡ç®—é˜ˆå€¼
        max_energy = np.max(selected_energies)
        threshold = max_energy * self.mel_threshold_ratio
        
        print(f"  åº”ç”¨NMSè¿‡æ»¤ (method={self.mel_nms_method}, min_distance={self.mel_min_distance}s)")
        print(f"  é˜ˆå€¼: {threshold:.4f} (æœ€å¤§å€¼çš„ {self.mel_threshold_ratio*100:.0f}%)")
        
        if self.mel_nms_method == "adaptive":
            sel_t, sel_e, sel_i = nms_adaptive(selected_energies, timestamps, self.mel_min_distance, 
                                               adaptive_ratio=self.mel_threshold_ratio)
        elif self.mel_nms_method == "window":
            sel_t, sel_e, sel_i = nms_window(selected_energies, timestamps, 1.0, top_k=1)
        else:  # basic
            sel_t, sel_e, sel_i = nms_1d(
                selected_energies, timestamps, self.mel_min_distance,
                threshold=threshold, max_points=self.mel_max_points
            )
        
        print(f"  è¿‡æ»¤å: {len(sel_t)} ä¸ªæ˜¾è‘—ç‚¹")
        
        # è½¬æ¢ä¸ºå…³é”®ç‚¹æ ¼å¼
        timeline = []
        for t, e in zip(sel_t, sel_e):
            relative = e / max_energy * 100
            timeline.append({
                'time': float(t),
                'type': 'MelEnergy',
                'energy': float(e),
                'relative_intensity': float(relative),
                'intensity': float(e / max_energy)
            })
        
        return {
            "meta": {"n_mel_points": len(timeline)},
            "keypoints": timeline,
            "energies": sel_e,
            "timestamps": sel_t,
            "sample_rate": samplerate
        }

    def analyze(self, audio_path):
        """æ ¹æ®é€‰æ‹©çš„æ–¹æ³•è¿›è¡Œåˆ†æ"""
        print(f"æ­£åœ¨åˆ†æéŸ³é¢‘: {audio_path} ...")
        print(f"ä½¿ç”¨æ£€æµ‹æ–¹æ³•: {self.detection_method}")
        
        if self.detection_method == "pitch":
            return self.analyze_pitch(audio_path)
        elif self.detection_method == "mel_energy":
            return self.analyze_mel_energy(audio_path)
        else:  # downbeat (é»˜è®¤)
            return self.analyze_downbeat(audio_path)
    
    def analyze_downbeat(self, audio_path):

        cache_key = _vca_cache_key(audio_path)

        # 1. èŠ‚å¥åˆ†æ (Rhythm) - è·å–å¼ºæ‹
        downbeats = np.array([])
        print(" -> æ£€æµ‹èŠ‚å¥ (Beats/Downbeats)...")
        print(f"    å‚æ•°: beats_per_bar={self.beats_per_bar}, BPMèŒƒå›´=[{self.min_bpm}, {self.max_bpm}], "
              f"transition_lambda={self.transition_lambda}")
        beat_act = _vca_cache_beat_act.get(cache_key)
        if beat_act is None:
            beat_proc = RNNDownBeatProcessor()
            beat_act = beat_proc(audio_path)
            _vca_cache_put(_vca_cache_beat_act, cache_key, beat_act)
        
        # ä½¿ç”¨ DBNDownBeatTrackingProcessorï¼ˆå·²é€šè¿‡ monkey-patch ä¿®å¤ NumPy 2.x å…¼å®¹æ€§ï¼‰
        beat_tracker = DBNDownBeatTrackingProcessor(
            beats_per_bar=self.beats_per_bar,
            min_bpm=self.min_bpm,
            max_bpm=self.max_bpm,
            num_tempi=self.num_tempi,
            transition_lambda=self.transition_lambda,
            observation_lambda=self.observation_lambda,
            threshold=self.dbn_threshold,
            correct=self.correct_beats,
            fps=self.fps
        )
        beat_info = beat_tracker(beat_act)
        beat_info = np.array(beat_info)
        
        # æå–å¼ºæ‹
        if len(beat_info) > 0:
            downbeats = beat_info[beat_info[:, 1] == 1][:, 0]
        else:
            print("    âš ï¸ æœªèƒ½æ£€æµ‹åˆ°èŠ‚æ‹")

        # 2. ä» beat_act ä¸­æå– downbeat æ¿€æ´»å€¼ä½œä¸º intensity
        # beat_act æ˜¯ (n_frames, n_classes) æ•°ç»„ï¼Œå…¶ä¸­ç¬¬äºŒåˆ—æ˜¯ downbeat æ¿€æ´»å€¼
        # fps æ˜¯å¸§ç‡ï¼Œç”¨äºå°†æ—¶é—´è½¬æ¢ä¸ºå¸§ç´¢å¼•

        def get_downbeat_activation(t, beat_act, fps):
            """è·å–æ—¶é—´tå¤„çš„downbeatæ¿€æ´»å€¼"""
            if beat_act is None or len(beat_act) == 0:
                return 0.5

            # å°†æ—¶é—´è½¬æ¢ä¸ºå¸§ç´¢å¼•
            frame_idx = int(t * fps)
            frame_idx = max(0, min(frame_idx, len(beat_act) - 1))

            # beat_act çš„ç¬¬äºŒåˆ—æ˜¯ downbeat æ¿€æ´»å€¼
            if beat_act.ndim == 2 and beat_act.shape[1] >= 2:
                activation = float(beat_act[frame_idx, 1])
            elif beat_act.ndim == 1:
                activation = float(beat_act[frame_idx])
            else:
                activation = 0.5

            # ç¡®ä¿æ¿€æ´»å€¼åœ¨åˆç†èŒƒå›´å†…
            if not np.isfinite(activation):
                activation = 0.5

            return activation

        # è®¡ç®—éŸ³é¢‘ä¿¡å· (ç”¨äºè¿”å›)
        sig = Signal(audio_path)

        # ç»“æœæ•´åˆ - åªä¿ç•™downbeat
        timeline = []

        # åªä¿ç•™ Downbeat (é‡æ‹)ï¼Œä½¿ç”¨ DBN æ¿€æ´»å€¼ä½œä¸º intensity
        for t in downbeats:
            activation = get_downbeat_activation(t, beat_act, self.fps)
            timeline.append({
                'time': float(t),
                'type': 'Downbeat',
                'activation': float(activation),
                'intensity': float(activation)
            })

        # æŒ‰æ—¶é—´æ’åº
        timeline.sort(key=lambda x: x['time'])

        return {
            "meta": {
                "n_downbeats": len(downbeats)
            },
            "keypoints": timeline,
            "downbeats": downbeats,
            "beat_info": beat_info,
            "beat_activation": beat_act,
            "audio_signal": sig,
            "sample_rate": sig.sample_rate
        }


def normalize_intensity_by_type(keypoints: List[dict]) -> List[dict]:
    """
    æŒ‰ç±»å‹å½’ä¸€åŒ–å…³é”®ç‚¹å¼ºåº¦ï¼Œä½¿ä¸åŒç±»å‹çš„å…³é”®ç‚¹å¯ä»¥å…¬å¹³æ¯”è¾ƒ

    æ¯ç§ç±»å‹ä½¿ç”¨å…¶ä¸»è¦ç‰¹å¾å€¼è¿›è¡Œå½’ä¸€åŒ–ï¼š
    - Downbeat: ä½¿ç”¨ activation (DBNæ¿€æ´»å€¼)

    å½’ä¸€åŒ–åæ‰€æœ‰ç±»å‹çš„å¼ºåº¦éƒ½åœ¨ 0 ~ 1 èŒƒå›´å†…

    Args:
        keypoints: åŸå§‹å…³é”®ç‚¹åˆ—è¡¨

    Returns:
        æ·»åŠ äº† normalized_intensity å­—æ®µçš„å…³é”®ç‚¹åˆ—è¡¨
    """
    if not keypoints:
        return []

    # æŒ‰ç±»å‹åˆ†ç»„
    by_type = {}
    for kp in keypoints:
        kp_type = kp.get('type', 'Unknown')
        by_type.setdefault(kp_type, []).append(kp)

    # ç±»å‹åˆ°ä¸»ç‰¹å¾å­—æ®µçš„æ˜ å°„
    type_to_feature = {
        'Downbeat': 'activation',
    }

    print(f"    æŒ‰ç±»å‹å½’ä¸€åŒ–å¼ºåº¦:")

    # å¯¹æ¯ç§ç±»å‹åˆ†åˆ«å½’ä¸€åŒ–
    for type_name, points in by_type.items():
        # è·å–è¯¥ç±»å‹çš„ä¸»ç‰¹å¾å­—æ®µ
        feature_key = type_to_feature.get(type_name, 'activation')

        # è·å–ç‰¹å¾å€¼ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™å°è¯•ä½¿ç”¨ intensityï¼ˆå…¼å®¹æ—§æ•°æ®ï¼‰
        intensities = []
        for p in points:
            val = p.get(feature_key, p.get('intensity', 0.5))
            intensities.append(val)

        min_i = min(intensities)
        max_i = max(intensities)
        range_i = max_i - min_i

        for i, p in enumerate(points):
            if range_i > 1e-6:
                p['normalized_intensity'] = (intensities[i] - min_i) / range_i
            else:
                # å¦‚æœè¯¥ç±»å‹æ‰€æœ‰ç‚¹å¼ºåº¦ç›¸åŒï¼Œå½’ä¸€åŒ–ä¸º 0.5
                p['normalized_intensity'] = 0.5
            # åŒæ—¶è®¾ç½® intensity å­—æ®µä»¥ä¿æŒå…¼å®¹æ€§
            p['intensity'] = intensities[i]

        print(f"      - {type_name}: {len(points)} ä¸ªç‚¹, "
              f"{feature_key} [{min_i:.3f}, {max_i:.3f}] -> å½’ä¸€åŒ– [0, 1]")

    return keypoints


def filter_significant_keypoints(
    keypoints: List[dict],
    min_interval: float = 0.0,
    top_k: int = 0,
    energy_percentile: float = 0.0,
    use_normalized_intensity: bool = True
) -> List[dict]:
    """
    è¿‡æ»¤å…³é”®ç‚¹ï¼Œåªä¿ç•™æ˜¾è‘—çš„ç‚¹

    Args:
        keypoints: åŸå§‹å…³é”®ç‚¹åˆ—è¡¨
        min_interval: æœ€å°é—´éš”ï¼ˆç§’ï¼‰ï¼Œé—´éš”å†…åªä¿ç•™æœ€å¼ºçš„ç‚¹
        top_k: åªä¿ç•™å¼ºåº¦æœ€é«˜çš„å‰Kä¸ªç‚¹ï¼Œ0è¡¨ç¤ºä¸é™åˆ¶
        energy_percentile: åªä¿ç•™å¼ºåº¦é«˜äºè¯¥ç™¾åˆ†ä½æ•°çš„ç‚¹(0-100)
        use_normalized_intensity: æ˜¯å¦ä½¿ç”¨å½’ä¸€åŒ–åçš„å¼ºåº¦è¿›è¡Œè¿‡æ»¤ï¼ˆæ¨èTrueï¼‰

    Returns:
        è¿‡æ»¤åçš„å…³é”®ç‚¹åˆ—è¡¨
    """
    if not keypoints:
        return []

    filtered = list(keypoints)
    print(f"\n=== å…³é”®ç‚¹è¿‡æ»¤æµç¨‹ ===")
    print(f"0. åˆå§‹å…³é”®ç‚¹: {len(filtered)} ä¸ª")

    # 0. å…ˆæŒ‰ç±»å‹å½’ä¸€åŒ–å¼ºåº¦
    if use_normalized_intensity:
        filtered = normalize_intensity_by_type(filtered)
        intensity_key = 'normalized_intensity'
    else:
        intensity_key = 'intensity'
        # ç¡®ä¿æ‰€æœ‰ç‚¹éƒ½æœ‰ normalized_intensity å­—æ®µï¼ˆè®¾ä¸ºåŸå§‹å€¼ï¼‰
        for kp in filtered:
            kp['normalized_intensity'] = kp['intensity']

    # 1. æŒ‰å¼ºåº¦ç™¾åˆ†ä½æ•°è¿‡æ»¤ï¼ˆä½¿ç”¨å½’ä¸€åŒ–å¼ºåº¦ï¼‰
    if energy_percentile > 0 and filtered:
        before_percentile = len(filtered)
        intensities = [kp[intensity_key] for kp in filtered]
        threshold = np.percentile(intensities, energy_percentile)
        filtered = [kp for kp in filtered if kp[intensity_key] >= threshold]
        print(f"3. å¼ºåº¦ç™¾åˆ†ä½è¿‡æ»¤å: {len(filtered)} ä¸ª "
              f"(å‡å°‘ {before_percentile - len(filtered)} ä¸ª, percentile={energy_percentile}, threshold={threshold:.3f})")

    # 2. æŒ‰æœ€å°é—´éš”è¿‡æ»¤ï¼ˆåœ¨æ¯ä¸ªé—´éš”å†…åªä¿ç•™æœ€å¼ºçš„ç‚¹ï¼‰
    if min_interval > 0 and filtered:
        before_interval = len(filtered)
        filtered.sort(key=lambda x: x['time'])
        interval_filtered = []
        current_interval_start = filtered[0]['time']
        current_best = filtered[0]

        for kp in filtered[1:]:
            if kp['time'] - current_interval_start < min_interval:
                # åœ¨åŒä¸€é—´éš”å†…ï¼Œä¿ç•™å¼ºåº¦æ›´é«˜çš„ï¼ˆä½¿ç”¨å½’ä¸€åŒ–å¼ºåº¦æ¯”è¾ƒï¼‰
                if kp[intensity_key] > current_best[intensity_key]:
                    current_best = kp
            else:
                # æ–°é—´éš”ï¼Œä¿å­˜ä¹‹å‰çš„æœ€ä½³ç‚¹
                interval_filtered.append(current_best)
                current_interval_start = kp['time']
                current_best = kp

        # æ·»åŠ æœ€åä¸€ä¸ª
        interval_filtered.append(current_best)
        filtered = interval_filtered
        print(f"3. æœ€å°é—´éš”è¿‡æ»¤å: {len(filtered)} ä¸ª (å‡å°‘ {before_interval - len(filtered)} ä¸ª, min_interval={min_interval}s)")

    # 4. åªä¿ç•™ top_k ä¸ªï¼ˆæ¯ç§ç±»å‹åˆ†åˆ«ä¿ç•™ top_k ä¸ªï¼‰
    if top_k > 0 and filtered:
        # æŒ‰ç±»å‹åˆ†ç»„
        by_type = {}
        for kp in filtered:
            kp_type = kp.get('type', 'Unknown')
            if kp_type not in by_type:
                by_type[kp_type] = []
            by_type[kp_type].append(kp)
        
        before_topk = len(filtered)
        
        # æ¯ç§ç±»å‹åˆ†åˆ«ä¿ç•™ top_k ä¸ªæœ€å¼ºçš„ç‚¹
        filtered_by_type = []
        type_summary_list = []
        for type_name, points in by_type.items():
            # æŒ‰å¼ºåº¦é™åºæ’åº
            points.sort(key=lambda x: x[intensity_key], reverse=True)
            # æ¯ç§ç±»å‹ä¿ç•™ top_k ä¸ªï¼ˆå¦‚æœè¯¥ç±»å‹ç‚¹æ•°å°‘äº top_kï¼Œåˆ™å…¨éƒ¨ä¿ç•™ï¼‰
            kept = min(top_k, len(points))
            filtered_by_type.extend(points[:kept])
            type_summary_list.append(f"{type_name}:{kept}")
        
        filtered = filtered_by_type
        filtered.sort(key=lambda x: x['time'])
        
        # æ‰“å°æ¯ç§ç±»å‹ä¿ç•™çš„æ•°é‡
        type_summary = ", ".join(sorted(type_summary_list))
        print(f"5. Top-K è¿‡æ»¤å: {len(filtered)} ä¸ª (å‡å°‘ {before_topk - len(filtered)} ä¸ª, "
              f"æ¯ç±»ä¿ç•™top_k={top_k}, æŒ‰ç±»å‹: {type_summary})")

    print(f"=== è¿‡æ»¤å®Œæˆï¼Œæœ€ç»ˆä¿ç•™ {len(filtered)} ä¸ªå…³é”®ç‚¹ ===\n")
    return filtered


def parse_time_str(time_str: str) -> float:
    """
    è§£ææ—¶é—´å­—ç¬¦ä¸²ä¸ºç§’æ•°
    æ”¯æŒæ ¼å¼: "MM:SS" æˆ– "HH:MM:SS" æˆ–ç›´æ¥æ•°å­—
    """
    if isinstance(time_str, (int, float)):
        return float(time_str)
    
    time_str = str(time_str).strip()
    parts = time_str.split(':')
    
    if len(parts) == 1:
        return float(parts[0])
    elif len(parts) == 2:
        return int(parts[0]) * 60 + float(parts[1])
    elif len(parts) == 3:
        return int(parts[0]) * 3600 + int(parts[1]) * 60 + float(parts[2])
    else:
        raise ValueError(f"æ— æ³•è§£ææ—¶é—´æ ¼å¼: {time_str}")


def load_sections_from_caption(caption_path: str) -> List[dict]:
    """
    ä» caption JSON æ–‡ä»¶åŠ è½½ sections ä¿¡æ¯
    
    Args:
        caption_path: caption.json æ–‡ä»¶è·¯å¾„
    
    Returns:
        sections åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« name, start_time, end_time
    """
    with open(caption_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    sections = []
    for sec in data.get('sections', []):
        try:
            start_time = parse_time_str(sec.get('Start_Time', 0))
            end_time = parse_time_str(sec.get('End_Time', 0))
            name = sec.get('name', 'Unknown')
            
            if end_time > start_time:
                sections.append({
                    'name': name,
                    'start_time': start_time,
                    'end_time': end_time,
                    'duration': end_time - start_time
                })
        except Exception as e:
            print(f"    âš ï¸ è§£æ section å¤±è´¥: {sec.get('name', 'Unknown')} - {e}")
    
    return sections


def filter_by_type(
    keypoints: List[dict],
    preferred_types: List[str] = None,
    mode: str = "boost",
    boost_factor: float = 1.5
) -> List[dict]:
    """
    æŒ‰å…³é”®ç‚¹ç±»å‹è¿›è¡Œè¿‡æ»¤æˆ–å¢å¼º

    å…³é”®ç‚¹ç±»å‹åŒ…æ‹¬:
    - "Downbeat" (é‡æ‹): èŠ‚å¥ä¸Šçš„å¼ºæ‹

    Args:
        keypoints: åŸå§‹å…³é”®ç‚¹åˆ—è¡¨
        preferred_types: ä¼˜å…ˆç±»å‹åˆ—è¡¨ï¼Œæ”¯æŒéƒ¨åˆ†åŒ¹é…
                        ä¾‹å¦‚ ["Downbeat"] ä¼šåŒ¹é…åŒ…å«è¿™äº›è¯çš„ç±»å‹
        mode: è¿‡æ»¤æ¨¡å¼
              - "only": åªä¿ç•™æŒ‡å®šç±»å‹çš„å…³é”®ç‚¹
              - "boost": å¢å¼ºæŒ‡å®šç±»å‹çš„æƒé‡ï¼ˆä¹˜ä»¥ boost_factorï¼‰
              - "exclude": æ’é™¤æŒ‡å®šç±»å‹
        boost_factor: å½“ mode="boost" æ—¶ï¼Œå¢å¼ºå› å­ï¼ˆé»˜è®¤ 1.5ï¼‰

    Returns:
        è¿‡æ»¤æˆ–å¢å¼ºåçš„å…³é”®ç‚¹åˆ—è¡¨
    """
    if not keypoints:
        return []

    if not preferred_types:
        return keypoints

    # å°† preferred_types è½¬ä¸ºå°å†™ä»¥ä¾¿åŒ¹é…
    preferred_lower = [t.lower() for t in preferred_types]

    def type_matches(kp_type: str, preferred_list: List[str]) -> bool:
        """æ£€æŸ¥å…³é”®ç‚¹ç±»å‹æ˜¯å¦åŒ¹é…ä¼˜å…ˆç±»å‹åˆ—è¡¨"""
        kp_type_lower = kp_type.lower()
        for preferred in preferred_list:
            if preferred in kp_type_lower:
                return True
        return False

    filtered = []

    print(f"\n    ğŸ·ï¸  æŒ‰ç±»å‹è¿‡æ»¤å…³é”®ç‚¹ (mode={mode}):")
    print(f"       ä¼˜å…ˆç±»å‹: {preferred_types}")

    # ç»Ÿè®¡å„ç±»å‹æ•°é‡
    type_counts_before = {}
    for kp in keypoints:
        kp_type = kp.get('type', 'Unknown')
        type_counts_before[kp_type] = type_counts_before.get(kp_type, 0) + 1

    if mode == "only":
        # åªä¿ç•™æŒ‡å®šç±»å‹
        for kp in keypoints:
            kp_type = kp.get('type', 'Unknown')
            if type_matches(kp_type, preferred_lower):
                filtered.append(kp)

    elif mode == "exclude":
        # æ’é™¤æŒ‡å®šç±»å‹
        for kp in keypoints:
            kp_type = kp.get('type', 'Unknown')
            if not type_matches(kp_type, preferred_lower):
                filtered.append(kp)

    elif mode == "boost":
        # å¢å¼ºæŒ‡å®šç±»å‹çš„æƒé‡
        for kp in keypoints:
            kp_copy = dict(kp)
            kp_type = kp_copy.get('type', 'Unknown')
            if type_matches(kp_type, preferred_lower):
                # å¢å¼ºå¼ºåº¦
                if 'normalized_intensity' in kp_copy:
                    kp_copy['normalized_intensity'] = min(1.0, kp_copy['normalized_intensity'] * boost_factor)
                if 'intensity' in kp_copy:
                    kp_copy['intensity'] = kp_copy['intensity'] * boost_factor
                kp_copy['type_boosted'] = True
            filtered.append(kp_copy)

    else:
        print(f"       âš ï¸ æœªçŸ¥æ¨¡å¼ '{mode}'ï¼Œè¿”å›åŸå§‹å…³é”®ç‚¹")
        return keypoints

    # ç»Ÿè®¡è¿‡æ»¤åå„ç±»å‹æ•°é‡
    type_counts_after = {}
    for kp in filtered:
        kp_type = kp.get('type', 'Unknown')
        type_counts_after[kp_type] = type_counts_after.get(kp_type, 0) + 1

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    for kp_type, count_before in type_counts_before.items():
        count_after = type_counts_after.get(kp_type, 0)
        if count_before != count_after:
            print(f"       - {kp_type}: {count_before} -> {count_after}")
        elif mode == "boost" and type_matches(kp_type, preferred_lower):
            print(f"       - {kp_type}: {count_before} (æƒé‡å¢å¼º x{boost_factor})")

    print(f"       è¿‡æ»¤å‰: {len(keypoints)} ä¸ª, è¿‡æ»¤å: {len(filtered)} ä¸ª")

    return filtered


def compute_composite_score(
    keypoints: List[dict],
    weight_downbeat: float = 1.0,
    weight_pitch: float = 1.0,
    weight_mel_energy: float = 1.0,
) -> List[dict]:
    """
    ä¸ºæ¯ä¸ªå…³é”®ç‚¹è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆåŠ æƒç»„åˆå¤šä¸ªæŒ‡æ ‡ï¼‰

    composite_score = k1 * downbeat_intensity + k2 * pitch_intensity + k3 * mel_energy_intensity

    æ³¨æ„ï¼šæ¯ä¸ªæŒ‡æ ‡çš„å¼ºåº¦éƒ½å·²åœ¨å„è‡ªç±»å‹å†…å½’ä¸€åŒ–åˆ° [0, 1] èŒƒå›´

    Args:
        keypoints: å…³é”®ç‚¹åˆ—è¡¨ï¼ˆå¿…é¡»å·²ç»è¿‡ normalize_intensity_by_type å¤„ç†ï¼‰
        weight_downbeat: Downbeat ç±»å‹çš„æƒé‡ (k1)
        weight_pitch: Pitch ç±»å‹çš„æƒé‡ (k2)
        weight_mel_energy: Mel Energy ç±»å‹çš„æƒé‡ (k3)

    Returns:
        æ·»åŠ äº† composite_score å­—æ®µçš„å…³é”®ç‚¹åˆ—è¡¨
    """
    if not keypoints:
        return []

    print(f"\n    ğŸ¯ è®¡ç®—ç»¼åˆè¯„åˆ† (æƒé‡: Downbeat={weight_downbeat:.2f}, Pitch={weight_pitch:.2f}, MelEnergy={weight_mel_energy:.2f})")

    # ä¸ºæ¯ä¸ªç‚¹è®¡ç®—ç»¼åˆåˆ†æ•°
    for kp in keypoints:
        kp_type = kp.get('type', 'Unknown')
        normalized_intensity = kp.get('normalized_intensity', 0.5)

        # æ ¹æ®ç±»å‹åº”ç”¨å¯¹åº”çš„æƒé‡
        if kp_type == 'Downbeat':
            composite_score = weight_downbeat * normalized_intensity
        elif kp_type == 'Pitch':
            composite_score = weight_pitch * normalized_intensity
        elif kp_type == 'MelEnergy':
            composite_score = weight_mel_energy * normalized_intensity
        else:
            # æœªçŸ¥ç±»å‹ï¼Œä½¿ç”¨å¹³å‡æƒé‡
            avg_weight = (weight_downbeat + weight_pitch + weight_mel_energy) / 3.0
            composite_score = avg_weight * normalized_intensity

        kp['composite_score'] = composite_score

    # ç»Ÿè®¡æ¯ç§ç±»å‹çš„æ•°é‡å’Œå¹³å‡åˆ†æ•°
    type_stats = {}
    for kp in keypoints:
        kp_type = kp.get('type', 'Unknown')
        if kp_type not in type_stats:
            type_stats[kp_type] = {'count': 0, 'total_score': 0.0}
        type_stats[kp_type]['count'] += 1
        type_stats[kp_type]['total_score'] += kp['composite_score']

    print(f"    å„ç±»å‹ç»¼åˆè¯„åˆ†ç»Ÿè®¡:")
    for kp_type, stats in sorted(type_stats.items()):
        avg_score = stats['total_score'] / stats['count']
        print(f"      - {kp_type}: {stats['count']} ä¸ªç‚¹, å¹³å‡åˆ†æ•°={avg_score:.3f}")

    return keypoints


def filter_by_sections(
    keypoints: List[dict],
    sections: List[dict],
    section_min_interval: float = 0.0,
    use_normalized_intensity: bool = True,
    min_segment_duration: float = 3.0,
    max_segment_duration: float = 15.0,
    total_shots: int = 20,
    audio_duration: float = None,
    weight_downbeat: float = 1.0,
    weight_pitch: float = 1.0,
    weight_mel_energy: float = 1.0,
) -> List[dict]:
    """
    åŸºäºéŸ³ä¹æ®µè½ï¼ˆsectionsï¼‰è¿›è¡Œå…³é”®ç‚¹è¿‡æ»¤ï¼ˆæŒ‰æ¯”ä¾‹åˆ†é…æ¨¡å¼ï¼‰
    æ ¹æ®æ¯ä¸ªæ®µè½çš„å…³é”®ç‚¹å¯†åº¦ï¼ŒæŒ‰æ¯”ä¾‹åˆ†é…æ€»é•œå¤´æ•°

    Args:
        keypoints: åŸå§‹å…³é”®ç‚¹åˆ—è¡¨
        sections: æ®µè½åˆ—è¡¨ï¼Œæ¯ä¸ªåŒ…å« name, start_time, end_time
        section_min_interval: å…¨å±€æœ€å°é—´éš”ï¼ˆè·¨æ®µè½åº”ç”¨ï¼‰
        use_normalized_intensity: æ˜¯å¦ä½¿ç”¨å½’ä¸€åŒ–åçš„å¼ºåº¦è¿›è¡Œè¿‡æ»¤ï¼ˆæ¨èTrueï¼‰
        min_segment_duration: æœ€å°ç‰‡æ®µæ—¶é•¿ï¼ˆç”¨äºè¾¹ç•Œæ£€æŸ¥å’Œåˆå¹¶è¿‡çŸ­ç‰‡æ®µï¼‰
        max_segment_duration: ç”¨äºåˆ†å‰²è¿‡é•¿ç‰‡æ®µçš„æœ€å¤§ç‰‡æ®µæ—¶é•¿ï¼ˆé»˜è®¤15sï¼‰
        total_shots: æ€»é•œå¤´æ•°ï¼ŒæŒ‰æ¯”ä¾‹åˆ†é…ç»™å„sectionï¼ˆåŸºäºå…³é”®ç‚¹å¯†åº¦ï¼‰
        audio_duration: éŸ³é¢‘æ€»æ—¶é•¿ï¼ˆç”¨äºè¾¹ç•Œæ£€æŸ¥ï¼‰
        weight_downbeat: Downbeat ç±»å‹çš„æƒé‡
        weight_pitch: Pitch ç±»å‹çš„æƒé‡
        weight_mel_energy: Mel Energy ç±»å‹çš„æƒé‡

    Returns:
        è¿‡æ»¤åçš„å…³é”®ç‚¹åˆ—è¡¨
    """
    if not keypoints or not sections:
        return keypoints

    # å…ˆæŒ‰ç±»å‹å½’ä¸€åŒ–å¼ºåº¦
    if use_normalized_intensity:
        keypoints = normalize_intensity_by_type(list(keypoints))
        intensity_key = 'normalized_intensity'
    else:
        intensity_key = 'intensity'
        # ç¡®ä¿æ‰€æœ‰ç‚¹éƒ½æœ‰ normalized_intensity å­—æ®µ
        for kp in keypoints:
            if 'normalized_intensity' not in kp:
                kp['normalized_intensity'] = kp['intensity']

    # è®¡ç®—ç»¼åˆè¯„åˆ†ï¼ˆåŠ æƒç»„åˆå¤šä¸ªæŒ‡æ ‡ï¼‰
    keypoints = compute_composite_score(
        keypoints,
        weight_downbeat=weight_downbeat,
        weight_pitch=weight_pitch,
        weight_mel_energy=weight_mel_energy,
    )
    # ä½¿ç”¨ç»¼åˆè¯„åˆ†ä½œä¸ºæ’åºå’Œæ¯”è¾ƒçš„ä¾æ®
    score_key = 'composite_score'

    filtered = []

    print(f"\n    ğŸ“‚ åŸºäº {len(sections)} ä¸ªéŸ³ä¹æ®µè½è¿›è¡Œè¿‡æ»¤:")
    print(f"    ğŸ¯ æŒ‰æ¯”ä¾‹åˆ†é…æ€»é•œå¤´æ•°: {total_shots}")

    # è®¡ç®—æ¯ä¸ªsectionåº”è¯¥ä¿ç•™çš„å…³é”®ç‚¹æ•°é‡ï¼ˆæŒ‰æ¯”ä¾‹åˆ†é…ï¼‰
    section_top_k_map = {}  # å­˜å‚¨æ¯ä¸ªsectionåº”è¯¥ä¿ç•™çš„å…³é”®ç‚¹æ•°é‡

    # Step 1: ç»Ÿè®¡æ¯ä¸ªsectionå†…çš„keypointæ•°é‡
    section_keypoint_counts = {}
    total_keypoints = 0

    for sec in sections:
        start_val = sec.get('start_time', sec.get('Start_Time', 0))
        end_val = sec.get('end_time', sec.get('End_Time', 0))

        try:
            start = parse_time_str(start_val)
            end = parse_time_str(end_val)
        except Exception as e:
            continue

        section_name = sec.get('name', 'Unknown')
        section_points = [kp for kp in keypoints if start <= kp['time'] < end]
        section_keypoint_counts[section_name] = len(section_points)
        total_keypoints += len(section_points)

    # Step 2: è®¡ç®—æ¯”ä¾‹å¹¶åˆ†é…é•œå¤´æ•°
    if total_keypoints > 0:
        print(f"    ğŸ“Š æ€»å…³é”®ç‚¹æ•°: {total_keypoints}")
        allocated_shots = 0

        for section_name, count in section_keypoint_counts.items():
            ratio = count / total_keypoints
            allocated = max(1, round(total_shots * ratio))  # è‡³å°‘ä¿ç•™1ä¸ª
            section_top_k_map[section_name] = allocated
            allocated_shots += allocated
            print(f"       [{section_name}] å…³é”®ç‚¹: {count} ({ratio*100:.1f}%) -> åˆ†é…é•œå¤´æ•°: {allocated}")

        # Step 3: å¦‚æœåˆ†é…æ€»æ•°ä¸ç­‰äº total_shotsï¼Œè°ƒæ•´æœ€å¤§sectionçš„æ•°é‡
        if allocated_shots != total_shots:
            diff = total_shots - allocated_shots
            # æ‰¾åˆ°keypointæ•°é‡æœ€å¤šçš„section
            max_section = max(section_keypoint_counts, key=section_keypoint_counts.get)
            section_top_k_map[max_section] += diff
            print(f"    âš–ï¸  è°ƒæ•´ [{max_section}]: {section_top_k_map[max_section] - diff} -> {section_top_k_map[max_section]} (è¡¥å¿å·®å€¼: {diff})")

    for sec in sections:
        name = sec.get('name', 'Unknown')

        # å…¼å®¹ä¸åŒçš„é”®åå’Œæ—¶é—´æ ¼å¼
        start_val = sec.get('start_time', sec.get('Start_Time', 0))
        end_val = sec.get('end_time', sec.get('End_Time', 0))

        try:
            start = parse_time_str(start_val)
            end = parse_time_str(end_val)
        except Exception as e:
            print(f"       âš ï¸ è·³è¿‡æ— æ•ˆæ—¶é—´æ®µ: {name} ({start_val}-{end_val}) - {e}")
            continue

        duration = sec.get('duration', end - start)

        # ä½¿ç”¨æŒ‰æ¯”ä¾‹åˆ†é…çš„é•œå¤´æ•°
        actual_top_k = section_top_k_map.get(name, 0)

        if actual_top_k == 0:
            # å¦‚æœè¯¥ section æ²¡æœ‰åˆ†é…åˆ°é•œå¤´ï¼ˆå¯èƒ½æ˜¯å› ä¸ºæ²¡æœ‰å…³é”®ç‚¹ï¼‰ï¼Œè·³è¿‡
            continue

        # è·å–è¯¥æ®µè½å†…çš„æ‰€æœ‰å…³é”®ç‚¹
        section_points = [kp for kp in keypoints
                         if start <= kp['time'] < end]

        if not section_points:
            print(f"       [{name}] {start:.1f}s-{end:.1f}s: æ— å…³é”®ç‚¹")
            continue

        # æŒ‰æ¯”ä¾‹åˆ†é…æ¨¡å¼ï¼šç›´æ¥æŒ‰ç»¼åˆè¯„åˆ†æ’åºå– top_k
        section_points.sort(key=lambda x: x[score_key], reverse=True)
        selected = section_points[:actual_top_k]

        # ä¸ºé€‰ä¸­çš„ç‚¹æ·»åŠ æ®µè½ä¿¡æ¯
        for pt in selected:
            pt['section'] = name

        filtered.extend(selected)

        # æ‰“å°ä¿ç•™ä¿¡æ¯
        print(f"       [{name}] {start:.1f}s-{end:.1f}s ({duration:.1f}s): "
              f"ä¿ç•™ {len(selected)}/{len([kp for kp in keypoints if start <= kp['time'] < end])} ä¸ªç‚¹"
              f" (åˆ†é…: {actual_top_k})")

    # æŒ‰æ—¶é—´æ’åº
    filtered.sort(key=lambda x: x['time'])

    print(f"    æ®µè½è¿‡æ»¤åå…±: {len(filtered)} ä¸ªå…³é”®ç‚¹")

    # 4. å…¨å±€ min_interval è¿‡æ»¤ï¼ˆè·¨æ®µè½åº”ç”¨ï¼‰
    # è¿™ä¸€æ­¥ç¡®ä¿å³ä½¿æ¥è‡ªä¸åŒ section çš„ç‚¹ä¹Ÿæ»¡è¶³æœ€å°é—´éš”è¦æ±‚
    if section_min_interval > 0 and len(filtered) > 1:
        before_global = len(filtered)
        global_filtered = []
        current_start = filtered[0]['time']
        current_best = filtered[0]

        for kp in filtered[1:]:
            if kp['time'] - current_start < section_min_interval:
                # åœ¨é—´éš”å†…ï¼Œä¿ç•™ç»¼åˆè¯„åˆ†æ›´é«˜çš„
                if kp[score_key] > current_best[score_key]:
                    current_best = kp
            else:
                # æ–°é—´éš”ï¼Œä¿å­˜ä¹‹å‰çš„æœ€ä½³ç‚¹
                global_filtered.append(current_best)
                current_start = kp['time']
                current_best = kp
        
        # æ·»åŠ æœ€åä¸€ä¸ª
        global_filtered.append(current_best)
        filtered = global_filtered

        if len(filtered) < before_global:
            print(f"    å…¨å±€ min_interval è¿‡æ»¤: {before_global} -> {len(filtered)} ä¸ªå…³é”®ç‚¹ "
                  f"(ç§»é™¤äº† {before_global - len(filtered)} ä¸ªè·¨æ®µè½çš„è¿‡è¿‘ç‚¹)")

    # 5. ç¡®ä¿ç‰‡æ®µé•¿åº¦ä¸è¶…è¿‡ max_segment_duration
    # å¦‚æœç›¸é‚»ä¸¤ä¸ªå…³é”®ç‚¹ä¹‹é—´çš„è·ç¦»è¶…è¿‡ max_segment_durationï¼Œä»åŸå§‹ keypoints ä¸­æ‰¾æœ€è¿‘çš„ç‚¹å¡«å……
    if max_segment_duration > 0 and len(filtered) > 1:
        before_split = len(filtered)
        filtered.sort(key=lambda x: x['time'])
        
        # è®°å½•å·²ç»åœ¨ filtered ä¸­çš„ç‚¹çš„æ—¶é—´ï¼ˆç”¨äºå¿«é€ŸæŸ¥æ‰¾ï¼‰
        filtered_times = set(kp['time'] for kp in filtered)
        
        # æ„å»ºåŸå§‹ keypoints çš„æ—¶é—´ç´¢å¼•ï¼ˆæ’é™¤å·²ç»åœ¨ filtered ä¸­çš„ï¼‰
        available_keypoints = [kp for kp in keypoints if kp['time'] not in filtered_times]
        available_keypoints.sort(key=lambda x: x['time'])
        
        split_filtered = []
        
        for i in range(len(filtered)):
            split_filtered.append(filtered[i])
            
            # æ£€æŸ¥ä¸ä¸‹ä¸€ä¸ªç‚¹çš„è·ç¦»
            if i < len(filtered) - 1:
                current_time = filtered[i]['time']
                next_time = filtered[i + 1]['time']
                gap = next_time - current_time
                
                # å¦‚æœé—´éš”è¶…è¿‡ max_segment_durationï¼Œä»åŸå§‹ç‚¹ä¸­æ‰¾åˆé€‚çš„ç‚¹å¡«å……
                if gap > max_segment_duration:
                    # è®¡ç®—éœ€è¦æ’å…¥å¤šå°‘ä¸ªç‚¹
                    num_splits = int(np.ceil(gap / max_segment_duration)) - 1
                    
                    # æ‰¾åˆ°ä½äºè¿™ä¸ªé—´éš”å†…çš„æ‰€æœ‰å¯ç”¨åŸå§‹ç‚¹
                    candidates = [kp for kp in available_keypoints 
                                 if current_time < kp['time'] < next_time]
                    
                    if candidates:
                        # è®¡ç®—ç†æƒ³æ’å…¥ä½ç½®
                        ideal_positions = []
                        for j in range(1, num_splits + 1):
                            ideal_time = current_time + (gap * j / (num_splits + 1))
                            ideal_positions.append(ideal_time)
                        
                        # ä¸ºæ¯ä¸ªç†æƒ³ä½ç½®æ‰¾åˆ°æœ€æ¥è¿‘çš„å€™é€‰ç‚¹
                        selected = []
                        used_indices = set()
                        
                        for ideal_time in ideal_positions:
                            # æ‰¾åˆ°æœ€æ¥è¿‘ç†æƒ³æ—¶é—´çš„å€™é€‰ç‚¹ï¼ˆæœªè¢«ä½¿ç”¨çš„ï¼‰
                            best_candidate = None
                            best_distance = float('inf')
                            best_idx = -1
                            
                            for idx, candidate in enumerate(candidates):
                                if idx in used_indices:
                                    continue
                                distance = abs(candidate['time'] - ideal_time)
                                if distance < best_distance:
                                    best_distance = distance
                                    best_candidate = candidate
                                    best_idx = idx
                            
                            if best_candidate:
                                selected.append(best_candidate)
                                used_indices.add(best_idx)
                        
                        # æŒ‰æ—¶é—´æ’åºå¹¶æ·»åŠ é€‰ä¸­çš„ç‚¹
                        selected.sort(key=lambda x: x['time'])
                        split_filtered.extend(selected)
        
        filtered = split_filtered
        filtered.sort(key=lambda x: x['time'])

        if len(filtered) > before_split:
            print(f"    æœ€å¤§ç‰‡æ®µé™åˆ¶: ä»åŸå§‹ç‚¹ä¸­è¡¥å……äº† {len(filtered) - before_split} ä¸ªå…³é”®ç‚¹ "
                  f"(max_segment={max_segment_duration}s)")

    # 6. è¾¹ç•Œæ£€æŸ¥å’ŒçŸ­ç‰‡æ®µå¤„ç†
    # ç¡®ä¿ç¬¬ä¸€ä¸ªå…³é”®ç‚¹ä¸ä¼šå¤ªæ—©ï¼Œæœ€åä¸€ä¸ªå…³é”®ç‚¹ä¸ä¼šå¤ªæ™š
    if min_segment_duration > 0 and len(filtered) > 0:
        before_boundary = len(filtered)
        boundary_filtered = []

        # å¦‚æœæä¾›äº†audio_durationï¼Œè¿›è¡Œè¾¹ç•Œæ£€æŸ¥
        if audio_duration and audio_duration > 0:
            for i, kp in enumerate(filtered):
                t = kp['time']

                # æ£€æŸ¥ç¬¬ä¸€ä¸ªå…³é”®ç‚¹
                if i == 0:
                    if t < min_segment_duration:
                        # ç¬¬ä¸€ä¸ªç‚¹å¤ªæ—©ï¼Œè·³è¿‡ï¼ˆä»0åˆ°è¿™ä¸ªç‚¹çš„ç‰‡æ®µå¤ªçŸ­ï¼‰
                        print(f"    âš ï¸  è·³è¿‡ç¬¬ä¸€ä¸ªå…³é”®ç‚¹ {t:.2f}s (< min_segment={min_segment_duration}s)")
                        continue

                # æ£€æŸ¥æœ€åä¸€ä¸ªå…³é”®ç‚¹
                if i == len(filtered) - 1:
                    remaining = audio_duration - t
                    if remaining < min_segment_duration:
                        # æœ€åä¸€ä¸ªç‚¹å¤ªæ™šï¼Œè·³è¿‡ï¼ˆä»è¿™ä¸ªç‚¹åˆ°ç»“å°¾çš„ç‰‡æ®µå¤ªçŸ­ï¼‰
                        print(f"    âš ï¸  è·³è¿‡æœ€åä¸€ä¸ªå…³é”®ç‚¹ {t:.2f}s (å‰©ä½™ {remaining:.2f}s < min_segment={min_segment_duration}s)")
                        continue

                # æ£€æŸ¥ç›¸é‚»å…³é”®ç‚¹ä¹‹é—´çš„é—´éš”
                if len(boundary_filtered) > 0:
                    prev_t = boundary_filtered[-1]['time']
                    gap = t - prev_t

                    if gap < min_segment_duration:
                        # ç›¸é‚»ç‚¹é—´éš”å¤ªå°ï¼Œä¿ç•™ç»¼åˆè¯„åˆ†æ›´é«˜çš„
                        if kp.get(score_key, 0) > boundary_filtered[-1].get(score_key, 0):
                            # å½“å‰ç‚¹è¯„åˆ†æ›´é«˜ï¼Œæ›¿æ¢å‰ä¸€ä¸ªç‚¹
                            boundary_filtered[-1] = kp
                            print(f"    âš ï¸  åˆå¹¶çŸ­ç‰‡æ®µ: [{prev_t:.2f}s - {t:.2f}s] ({gap:.2f}s < {min_segment_duration}s), ä¿ç•™è¯„åˆ†æ›´é«˜çš„ç‚¹")
                        # else: ä¿ç•™å‰ä¸€ä¸ªç‚¹ï¼Œè·³è¿‡å½“å‰ç‚¹
                        continue

                boundary_filtered.append(kp)
        else:
            # æ²¡æœ‰audio_durationï¼Œåªåšç›¸é‚»ç‚¹é—´éš”æ£€æŸ¥
            for i, kp in enumerate(filtered):
                if len(boundary_filtered) > 0:
                    prev_t = boundary_filtered[-1]['time']
                    gap = kp['time'] - prev_t

                    if gap < min_segment_duration:
                        # ç›¸é‚»ç‚¹é—´éš”å¤ªå°ï¼Œä¿ç•™ç»¼åˆè¯„åˆ†æ›´é«˜çš„
                        if kp.get(score_key, 0) > boundary_filtered[-1].get(score_key, 0):
                            boundary_filtered[-1] = kp
                        continue

                boundary_filtered.append(kp)

        filtered = boundary_filtered

        if len(filtered) < before_boundary:
            removed = before_boundary - len(filtered)
            print(f"    è¾¹ç•Œå’ŒçŸ­ç‰‡æ®µå¤„ç†: {before_boundary} -> {len(filtered)} ä¸ªå…³é”®ç‚¹ "
                  f"(ç§»é™¤/åˆå¹¶äº† {removed} ä¸ªç‚¹, min_segment={min_segment_duration}s)")

    return filtered


def main():
    parser = argparse.ArgumentParser(
        description='éŸ³é¢‘å…³é”®ç‚¹æ£€æµ‹ - æ”¯æŒDownbeat/Pitch/MelEnergy',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # Downbeatæ£€æµ‹ï¼ˆé»˜è®¤ï¼‰
  python audio_Madmom.py audio.wav

  # Pitchæ£€æµ‹
  python audio_Madmom.py audio.wav --method pitch --pitch-min-distance 0.3 --pitch-threshold 0.8

  # Melèƒ½é‡æ£€æµ‹
  python audio_Madmom.py audio.wav --method mel_energy --mel-min-distance 0.5 --mel-threshold 0.3

  # æ£€æµ‹3/4æ‹æˆ–4/4æ‹çš„éŸ³ä¹
  python audio_Madmom.py audio.wav --method downbeat --beats-per-bar 3 4
        """
    )
    parser.add_argument('audio_path', type=str, help='éŸ³é¢‘æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--method', type=str, default='downbeat', 
                        choices=['downbeat', 'pitch', 'mel_energy'],
                        help='æ£€æµ‹æ–¹æ³• (default: downbeat)')
    
    # === Downbeat/DBN èŠ‚æ‹æ£€æµ‹å‚æ•° ===
    beat_group = parser.add_argument_group('DBNèŠ‚æ‹æ£€æµ‹å‚æ•°')
    beat_group.add_argument('--beats-per-bar', type=int, nargs='+', default=[4],
                        help='æ¯å°èŠ‚çš„æ‹æ•°ï¼Œå¯æŒ‡å®šå¤šä¸ªå€¼å¦‚"3 4"åŒæ—¶æ£€æµ‹3/4å’Œ4/4æ‹ï¼Œé»˜è®¤[4]')
    beat_group.add_argument('--min-bpm', type=float, default=55.0,
                        help='æœ€å°BPMï¼Œé»˜è®¤55.0')
    beat_group.add_argument('--max-bpm', type=float, default=215.0,
                        help='æœ€å¤§BPMï¼Œé»˜è®¤215.0')
    beat_group.add_argument('--num-tempi', type=int, default=60,
                        help='å»ºæ¨¡çš„é€Ÿåº¦æ•°é‡ï¼Œé»˜è®¤60')
    beat_group.add_argument('--transition-lambda', type=float, default=100,
                        help='é€Ÿåº¦å˜åŒ–åˆ†å¸ƒå‚æ•°ï¼Œå€¼è¶Šå¤§è¶Šå€¾å‘ä¿æŒæ’å®šé€Ÿåº¦ï¼Œé»˜è®¤100')
    beat_group.add_argument('--observation-lambda', type=int, default=16,
                        help='å°†ä¸€ä¸ªèŠ‚æ‹å‘¨æœŸåˆ†æˆçš„éƒ¨åˆ†æ•°ï¼Œé»˜è®¤16')
    beat_group.add_argument('--dbn-threshold', type=float, default=0.05,
                        help='DBNæ¿€æ´»å€¼é˜ˆå€¼ï¼Œé»˜è®¤0.05')
    beat_group.add_argument('--no-correct-beats', action='store_true',
                        help='ä¸å¯¹é½èŠ‚æ‹åˆ°æœ€è¿‘çš„æ¿€æ´»å³°å€¼')
    beat_group.add_argument('--fps', type=int, default=100,
                        help='å¸§ç‡(ç”¨äºèŠ‚æ‹æ£€æµ‹)ï¼Œé»˜è®¤100')
    
    # === Pitchæ£€æµ‹å‚æ•° ===
    pitch_group = parser.add_argument_group('Pitchæ£€æµ‹å‚æ•°')
    pitch_group.add_argument('--pitch-tolerance', type=float, default=0.8,
                        help='Pitchæ£€æµ‹å®¹å·®ï¼Œé»˜è®¤0.8')
    pitch_group.add_argument('--pitch-threshold', type=float, default=0.8,
                        help='Pitchç½®ä¿¡åº¦é˜ˆå€¼ï¼Œé»˜è®¤0.8')
    pitch_group.add_argument('--pitch-min-distance', type=float, default=0.5,
                        help='Pitchæ£€æµ‹çš„æœ€å°é—´éš”(ç§’)ï¼Œé»˜è®¤0.5')
    pitch_group.add_argument('--pitch-nms', type=str, default='basic',
                        choices=['basic', 'adaptive', 'window'],
                        help='Pitch NMSæ–¹æ³•ï¼Œé»˜è®¤basic')
    pitch_group.add_argument('--pitch-max-points', type=int, default=20,
                        help='Pitchæœ€å¤§ä¿ç•™ç‚¹æ•°ï¼Œé»˜è®¤20')
    
    # === Melèƒ½é‡æ£€æµ‹å‚æ•° ===
    mel_group = parser.add_argument_group('Melèƒ½é‡æ£€æµ‹å‚æ•°')
    mel_group.add_argument('--mel-win-size', type=int, default=512,
                        help='FFTçª—å£å¤§å°ï¼Œé»˜è®¤512')
    mel_group.add_argument('--mel-n-filters', type=int, default=40,
                        help='Melæ»¤æ³¢å™¨æ•°é‡ï¼Œé»˜è®¤40')
    mel_group.add_argument('--mel-threshold', type=float, default=0.3,
                        help='èƒ½é‡é˜ˆå€¼æ¯”ä¾‹(0-1)ï¼Œé»˜è®¤0.3')
    mel_group.add_argument('--mel-min-distance', type=float, default=0.5,
                        help='Melèƒ½é‡æ£€æµ‹çš„æœ€å°é—´éš”(ç§’)ï¼Œé»˜è®¤0.5')
    mel_group.add_argument('--mel-nms', type=str, default='basic',
                        choices=['basic', 'adaptive', 'window'],
                        help='Mel NMSæ–¹æ³•ï¼Œé»˜è®¤basic')
    mel_group.add_argument('--mel-max-points', type=int, default=20,
                        help='Melæœ€å¤§ä¿ç•™ç‚¹æ•°ï¼Œé»˜è®¤20')
    
    # === æ˜¾è‘—æ€§è¿‡æ»¤å‚æ•° ===
    filter_group = parser.add_argument_group('æ˜¾è‘—æ€§è¿‡æ»¤å‚æ•°')
    filter_group.add_argument('--min-interval', type=float, default=0.0,
                        help='å…³é”®ç‚¹ä¹‹é—´çš„æœ€å°é—´éš”(ç§’)ï¼Œé—´éš”å†…åªä¿ç•™æœ€å¼ºçš„ç‚¹ï¼Œé»˜è®¤0.0ï¼ˆä¸è¿‡æ»¤ï¼‰')
    filter_group.add_argument('--top-k', type=int, default=0,
                        help='åªä¿ç•™å¼ºåº¦æœ€é«˜çš„å‰Kä¸ªå…³é”®ç‚¹ï¼Œé»˜è®¤0ï¼ˆä¸é™åˆ¶ï¼‰')
    filter_group.add_argument('--energy-percentile', type=float, default=0.0,
                        help='åªä¿ç•™èƒ½é‡é«˜äºè¯¥ç™¾åˆ†ä½æ•°çš„ç‚¹(0-100)ï¼Œé»˜è®¤0ï¼ˆä¸è¿‡æ»¤ï¼‰')
    
    # === åŸºäº Caption æ®µè½è¿‡æ»¤å‚æ•° ===
    caption_group = parser.add_argument_group('åŸºäºCaptionæ®µè½è¿‡æ»¤å‚æ•°')
    caption_group.add_argument('--caption', type=str, default=None,
                        help='caption.json æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºè¯»å–éŸ³ä¹æ®µè½(sections)åˆ’åˆ†')
    caption_group.add_argument('--section-top-k', type=int, default=0,
                        help='æ¯ä¸ªéŸ³ä¹æ®µè½å†…ä¿ç•™çš„æœ€å¼ºç‚¹æ•°é‡ï¼Œé»˜è®¤0')
    caption_group.add_argument('--section-min-interval', type=float, default=0.0,
                        help='æ¯ä¸ªéŸ³ä¹æ®µè½å†…çš„æœ€å°é—´éš”(ç§’)ï¼Œé»˜è®¤0ï¼ˆä¸é™åˆ¶ï¼‰')
    caption_group.add_argument('--section-energy-percentile', type=float, default=0.0,
                        help='æ¯ä¸ªéŸ³ä¹æ®µè½å†…çš„å¼ºåº¦ç™¾åˆ†ä½æ•°é˜ˆå€¼(0-100)ï¼Œåªä¿ç•™é«˜äºè¯¥é˜ˆå€¼çš„ç‚¹ï¼Œé»˜è®¤0ï¼ˆä¸è¿‡æ»¤ï¼‰')

    args = parser.parse_args()

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(args.audio_path):
        print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.audio_path}")
        return

    print(f"\n{'='*60}")
    print(f"ğŸµ éŸ³é¢‘å…³é”®ç‚¹æ£€æµ‹")
    print(f"{'='*60}")

    start_time = time.time()

    try:
        # æ ¹æ®æ£€æµ‹æ–¹æ³•åˆ›å»ºæ£€æµ‹å™¨
        detector = SensoryKeypointDetector(
            detection_method=args.method,
            # Downbeatå‚æ•°
            beats_per_bar=args.beats_per_bar,
            min_bpm=args.min_bpm,
            max_bpm=args.max_bpm,
            num_tempi=args.num_tempi,
            transition_lambda=args.transition_lambda,
            observation_lambda=args.observation_lambda,
            dbn_threshold=args.dbn_threshold,
            correct_beats=not args.no_correct_beats,
            fps=args.fps,
            # Pitchå‚æ•°
            pitch_tolerance=args.pitch_tolerance,
            pitch_threshold=args.pitch_threshold,
            pitch_min_distance=args.pitch_min_distance,
            pitch_nms_method=args.pitch_nms,
            pitch_max_points=args.pitch_max_points,
            # Melå‚æ•°
            mel_win_s=args.mel_win_size,
            mel_n_filters=args.mel_n_filters,
            mel_threshold_ratio=args.mel_threshold,
            mel_min_distance=args.mel_min_distance,
            mel_nms_method=args.mel_nms,
            mel_max_points=args.mel_max_points,
        )
        result = detector.analyze(args.audio_path)

        elapsed_time = time.time() - start_time

        # è¾“å‡ºç»“æœ
        print(f"\n{'='*50}")
        print(f"æ£€æµ‹å®Œæˆ! è€—æ—¶: {elapsed_time:.2f}ç§’")
        print(f"{'='*50}")
        
        print(f"\nğŸ“Š åˆ†ææŠ¥å‘Š:")
        if args.method == "downbeat":
            print(f"  æ£€æµ‹åˆ° {len(result['downbeats'])} ä¸ªå¼ºæ‹")
        elif args.method == "pitch":
            print(f"  æ£€æµ‹åˆ° {len(result['keypoints'])} ä¸ªPitchå…³é”®ç‚¹")
        elif args.method == "mel_energy":
            print(f"  æ£€æµ‹åˆ° {len(result['keypoints'])} ä¸ªMelèƒ½é‡å…³é”®ç‚¹")
        
        print(f"  å…³é”®ç‚¹: {len(result['keypoints'])} ä¸ª")
        
        # å¯¹äºdownbeatæ–¹æ³•åº”ç”¨æ˜¾è‘—æ€§è¿‡æ»¤
        if args.method == "downbeat":
            original_count = len(result['keypoints'])
            need_filter = (args.min_interval > 0 or args.top_k > 0 or 
                           args.energy_percentile > 0)
            
            if need_filter:
                print(f"\nğŸ” åº”ç”¨æ˜¾è‘—æ€§è¿‡æ»¤...")
                filtered_keypoints = filter_significant_keypoints(
                    result['keypoints'],
                    min_interval=args.min_interval,
                    top_k=args.top_k,
                    energy_percentile=args.energy_percentile
                )
                result['keypoints_original'] = result['keypoints']
                result['keypoints'] = filtered_keypoints
                print(f"  è¿‡æ»¤åå…³é”®ç‚¹: {len(filtered_keypoints)} ä¸ª (å‡å°‘äº† {original_count - len(filtered_keypoints)} ä¸ª)")
            
            # åŸºäº Caption æ®µè½è¿‡æ»¤
            if args.caption:
                if os.path.exists(args.caption):
                    print(f"\nğŸ“‚ åŠ è½½ Caption æ®µè½ä¿¡æ¯: {args.caption}")
                    sections = load_sections_from_caption(args.caption)
                    
                    if sections:
                        print(f"  å…±è§£æåˆ° {len(sections)} ä¸ªæ®µè½:")
                        for sec in sections:
                            print(f"    - {sec['name']}: {sec['start_time']:.1f}s - {sec['end_time']:.1f}s")
                        
                        filtered_keypoints = filter_by_sections(
                            result['keypoints'],
                            sections,
                            section_top_k=args.section_top_k,
                            section_min_interval=args.section_min_interval,
                            section_energy_percentile=args.section_energy_percentile
                        )
                        
                        if 'keypoints_original' not in result:
                            result['keypoints_original'] = result['keypoints']
                        result['keypoints'] = filtered_keypoints
                        result['sections'] = sections
                    else:
                        print(f"  âš ï¸ æœªèƒ½ä» caption æ–‡ä»¶è§£æåˆ°æœ‰æ•ˆæ®µè½")
                else:
                    print(f"  âš ï¸ Caption æ–‡ä»¶ä¸å­˜åœ¨: {args.caption}")
        
        print(f"\nå‰ 15 ä¸ªå…³é”®ç‚¹:")
        print(f"{'æ—¶é—´(ç§’)':>10} | {'ç±»å‹':<15} | {'å¼ºåº¦':>6}")
        print("-" * 45)
        for pt in result['keypoints'][:15]:
            print(f"{pt['time']:10.3f} | {pt['type']:<15} | {pt['intensity']:6.2f}")
        
        if len(result['keypoints']) > 15:
            print(f"  ... (å…± {len(result['keypoints'])} ä¸ªå…³é”®ç‚¹)")

    except Exception as e:
        import traceback
        print(f"\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        traceback.print_exc()
        print("\nè¯·ç¡®ä¿å·²å®‰è£… madmom å’Œ ffmpeg")
        return

    return result


# --- ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤ç¤ºä¾‹
    if len(sys.argv) == 1:
        print("ç”¨æ³•: python audio_Madmom.py <éŸ³é¢‘æ–‡ä»¶è·¯å¾„> [é€‰é¡¹]")
        print("ä½¿ç”¨ --help æŸ¥çœ‹è¯¦ç»†å¸®åŠ©")
        sys.exit(0)
    
    main()