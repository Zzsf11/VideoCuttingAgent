========================================================
Loading environment modules...
Environment loaded.

========================================================
LLM Service Starting on Compute Node
Job ID: 5044
Compute Node Hostname: gpuh07
LLM Listening Port: 8735

--- Instructions for connecting from the LOGIN NODE ---
1. Open a NEW terminal on the login node.
2. Create an SSH tunnel with this command:
   ssh -L 8888:localhost:8735 cit_shifangzhao@gpuh07
   (You can replace 8888 with another unused port on the login node if needed)
3. Once the tunnel is active, you can interact with the LLM at:
   API Base URL: http://localhost:8888/v1
========================================================

INFO 12-11 10:42:58 [__init__.py:216] Automatically detected platform cuda.
WARNING 12-11 10:43:25 [__init__.py:1750] argument 'disable_mm_preprocessor_cache' is deprecated
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:43:25 [api_server.py:1839] vLLM API server version 0.11.0rc2.dev113+gf9e714813
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:43:25 [utils.py:233] non-default args: {'model_tag': '/public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-VL-30B-A3B-Instruct', 'port': 8735, 'enable_auto_tool_choice': True, 'tool_call_parser': 'hermes', 'model': '/public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-VL-30B-A3B-Instruct', 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 65536, 'tensor_parallel_size': 2, 'disable_mm_preprocessor_cache': True, 'max_num_seqs': 64}
[1;36m(APIServer pid=714896)[0;0m WARNING 12-11 10:43:25 [__init__.py:2877] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[1;36m(APIServer pid=714896)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[1;36m(APIServer pid=714896)[0;0m WARNING 12-11 10:43:25 [arg_utils.py:970] `--disable-mm-preprocessor-cache` is deprecated and will be removed in v0.13. Please use `--mm-processor-cache-gb 0` instead.
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:43:25 [model.py:551] Resolved architecture: Qwen3VLMoeForConditionalGeneration
[1;36m(APIServer pid=714896)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:43:25 [model.py:1538] Using max model len 65536
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:43:28 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-11 10:43:45 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=722733)[0;0m INFO 12-11 10:44:00 [core.py:648] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=722733)[0;0m INFO 12-11 10:44:01 [core.py:78] Initializing a V1 LLM engine (v0.11.0rc2.dev113+gf9e714813) with config: model='/public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-VL-30B-A3B-Instruct', speculative_config=None, tokenizer='/public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-VL-30B-A3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=65536, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-VL-30B-A3B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=None, compilation_config={'level': 3, 'debug_dump_path': None, 'cache_dir': '', 'backend': '', 'custom_ops': [], 'splitting_ops': ['vllm.unified_attention', 'vllm.unified_attention_with_output', 'vllm.mamba_mixer2', 'vllm.mamba_mixer', 'vllm.short_conv', 'vllm.linear_attention', 'vllm.plamo2_mamba_mixer', 'vllm.gdn_attention', 'vllm.sparse_attn_indexer'], 'use_inductor': True, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'use_cudagraph': True, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], 'cudagraph_copy_inputs': False, 'full_cuda_graph': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_capture_size': 128, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=722733)[0;0m WARNING 12-11 10:44:01 [multiproc_executor.py:720] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
[1;36m(EngineCore_DP0 pid=722733)[0;0m INFO 12-11 10:44:01 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_438258d6'), local_subscribe_addr='ipc:///tmp/687aac1e-7557-43cf-9388-68acacabe0b8', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-11 10:44:13 [__init__.py:216] Automatically detected platform cuda.
INFO 12-11 10:44:13 [__init__.py:216] Automatically detected platform cuda.
INFO 12-11 10:44:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_7355dea0'), local_subscribe_addr='ipc:///tmp/2098c4ae-f9e5-49a6-8f7e-dc3ee1728005', remote_subscribe_addr=None, remote_addr_ipv6=False)
INFO 12-11 10:44:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_aad60ec3'), local_subscribe_addr='ipc:///tmp/4651e34c-15e1-4284-ad50-72e9e4fc98cd', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-11 10:44:33 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-11 10:44:33 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-11 10:44:33 [pynccl.py:104] vLLM is using nccl==2.27.3
INFO 12-11 10:44:33 [pynccl.py:104] vLLM is using nccl==2.27.3
INFO 12-11 10:44:34 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
INFO 12-11 10:44:34 [custom_all_reduce.py:35] Skipping P2P check and trusting the driver's P2P report.
INFO 12-11 10:44:34 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_e23ad28b'), local_subscribe_addr='ipc:///tmp/dea9c3dc-8a6e-4629-9990-6768b052593f', remote_subscribe_addr=None, remote_addr_ipv6=False)
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
[Gloo] Rank 1 is connected to 1 peer ranks. Expected number of connected peer ranks is : 1
INFO 12-11 10:44:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-11 10:44:34 [__init__.py:1384] Found nccl from library libnccl.so.2
INFO 12-11 10:44:34 [pynccl.py:104] vLLM is using nccl==2.27.3
INFO 12-11 10:44:34 [pynccl.py:104] vLLM is using nccl==2.27.3
INFO 12-11 10:44:34 [parallel_state.py:1208] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1
INFO 12-11 10:44:34 [parallel_state.py:1208] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
WARNING 12-11 10:44:34 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
WARNING 12-11 10:44:34 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(Worker_TP1 pid=724732)[0;0m INFO 12-11 10:44:37 [gpu_model_runner.py:2707] Starting to load model /public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-VL-30B-A3B-Instruct...
[1;36m(Worker_TP0 pid=724731)[0;0m INFO 12-11 10:44:37 [gpu_model_runner.py:2707] Starting to load model /public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-VL-30B-A3B-Instruct...
[1;36m(Worker_TP1 pid=724732)[0;0m INFO 12-11 10:44:38 [gpu_model_runner.py:2739] Loading model from scratch...
[1;36m(Worker_TP0 pid=724731)[0;0m INFO 12-11 10:44:38 [gpu_model_runner.py:2739] Loading model from scratch...
[1;36m(Worker_TP1 pid=724732)[0;0m INFO 12-11 10:44:38 [cuda.py:356] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=724731)[0;0m INFO 12-11 10:44:38 [cuda.py:356] Using Flash Attention backend on V1 engine.
[1;36m(Worker_TP0 pid=724731)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/13 [00:00<?, ?it/s]
[1;36m(Worker_TP0 pid=724731)[0;0m Loading safetensors checkpoint shards:   8% Completed | 1/13 [00:08<01:38,  8.19s/it]
[1;36m(Worker_TP0 pid=724731)[0;0m Loading safetensors checkpoint shards:  15% Completed | 2/13 [00:16<01:29,  8.10s/it]
[1;36m(Worker_TP0 pid=724731)[0;0m Loading safetensors checkpoint shards:  23% Completed | 3/13 [00:22<01:13,  7.32s/it]
[1;36m(Worker_TP0 pid=724731)[0;0m Loading safetensors checkpoint shards:  31% Completed | 4/13 [00:30<01:08,  7.58s/it]
[1;36m(Worker_TP0 pid=724731)[0;0m Loading safetensors checkpoint shards:  38% Completed | 5/13 [00:38<01:00,  7.62s/it]
[1;36m(Worker_TP0 pid=724731)[0;0m Loading safetensors checkpoint shards:  46% Completed | 6/13 [00:46<00:54,  7.80s/it]
[1;36m(Worker_TP0 pid=724731)[0;0m Loading safetensors checkpoint shards:  54% Completed | 7/13 [00:54<00:47,  7.92s/it]
[1;36m(Worker_TP0 pid=724731)[0;0m Loading safetensors checkpoint shards:  62% Completed | 8/13 [01:02<00:39,  7.88s/it]
[1;36m(Worker_TP0 pid=724731)[0;0m Loading safetensors checkpoint shards:  69% Completed | 9/13 [01:10<00:31,  7.92s/it]
[1;36m(Worker_TP0 pid=724731)[0;0m Loading safetensors checkpoint shards:  77% Completed | 10/13 [01:18<00:23,  7.90s/it]
[1;36m(Worker_TP0 pid=724731)[0;0m Loading safetensors checkpoint shards:  85% Completed | 11/13 [01:26<00:15,  7.93s/it]
[1;36m(Worker_TP0 pid=724731)[0;0m Loading safetensors checkpoint shards:  92% Completed | 12/13 [01:34<00:08,  8.02s/it]
[1;36m(Worker_TP0 pid=724731)[0;0m Loading safetensors checkpoint shards: 100% Completed | 13/13 [01:38<00:00,  6.66s/it]
[1;36m(Worker_TP0 pid=724731)[0;0m Loading safetensors checkpoint shards: 100% Completed | 13/13 [01:38<00:00,  7.54s/it]
[1;36m(Worker_TP0 pid=724731)[0;0m 
[1;36m(Worker_TP0 pid=724731)[0;0m INFO 12-11 10:46:17 [default_loader.py:267] Loading weights took 98.10 seconds
[1;36m(Worker_TP1 pid=724732)[0;0m INFO 12-11 10:46:17 [default_loader.py:267] Loading weights took 98.11 seconds
[1;36m(Worker_TP1 pid=724732)[0;0m INFO 12-11 10:46:17 [gpu_model_runner.py:2758] Model loading took 29.3109 GiB and 98.998166 seconds
[1;36m(Worker_TP0 pid=724731)[0;0m INFO 12-11 10:46:17 [gpu_model_runner.py:2758] Model loading took 29.3109 GiB and 98.935613 seconds
[1;36m(Worker_TP0 pid=724731)[0;0m INFO 12-11 10:46:17 [gpu_model_runner.py:3447] Encoder cache will be initialized with a budget of 153600 tokens, and profiled with 1 video items of the maximum feature size.
[1;36m(Worker_TP1 pid=724732)[0;0m INFO 12-11 10:46:17 [gpu_model_runner.py:3447] Encoder cache will be initialized with a budget of 153600 tokens, and profiled with 1 video items of the maximum feature size.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
TOKENIZERS_PARALLELISM=(true | false)
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[1;36m(Worker_TP0 pid=724731)[0;0m INFO 12-11 10:46:41 [backends.py:548] Using cache directory: /public_hw/home/cit_shifangzhao/.cache/vllm/torch_compile_cache/2cd63e508c/rank_0_0/backbone for vLLM's torch.compile
[1;36m(Worker_TP1 pid=724732)[0;0m INFO 12-11 10:46:41 [backends.py:548] Using cache directory: /public_hw/home/cit_shifangzhao/.cache/vllm/torch_compile_cache/2cd63e508c/rank_1_0/backbone for vLLM's torch.compile
[1;36m(Worker_TP1 pid=724732)[0;0m INFO 12-11 10:46:41 [backends.py:559] Dynamo bytecode transform time: 14.27 s
[1;36m(Worker_TP0 pid=724731)[0;0m INFO 12-11 10:46:41 [backends.py:559] Dynamo bytecode transform time: 14.26 s
[1;36m(Worker_TP1 pid=724732)[0;0m INFO 12-11 10:46:50 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.662 s
[1;36m(Worker_TP0 pid=724731)[0;0m INFO 12-11 10:46:51 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 9.042 s
[1;36m(Worker_TP0 pid=724731)[0;0m WARNING 12-11 10:46:52 [fused_moe.py:798] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/public_hw/home/cit_shifangzhao/miniconda3/envs/VCA/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_H100_80GB_HBM3.json']
[1;36m(Worker_TP1 pid=724732)[0;0m WARNING 12-11 10:46:52 [fused_moe.py:798] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/public_hw/home/cit_shifangzhao/miniconda3/envs/VCA/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_H100_80GB_HBM3.json']
[1;36m(Worker_TP0 pid=724731)[0;0m INFO 12-11 10:46:52 [monitor.py:32] torch.compile takes 14.26 s in total
[1;36m(Worker_TP1 pid=724732)[0;0m INFO 12-11 10:46:52 [monitor.py:32] torch.compile takes 14.27 s in total
[1;36m(Worker_TP1 pid=724732)[0;0m INFO 12-11 10:46:53 [gpu_worker.py:298] Available KV cache memory: 38.14 GiB
[1;36m(Worker_TP0 pid=724731)[0;0m INFO 12-11 10:46:53 [gpu_worker.py:298] Available KV cache memory: 38.14 GiB
[1;36m(EngineCore_DP0 pid=722733)[0;0m INFO 12-11 10:46:53 [kv_cache_utils.py:1087] GPU KV cache size: 833,216 tokens
[1;36m(EngineCore_DP0 pid=722733)[0;0m INFO 12-11 10:46:53 [kv_cache_utils.py:1091] Maximum concurrency for 65,536 tokens per request: 12.71x
[1;36m(EngineCore_DP0 pid=722733)[0;0m INFO 12-11 10:46:53 [kv_cache_utils.py:1087] GPU KV cache size: 833,216 tokens
[1;36m(EngineCore_DP0 pid=722733)[0;0m INFO 12-11 10:46:53 [kv_cache_utils.py:1091] Maximum concurrency for 65,536 tokens per request: 12.71x
[1;36m(Worker_TP0 pid=724731)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|â–Œ         | 1/19 [00:00<00:04,  4.10it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  11%|â–ˆ         | 2/19 [00:00<00:02,  6.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|â–ˆâ–ˆ        | 4/19 [00:00<00:01,  8.52it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:00<00:01,  9.70it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [00:00<00:01, 10.38it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [00:01<00:00, 10.88it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:01<00:00, 11.23it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [00:01<00:00, 11.46it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [00:01<00:00, 11.56it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:01<00:00, 11.79it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:01<00:00,  9.71it/s]
[1;36m(Worker_TP0 pid=724731)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):   9%|â–‰         | 1/11 [00:00<00:01,  5.44it/s]Capturing CUDA graphs (decode, FULL):  27%|â–ˆâ–ˆâ–‹       | 3/11 [00:00<00:00,  9.44it/s]Capturing CUDA graphs (decode, FULL):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 5/11 [00:00<00:00, 11.26it/s]Capturing CUDA graphs (decode, FULL):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 7/11 [00:00<00:00, 12.19it/s]Capturing CUDA graphs (decode, FULL):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 9/11 [00:00<00:00, 12.79it/s][1;36m(Worker_TP1 pid=724732)[0;0m INFO 12-11 10:46:56 [custom_all_reduce.py:203] Registering 2880 cuda graph addresses
Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 13.19it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 11.97it/s]
[1;36m(Worker_TP0 pid=724731)[0;0m INFO 12-11 10:46:56 [custom_all_reduce.py:203] Registering 2880 cuda graph addresses
[1;36m(Worker_TP0 pid=724731)[0;0m INFO 12-11 10:46:57 [gpu_model_runner.py:3583] Graph capturing finished in 4 secs, took 0.28 GiB
[1;36m(Worker_TP1 pid=724732)[0;0m INFO 12-11 10:46:57 [gpu_model_runner.py:3583] Graph capturing finished in 4 secs, took 0.28 GiB
[1;36m(EngineCore_DP0 pid=722733)[0;0m INFO 12-11 10:46:57 [core.py:211] init engine (profile, create kv cache, warmup model) took 39.71 seconds
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 52076
[1;36m(EngineCore_DP0 pid=722733)[0;0m INFO 12-11 10:47:01 [gc_utils.py:41] GC Debug Config. enabled:False,top_objects:-1
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [api_server.py:1634] Supported_tasks: ['generate']
[1;36m(APIServer pid=714896)[0;0m WARNING 12-11 10:47:01 [model.py:1417] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [serving_responses.py:140] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [serving_responses.py:169] "auto" tool choice has been enabled please note that while the parallel_tool_calls client option is preset for compatibility reasons, it will be ignored.
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [serving_chat.py:99] "auto" tool choice has been enabled please note that while the parallel_tool_calls client option is preset for compatibility reasons, it will be ignored.
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [serving_chat.py:139] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [serving_completion.py:76] Using default completion sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8735
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /docs, Methods: HEAD, GET
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /redoc, Methods: HEAD, GET
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:47:01 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=714896)[0;0m INFO:     Started server process [714896]
[1;36m(APIServer pid=714896)[0;0m INFO:     Waiting for application startup.
[1;36m(APIServer pid=714896)[0;0m INFO:     Application startup complete.
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:51:03 [chat_utils.py:560] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:44306 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:51:12 [loggers.py:127] Engine 000: Avg prompt throughput: 525.8 tokens/s, Avg generation throughput: 133.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 0.0%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:44844 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:44848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:60494 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:51:26 [loggers.py:127] Engine 000: Avg prompt throughput: 476.0 tokens/s, Avg generation throughput: 64.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 17.6%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:60510 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:49366 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:49368 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:51:36 [loggers.py:127] Engine 000: Avg prompt throughput: 2629.9 tokens/s, Avg generation throughput: 128.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 33.9%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:49384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:51146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:51148 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:51:46 [loggers.py:127] Engine 000: Avg prompt throughput: 3291.5 tokens/s, Avg generation throughput: 70.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 43.9%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:51152 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:55926 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:55942 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:51:56 [loggers.py:127] Engine 000: Avg prompt throughput: 2661.2 tokens/s, Avg generation throughput: 138.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 46.2%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:37934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:37950 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:52:06 [loggers.py:127] Engine 000: Avg prompt throughput: 2233.4 tokens/s, Avg generation throughput: 102.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 53.3%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:37960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:50136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:50146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:52:16 [loggers.py:127] Engine 000: Avg prompt throughput: 3002.0 tokens/s, Avg generation throughput: 106.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.2%, Prefix cache hit rate: 52.6%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:50156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:48394 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:48402 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:52:26 [loggers.py:127] Engine 000: Avg prompt throughput: 1856.6 tokens/s, Avg generation throughput: 116.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 56.0%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:48414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:41150 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:41154 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:41158 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:52:37 [loggers.py:127] Engine 000: Avg prompt throughput: 1151.9 tokens/s, Avg generation throughput: 131.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 56.8%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:41162 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:53228 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:53234 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:52:47 [loggers.py:127] Engine 000: Avg prompt throughput: 3750.6 tokens/s, Avg generation throughput: 96.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 57.5%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:53240 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:55934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:55946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:52:57 [loggers.py:127] Engine 000: Avg prompt throughput: 3260.5 tokens/s, Avg generation throughput: 115.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 61.8%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:55954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:38442 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:38452 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:38458 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:53:07 [loggers.py:127] Engine 000: Avg prompt throughput: 3948.2 tokens/s, Avg generation throughput: 112.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 65.9%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:53440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:53444 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:53454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:53:17 [loggers.py:127] Engine 000: Avg prompt throughput: 6616.4 tokens/s, Avg generation throughput: 109.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 71.2%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:53464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:43616 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:43626 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:43642 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:53:27 [loggers.py:127] Engine 000: Avg prompt throughput: 5723.8 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 74.4%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:48232 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:48246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:48250 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:53:37 [loggers.py:127] Engine 000: Avg prompt throughput: 9362.0 tokens/s, Avg generation throughput: 104.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 78.0%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:48254 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:43930 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:43944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:53:47 [loggers.py:127] Engine 000: Avg prompt throughput: 7483.7 tokens/s, Avg generation throughput: 105.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 4.0%, Prefix cache hit rate: 80.3%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:43952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:53:57 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 80.7%
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:54:07 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 80.7%
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:54:17 [loggers.py:127] Engine 000: Avg prompt throughput: 124.6 tokens/s, Avg generation throughput: 77.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 80.5%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:58762 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:38804 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:38818 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:38824 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:54:27 [loggers.py:127] Engine 000: Avg prompt throughput: 1027.6 tokens/s, Avg generation throughput: 141.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 79.9%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:42880 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:54:37 [loggers.py:127] Engine 000: Avg prompt throughput: 1750.3 tokens/s, Avg generation throughput: 105.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 78.5%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:42886 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:42354 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:42370 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:42384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:42390 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:54:47 [loggers.py:127] Engine 000: Avg prompt throughput: 2334.0 tokens/s, Avg generation throughput: 107.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 77.0%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:42134 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:42146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:42150 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:42162 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:42164 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:54:57 [loggers.py:127] Engine 000: Avg prompt throughput: 8318.2 tokens/s, Avg generation throughput: 141.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.4%, Prefix cache hit rate: 77.6%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:54874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:54876 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:54890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:54902 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:55:07 [loggers.py:127] Engine 000: Avg prompt throughput: 7659.0 tokens/s, Avg generation throughput: 139.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 78.8%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:46714 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:46716 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:46732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:55:17 [loggers.py:127] Engine 000: Avg prompt throughput: 10128.3 tokens/s, Avg generation throughput: 133.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 80.4%
[1;36m(APIServer pid=714896)[0;0m INFO:     127.0.0.1:58342 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:55:27 [loggers.py:127] Engine 000: Avg prompt throughput: 8134.3 tokens/s, Avg generation throughput: 89.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 81.5%
[1;36m(APIServer pid=714896)[0;0m INFO 12-11 10:55:37 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 81.5%
