========================================================
Loading environment modules...
Environment loaded.

========================================================
LLM Service Starting on Compute Node
Job ID: 5458
Compute Node Hostname: gpuh02
LLM Listening Port: 8511

--- Instructions for connecting from the LOGIN NODE ---
1. Open a NEW terminal on the login node.
2. Create an SSH tunnel with this command:
   ssh -L 8888:localhost:8511 cit_shifangzhao@gpuh02
   (You can replace 8888 with another unused port on the login node if needed)
3. Once the tunnel is active, you can interact with the LLM at:
   API Base URL: http://localhost:8888/v1
========================================================

WARNING 12-15 15:33:21 [argparse_utils.py:82] argument 'disable_mm_preprocessor_cache' is deprecated
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:33:21 [api_server.py:1772] vLLM API server version 0.12.0
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:33:21 [utils.py:253] non-default args: {'model_tag': '/public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-VL-30B-A3B-Instruct', 'port': 8511, 'enable_auto_tool_choice': True, 'tool_call_parser': 'hermes', 'model': '/public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-VL-30B-A3B-Instruct', 'trust_remote_code': True, 'dtype': 'bfloat16', 'max_model_len': 65536, 'tensor_parallel_size': 2, 'disable_mm_preprocessor_cache': True, 'max_num_seqs': 64}
[0;36m(APIServer pid=2399328)[0;0m WARNING 12-15 15:33:21 [system_utils.py:258] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[0;36m(APIServer pid=2399328)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[0;36m(APIServer pid=2399328)[0;0m WARNING 12-15 15:33:21 [arg_utils.py:1192] `--disable-mm-preprocessor-cache` is deprecated and will be removed in v0.13. Please use `--mm-processor-cache-gb 0` instead.
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:33:21 [model.py:637] Resolved architecture: Qwen3VLMoeForConditionalGeneration
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:33:21 [model.py:1750] Using max model len 65536
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:33:25 [scheduler.py:228] Chunked prefill is enabled with max_num_batched_tokens=8192.
[0;36m(EngineCore_DP0 pid=2406840)[0;0m INFO 12-15 15:33:58 [core.py:93] Initializing a V1 LLM engine (v0.12.0) with config: model='/public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-VL-30B-A3B-Instruct', speculative_config=None, tokenizer='/public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-VL-30B-A3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=65536, download_dir=None, load_format=auto, tensor_parallel_size=2, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01), seed=0, served_model_name=/public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-VL-30B-A3B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': False, 'fuse_act_quant': False, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': 128, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>}, 'local_cache_dir': None}
[0;36m(EngineCore_DP0 pid=2406840)[0;0m WARNING 12-15 15:33:58 [multiproc_executor.py:880] Reducing Torch parallelism from 16 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.
INFO 12-15 15:34:32 [parallel_state.py:1200] world_size=2 rank=0 local_rank=0 distributed_init_method=tcp://127.0.0.1:59373 backend=nccl
INFO 12-15 15:34:32 [parallel_state.py:1200] world_size=2 rank=1 local_rank=1 distributed_init_method=tcp://127.0.0.1:59373 backend=nccl
INFO 12-15 15:34:33 [pynccl.py:111] vLLM is using nccl==2.27.5
INFO 12-15 15:34:35 [parallel_state.py:1408] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0
INFO 12-15 15:34:35 [parallel_state.py:1408] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 1, EP rank 1
[0;36m(Worker_TP0 pid=2410013)[0;0m INFO 12-15 15:34:39 [gpu_model_runner.py:3467] Starting to load model /public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-VL-30B-A3B-Instruct...
[0;36m(Worker_TP1 pid=2410014)[0;0m /public_hw/home/cit_shifangzhao/miniconda3/envs/VCA/lib/python3.12/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(Worker_TP1 pid=2410014)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(Worker_TP1 pid=2410014)[0;0m   warnings.warn(
[0;36m(Worker_TP1 pid=2410014)[0;0m INFO 12-15 15:35:08 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP1 pid=2410014)[0;0m INFO 12-15 15:35:08 [layer.py:379] Enabled separate cuda stream for MoE shared_experts
[0;36m(Worker_TP0 pid=2410013)[0;0m /public_hw/home/cit_shifangzhao/miniconda3/envs/VCA/lib/python3.12/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:174: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[0;36m(Worker_TP0 pid=2410013)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[0;36m(Worker_TP0 pid=2410013)[0;0m   warnings.warn(
[0;36m(Worker_TP0 pid=2410013)[0;0m INFO 12-15 15:35:21 [cuda.py:411] Using FLASH_ATTN attention backend out of potential backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
[0;36m(Worker_TP0 pid=2410013)[0;0m INFO 12-15 15:35:21 [layer.py:379] Enabled separate cuda stream for MoE shared_experts
[0;36m(Worker_TP0 pid=2410013)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/13 [00:00<?, ?it/s]
[0;36m(Worker_TP0 pid=2410013)[0;0m Loading safetensors checkpoint shards:   8% Completed | 1/13 [00:02<00:29,  2.49s/it]
[0;36m(Worker_TP0 pid=2410013)[0;0m Loading safetensors checkpoint shards:  15% Completed | 2/13 [00:04<00:24,  2.21s/it]
[0;36m(Worker_TP0 pid=2410013)[0;0m Loading safetensors checkpoint shards:  23% Completed | 3/13 [00:07<00:27,  2.74s/it]
[0;36m(Worker_TP0 pid=2410013)[0;0m Loading safetensors checkpoint shards:  31% Completed | 4/13 [00:15<00:40,  4.53s/it]
[0;36m(Worker_TP0 pid=2410013)[0;0m Loading safetensors checkpoint shards:  38% Completed | 5/13 [00:21<00:40,  5.12s/it]
[0;36m(Worker_TP0 pid=2410013)[0;0m Loading safetensors checkpoint shards:  46% Completed | 6/13 [00:26<00:36,  5.23s/it]
[0;36m(Worker_TP0 pid=2410013)[0;0m Loading safetensors checkpoint shards:  54% Completed | 7/13 [00:32<00:32,  5.37s/it]
[0;36m(Worker_TP0 pid=2410013)[0;0m Loading safetensors checkpoint shards:  62% Completed | 8/13 [00:38<00:28,  5.74s/it]
[0;36m(Worker_TP0 pid=2410013)[0;0m Loading safetensors checkpoint shards:  69% Completed | 9/13 [00:46<00:25,  6.26s/it]
[0;36m(Worker_TP0 pid=2410013)[0;0m Loading safetensors checkpoint shards:  77% Completed | 10/13 [00:54<00:20,  6.72s/it]
[0;36m(Worker_TP0 pid=2410013)[0;0m Loading safetensors checkpoint shards:  85% Completed | 11/13 [01:01<00:14,  7.01s/it]
[0;36m(Worker_TP0 pid=2410013)[0;0m Loading safetensors checkpoint shards:  92% Completed | 12/13 [01:09<00:07,  7.30s/it]
[0;36m(Worker_TP0 pid=2410013)[0;0m Loading safetensors checkpoint shards: 100% Completed | 13/13 [01:12<00:00,  6.03s/it]
[0;36m(Worker_TP0 pid=2410013)[0;0m Loading safetensors checkpoint shards: 100% Completed | 13/13 [01:12<00:00,  5.60s/it]
[0;36m(Worker_TP0 pid=2410013)[0;0m 
[0;36m(Worker_TP0 pid=2410013)[0;0m INFO 12-15 15:36:35 [default_loader.py:308] Loading weights took 72.94 seconds
[0;36m(Worker_TP0 pid=2410013)[0;0m INFO 12-15 15:36:35 [gpu_model_runner.py:3549] Model loading took 29.3114 GiB memory and 115.656384 seconds
[0;36m(Worker_TP0 pid=2410013)[0;0m INFO 12-15 15:36:36 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 153600 tokens, and profiled with 1 video items of the maximum feature size.
[0;36m(Worker_TP1 pid=2410014)[0;0m INFO 12-15 15:36:36 [gpu_model_runner.py:4306] Encoder cache will be initialized with a budget of 153600 tokens, and profiled with 1 video items of the maximum feature size.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[0;36m(Worker_TP0 pid=2410013)[0;0m INFO 12-15 15:36:58 [backends.py:655] Using cache directory: /public_hw/home/cit_shifangzhao/.cache/vllm/torch_compile_cache/bfc544de91/rank_0_0/backbone for vLLM's torch.compile
[0;36m(Worker_TP0 pid=2410013)[0;0m INFO 12-15 15:36:58 [backends.py:715] Dynamo bytecode transform time: 15.16 s
[0;36m(Worker_TP1 pid=2410014)[0;0m INFO 12-15 15:37:04 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.253 s
[0;36m(Worker_TP0 pid=2410013)[0;0m INFO 12-15 15:37:04 [backends.py:216] Directly load the compiled graph(s) for dynamic shape from the cache, took 5.482 s
[0;36m(Worker_TP1 pid=2410014)[0;0m WARNING 12-15 15:37:06 [fused_moe.py:888] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/public_hw/home/cit_shifangzhao/miniconda3/envs/VCA/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_H100_80GB_HBM3.json']
[0;36m(Worker_TP0 pid=2410013)[0;0m WARNING 12-15 15:37:06 [fused_moe.py:888] Using default MoE config. Performance might be sub-optimal! Config file not found at ['/public_hw/home/cit_shifangzhao/miniconda3/envs/VCA/lib/python3.12/site-packages/vllm/model_executor/layers/fused_moe/configs/E=128,N=384,device_name=NVIDIA_H100_80GB_HBM3.json']
[0;36m(Worker_TP0 pid=2410013)[0;0m INFO 12-15 15:37:07 [monitor.py:34] torch.compile takes 20.64 s in total
[0;36m(Worker_TP0 pid=2410013)[0;0m INFO 12-15 15:37:08 [gpu_worker.py:359] Available KV cache memory: 38.15 GiB
[0;36m(EngineCore_DP0 pid=2406840)[0;0m INFO 12-15 15:37:09 [kv_cache_utils.py:1286] GPU KV cache size: 833,344 tokens
[0;36m(EngineCore_DP0 pid=2406840)[0;0m INFO 12-15 15:37:09 [kv_cache_utils.py:1291] Maximum concurrency for 65,536 tokens per request: 12.72x
[0;36m(Worker_TP1 pid=2410014)[0;0m 2025-12-15 15:37:09,277 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(Worker_TP0 pid=2410013)[0;0m 2025-12-15 15:37:09,277 - INFO - autotuner.py:256 - flashinfer.jit: [Autotuner]: Autotuning process starts ...
[0;36m(Worker_TP1 pid=2410014)[0;0m 2025-12-15 15:37:09,327 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(Worker_TP0 pid=2410013)[0;0m 2025-12-15 15:37:09,329 - INFO - autotuner.py:262 - flashinfer.jit: [Autotuner]: Autotuning process ends
[0;36m(Worker_TP0 pid=2410013)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/19 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   5%|â–Œ         | 1/19 [00:00<00:01,  9.08it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  16%|â–ˆâ–Œ        | 3/19 [00:00<00:01,  8.78it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  21%|â–ˆâ–ˆ        | 4/19 [00:00<00:01,  8.12it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  32%|â–ˆâ–ˆâ–ˆâ–      | 6/19 [00:00<00:01,  9.27it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 8/19 [00:00<00:01, 10.03it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 10/19 [00:01<00:00, 10.60it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 12/19 [00:01<00:00, 10.91it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 14/19 [00:01<00:00, 11.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 16/19 [00:01<00:00, 11.25it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 18/19 [00:01<00:00, 11.47it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19/19 [00:01<00:00, 10.63it/s]
[0;36m(Worker_TP0 pid=2410013)[0;0m Capturing CUDA graphs (decode, FULL):   0%|          | 0/11 [00:00<?, ?it/s]Capturing CUDA graphs (decode, FULL):  18%|â–ˆâ–Š        | 2/11 [00:00<00:00, 13.04it/s]Capturing CUDA graphs (decode, FULL):  36%|â–ˆâ–ˆâ–ˆâ–‹      | 4/11 [00:00<00:00, 13.54it/s]Capturing CUDA graphs (decode, FULL):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 6/11 [00:00<00:00, 13.61it/s][0;36m(Worker_TP1 pid=2410014)[0;0m INFO 12-15 15:37:12 [custom_all_reduce.py:216] Registering 2880 cuda graph addresses
Capturing CUDA graphs (decode, FULL):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 8/11 [00:00<00:00, 13.66it/s]Capturing CUDA graphs (decode, FULL):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 10/11 [00:00<00:00, 13.74it/s]Capturing CUDA graphs (decode, FULL): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 13.62it/s]
[0;36m(Worker_TP0 pid=2410013)[0;0m INFO 12-15 15:37:12 [custom_all_reduce.py:216] Registering 2880 cuda graph addresses
[0;36m(Worker_TP0 pid=2410013)[0;0m INFO 12-15 15:37:13 [gpu_model_runner.py:4466] Graph capturing finished in 4 secs, took -0.22 GiB
[0;36m(EngineCore_DP0 pid=2406840)[0;0m INFO 12-15 15:37:13 [core.py:254] init engine (profile, create kv cache, warmup model) took 37.17 seconds
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [api_server.py:1520] Supported tasks: ['generate']
[0;36m(APIServer pid=2399328)[0;0m WARNING 12-15 15:37:18 [model.py:1576] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [serving_responses.py:194] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [serving_engine.py:293] "auto" tool choice has been enabled.
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [serving_engine.py:293] "auto" tool choice has been enabled.
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [serving_chat.py:133] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [serving_completion.py:73] Using default completion sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [serving_engine.py:293] "auto" tool choice has been enabled.
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [serving_chat.py:133] Using default chat sampling params from model: {'temperature': 0.7, 'top_k': 20, 'top_p': 0.8}
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [api_server.py:1847] Starting vLLM API server 0 on http://0.0.0.0:8511
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:38] Available routes are:
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /openapi.json, Methods: GET, HEAD
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /docs, Methods: GET, HEAD
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /docs/oauth2-redirect, Methods: GET, HEAD
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /redoc, Methods: GET, HEAD
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /health, Methods: GET
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /load, Methods: GET
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /pause, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /resume, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /is_paused, Methods: GET
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /tokenize, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /detokenize, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /v1/models, Methods: GET
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /version, Methods: GET
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /v1/responses, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /v1/responses/{response_id}, Methods: GET
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /v1/responses/{response_id}/cancel, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /v1/messages, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /v1/chat/completions, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /v1/completions, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /v1/audio/transcriptions, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /v1/audio/translations, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /scale_elastic_ep, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /is_scaling_elastic_ep, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /inference/v1/generate, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /ping, Methods: GET
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /ping, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /invocations, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /metrics, Methods: GET
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /classify, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /v1/embeddings, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /score, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /v1/score, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /rerank, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /v1/rerank, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /v2/rerank, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:18 [launcher.py:46] Route: /pooling, Methods: POST
[0;36m(APIServer pid=2399328)[0;0m INFO:     Started server process [2399328]
[0;36m(APIServer pid=2399328)[0;0m INFO:     Waiting for application startup.
[0;36m(APIServer pid=2399328)[0;0m INFO:     Application startup complete.
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:37:26 [chat_utils.py:574] Detected the chat template content format to be 'openai'. You can set `--chat-template-content-format` to override this.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
TOKENIZERS_PARALLELISMTo disable this warning, you can either:
=(true | false)
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[0;36m(EngineCore_DP0 pid=2406840)[0;0m INFO 12-15 15:38:27 [shm_broadcast.py:501] No available shared memory broadcast block found in 60 seconds. This typically happens when some processes are hanging or doing some time-consuming work (e.g. compilation, weight/kv cache quantization).
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:38:29 [loggers.py:236] Engine 000: Avg prompt throughput: 126.9 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:38:39 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 97.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:54330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:38:49 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 17.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:38:59 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:40:09 [loggers.py:236] Engine 000: Avg prompt throughput: 230.0 tokens/s, Avg generation throughput: 94.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 13.9%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34352 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:40:19 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 102.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 13.9%
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 15:40:29 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 13.9%
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 18:27:19 [loggers.py:236] Engine 000: Avg prompt throughput: 134.5 tokens/s, Avg generation throughput: 20.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 20.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50850 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 18:27:29 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 110.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 20.2%
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 18:27:39 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 20.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:53324 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:40:19 [loggers.py:236] Engine 000: Avg prompt throughput: 134.5 tokens/s, Avg generation throughput: 142.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 37.3%
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:40:29 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 37.3%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45030 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45042 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:47:59 [loggers.py:236] Engine 000: Avg prompt throughput: 957.0 tokens/s, Avg generation throughput: 59.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 27.6%
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:48:09 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 27.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45058 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:58258 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:48:19 [loggers.py:236] Engine 000: Avg prompt throughput: 2945.0 tokens/s, Avg generation throughput: 94.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 20.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:58262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:48:29 [loggers.py:236] Engine 000: Avg prompt throughput: 1798.0 tokens/s, Avg generation throughput: 84.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 24.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57916 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:48:39 [loggers.py:236] Engine 000: Avg prompt throughput: 1873.1 tokens/s, Avg generation throughput: 92.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 26.3%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:58414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:58422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49288 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49292 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:48:49 [loggers.py:236] Engine 000: Avg prompt throughput: 2800.7 tokens/s, Avg generation throughput: 82.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 33.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49302 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:58964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:48:59 [loggers.py:236] Engine 000: Avg prompt throughput: 2446.5 tokens/s, Avg generation throughput: 128.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 33.8%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:58972 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46580 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:49:09 [loggers.py:236] Engine 000: Avg prompt throughput: 1837.4 tokens/s, Avg generation throughput: 95.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 32.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46608 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49322 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49346 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49350 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:49:19 [loggers.py:236] Engine 000: Avg prompt throughput: 2938.4 tokens/s, Avg generation throughput: 137.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 38.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49366 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:33542 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:33550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:33566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:49:29 [loggers.py:236] Engine 000: Avg prompt throughput: 2329.4 tokens/s, Avg generation throughput: 116.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 38.4%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:33568 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:49:39 [loggers.py:236] Engine 000: Avg prompt throughput: 1895.7 tokens/s, Avg generation throughput: 94.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 38.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:55358 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:49:49 [loggers.py:236] Engine 000: Avg prompt throughput: 1639.3 tokens/s, Avg generation throughput: 100.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 41.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:55364 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:55372 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45108 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:49:59 [loggers.py:236] Engine 000: Avg prompt throughput: 1076.9 tokens/s, Avg generation throughput: 169.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 41.9%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45114 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:51872 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:51884 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:51900 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:50:10 [loggers.py:236] Engine 000: Avg prompt throughput: 2686.6 tokens/s, Avg generation throughput: 73.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 41.9%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:51908 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37572 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:50:20 [loggers.py:236] Engine 000: Avg prompt throughput: 4290.8 tokens/s, Avg generation throughput: 97.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 41.5%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37590 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:35012 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:35026 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:35032 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:50:30 [loggers.py:236] Engine 000: Avg prompt throughput: 2959.5 tokens/s, Avg generation throughput: 116.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 42.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:35042 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:52758 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:52760 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:52776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:50:40 [loggers.py:236] Engine 000: Avg prompt throughput: 1139.9 tokens/s, Avg generation throughput: 123.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 43.1%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:52782 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57110 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57120 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:50:50 [loggers.py:236] Engine 000: Avg prompt throughput: 2373.1 tokens/s, Avg generation throughput: 133.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 42.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:38500 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:51:00 [loggers.py:236] Engine 000: Avg prompt throughput: 2095.3 tokens/s, Avg generation throughput: 124.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 41.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:38504 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45164 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45174 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:51:10 [loggers.py:236] Engine 000: Avg prompt throughput: 1850.8 tokens/s, Avg generation throughput: 109.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 40.9%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45190 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48408 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:51:20 [loggers.py:236] Engine 000: Avg prompt throughput: 2486.2 tokens/s, Avg generation throughput: 112.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 40.9%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48434 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46064 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46078 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:51:30 [loggers.py:236] Engine 000: Avg prompt throughput: 1475.3 tokens/s, Avg generation throughput: 111.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 42.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46094 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:51:40 [loggers.py:236] Engine 000: Avg prompt throughput: 2941.9 tokens/s, Avg generation throughput: 106.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 43.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44942 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44962 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:51:50 [loggers.py:236] Engine 000: Avg prompt throughput: 1960.9 tokens/s, Avg generation throughput: 132.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 44.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44976 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:36248 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:52:00 [loggers.py:236] Engine 000: Avg prompt throughput: 2176.1 tokens/s, Avg generation throughput: 130.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 43.9%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:36256 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:36270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:53414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:53420 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:53424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:52:10 [loggers.py:236] Engine 000: Avg prompt throughput: 2782.9 tokens/s, Avg generation throughput: 120.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 45.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:53430 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46938 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46956 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:52:20 [loggers.py:236] Engine 000: Avg prompt throughput: 2481.3 tokens/s, Avg generation throughput: 104.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 47.5%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50420 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:52:30 [loggers.py:236] Engine 000: Avg prompt throughput: 2378.3 tokens/s, Avg generation throughput: 99.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.4%, Prefix cache hit rate: 48.9%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50430 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:58952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:58954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:52:40 [loggers.py:236] Engine 000: Avg prompt throughput: 1844.3 tokens/s, Avg generation throughput: 100.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 48.1%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:58958 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:41034 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:52:50 [loggers.py:236] Engine 000: Avg prompt throughput: 1737.2 tokens/s, Avg generation throughput: 101.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 47.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:41042 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:41056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34008 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:53:00 [loggers.py:236] Engine 000: Avg prompt throughput: 2494.4 tokens/s, Avg generation throughput: 104.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.9%, Prefix cache hit rate: 49.3%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34012 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34018 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:41328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:53:10 [loggers.py:236] Engine 000: Avg prompt throughput: 1964.1 tokens/s, Avg generation throughput: 103.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.7%
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 19:53:20 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 50.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49768 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:01:10 [loggers.py:236] Engine 000: Avg prompt throughput: 1841.7 tokens/s, Avg generation throughput: 59.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 51.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49772 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:01:20 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 30.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 51.6%
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:01:30 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 51.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:17:10 [loggers.py:236] Engine 000: Avg prompt throughput: 525.3 tokens/s, Avg generation throughput: 26.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 51.8%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34426 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:51450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:51454 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:17:20 [loggers.py:236] Engine 000: Avg prompt throughput: 2878.1 tokens/s, Avg generation throughput: 96.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 51.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:51464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:33986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:17:30 [loggers.py:236] Engine 000: Avg prompt throughput: 3575.2 tokens/s, Avg generation throughput: 97.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 52.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34002 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37050 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37056 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:17:40 [loggers.py:236] Engine 000: Avg prompt throughput: 2375.0 tokens/s, Avg generation throughput: 105.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 53.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37086 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:38206 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:17:50 [loggers.py:236] Engine 000: Avg prompt throughput: 554.6 tokens/s, Avg generation throughput: 172.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 52.8%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:38218 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60374 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:18:00 [loggers.py:236] Engine 000: Avg prompt throughput: 2208.1 tokens/s, Avg generation throughput: 89.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 53.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:55968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:55978 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:18:10 [loggers.py:236] Engine 000: Avg prompt throughput: 2474.4 tokens/s, Avg generation throughput: 134.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 54.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:55986 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34410 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34422 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34434 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:18:20 [loggers.py:236] Engine 000: Avg prompt throughput: 1927.2 tokens/s, Avg generation throughput: 117.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 54.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34446 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:18:30 [loggers.py:236] Engine 000: Avg prompt throughput: 1656.0 tokens/s, Avg generation throughput: 70.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 53.5%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43288 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57126 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57142 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57146 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57156 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:18:40 [loggers.py:236] Engine 000: Avg prompt throughput: 3096.5 tokens/s, Avg generation throughput: 150.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 54.1%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57168 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:51856 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:51860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:18:50 [loggers.py:236] Engine 000: Avg prompt throughput: 2489.0 tokens/s, Avg generation throughput: 74.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 55.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:51870 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:33950 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:33964 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:19:00 [loggers.py:236] Engine 000: Avg prompt throughput: 3239.7 tokens/s, Avg generation throughput: 103.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 55.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:33974 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50036 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50042 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:19:10 [loggers.py:236] Engine 000: Avg prompt throughput: 2767.4 tokens/s, Avg generation throughput: 98.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 54.9%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50044 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48246 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48248 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48262 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48264 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48270 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:19:20 [loggers.py:236] Engine 000: Avg prompt throughput: 5307.5 tokens/s, Avg generation throughput: 154.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 56.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48274 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60932 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60950 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:19:30 [loggers.py:236] Engine 000: Avg prompt throughput: 11817.1 tokens/s, Avg generation throughput: 142.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.5%, Prefix cache hit rate: 59.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37872 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37874 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37876 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:19:40 [loggers.py:236] Engine 000: Avg prompt throughput: 4128.4 tokens/s, Avg generation throughput: 155.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 60.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37890 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37854 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37872 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:19:50 [loggers.py:236] Engine 000: Avg prompt throughput: 2499.2 tokens/s, Avg generation throughput: 94.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 60.8%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37884 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57686 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:20:00 [loggers.py:236] Engine 000: Avg prompt throughput: 3164.9 tokens/s, Avg generation throughput: 105.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 60.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57692 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:41450 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:41452 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:20:10 [loggers.py:236] Engine 000: Avg prompt throughput: 1843.5 tokens/s, Avg generation throughput: 102.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 60.1%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:41468 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:51424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:20:20 [loggers.py:236] Engine 000: Avg prompt throughput: 902.5 tokens/s, Avg generation throughput: 170.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 60.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:51440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37204 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37214 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:20:30 [loggers.py:236] Engine 000: Avg prompt throughput: 1832.9 tokens/s, Avg generation throughput: 82.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 59.5%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37220 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57400 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57424 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:20:40 [loggers.py:236] Engine 000: Avg prompt throughput: 1165.3 tokens/s, Avg generation throughput: 168.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 59.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57434 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44330 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:20:50 [loggers.py:236] Engine 000: Avg prompt throughput: 2331.3 tokens/s, Avg generation throughput: 120.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 59.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44346 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57038 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:21:00 [loggers.py:236] Engine 000: Avg prompt throughput: 1754.2 tokens/s, Avg generation throughput: 109.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 59.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43632 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43648 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43658 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:21:10 [loggers.py:236] Engine 000: Avg prompt throughput: 2365.7 tokens/s, Avg generation throughput: 106.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 60.1%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43670 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50028 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50042 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:21:20 [loggers.py:236] Engine 000: Avg prompt throughput: 1846.2 tokens/s, Avg generation throughput: 113.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 59.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50048 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49268 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49284 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49292 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:21:30 [loggers.py:236] Engine 000: Avg prompt throughput: 1126.3 tokens/s, Avg generation throughput: 106.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 59.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49308 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:21:40 [loggers.py:236] Engine 000: Avg prompt throughput: 2363.6 tokens/s, Avg generation throughput: 148.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 59.3%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37896 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:21:50 [loggers.py:236] Engine 000: Avg prompt throughput: 1204.0 tokens/s, Avg generation throughput: 59.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 59.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49970 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:22:00 [loggers.py:236] Engine 000: Avg prompt throughput: 2408.0 tokens/s, Avg generation throughput: 54.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 60.3%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:59592 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:22:10 [loggers.py:236] Engine 000: Avg prompt throughput: 1684.2 tokens/s, Avg generation throughput: 99.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 59.9%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:59604 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:33216 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:22:20 [loggers.py:236] Engine 000: Avg prompt throughput: 559.0 tokens/s, Avg generation throughput: 61.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 60.0%
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:22:30 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 60.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49930 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:41:00 [loggers.py:236] Engine 000: Avg prompt throughput: 134.5 tokens/s, Avg generation throughput: 142.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 60.1%
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:41:10 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 60.1%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:55550 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:45:30 [loggers.py:236] Engine 000: Avg prompt throughput: 312.6 tokens/s, Avg generation throughput: 27.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 60.1%
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:45:40 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 60.1%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46328 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:46:00 [loggers.py:236] Engine 000: Avg prompt throughput: 1922.0 tokens/s, Avg generation throughput: 86.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 60.4%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46340 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:58002 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:46:10 [loggers.py:236] Engine 000: Avg prompt throughput: 1823.5 tokens/s, Avg generation throughput: 96.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 60.8%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:58010 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:36470 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:46:20 [loggers.py:236] Engine 000: Avg prompt throughput: 1925.0 tokens/s, Avg generation throughput: 71.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 60.5%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:36480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45280 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:46:30 [loggers.py:236] Engine 000: Avg prompt throughput: 1168.0 tokens/s, Avg generation throughput: 169.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 60.8%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45288 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46250 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46252 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:46:40 [loggers.py:236] Engine 000: Avg prompt throughput: 1835.8 tokens/s, Avg generation throughput: 103.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 61.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46266 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49236 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49238 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:46:50 [loggers.py:236] Engine 000: Avg prompt throughput: 2491.9 tokens/s, Avg generation throughput: 97.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 61.5%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:49240 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37268 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37276 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:47:00 [loggers.py:236] Engine 000: Avg prompt throughput: 3209.8 tokens/s, Avg generation throughput: 92.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 61.3%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37278 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:51326 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:51334 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:47:11 [loggers.py:236] Engine 000: Avg prompt throughput: 2683.1 tokens/s, Avg generation throughput: 106.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 61.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:51338 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:47:21 [loggers.py:236] Engine 000: Avg prompt throughput: 1204.0 tokens/s, Avg generation throughput: 57.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 61.3%
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:47:31 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 61.3%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34788 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:53:41 [loggers.py:236] Engine 000: Avg prompt throughput: 3403.6 tokens/s, Avg generation throughput: 31.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 61.9%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34802 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:35998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:53:51 [loggers.py:236] Engine 000: Avg prompt throughput: 3129.8 tokens/s, Avg generation throughput: 87.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 61.9%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:36010 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:42494 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:42510 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:54:01 [loggers.py:236] Engine 000: Avg prompt throughput: 1835.2 tokens/s, Avg generation throughput: 95.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 62.1%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:42526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46320 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46322 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46324 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:54:11 [loggers.py:236] Engine 000: Avg prompt throughput: 1134.7 tokens/s, Avg generation throughput: 132.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 62.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46332 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60792 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60796 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:54:21 [loggers.py:236] Engine 000: Avg prompt throughput: 2332.7 tokens/s, Avg generation throughput: 133.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.5%, Prefix cache hit rate: 62.5%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60800 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:52362 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:54:31 [loggers.py:236] Engine 000: Avg prompt throughput: 1734.8 tokens/s, Avg generation throughput: 98.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 62.8%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:52378 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44294 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44304 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44320 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:54:41 [loggers.py:236] Engine 000: Avg prompt throughput: 984.9 tokens/s, Avg generation throughput: 158.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 62.9%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44336 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43502 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43514 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:54:51 [loggers.py:236] Engine 000: Avg prompt throughput: 3631.4 tokens/s, Avg generation throughput: 78.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 63.4%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:36982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:36998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37010 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:55:01 [loggers.py:236] Engine 000: Avg prompt throughput: 2670.2 tokens/s, Avg generation throughput: 80.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 63.9%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37020 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40632 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40654 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:55:11 [loggers.py:236] Engine 000: Avg prompt throughput: 3929.9 tokens/s, Avg generation throughput: 109.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 64.5%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40658 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:36414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:36430 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:36440 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:55:21 [loggers.py:236] Engine 000: Avg prompt throughput: 3207.6 tokens/s, Avg generation throughput: 118.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 64.3%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:36448 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60304 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60312 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60316 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:55:31 [loggers.py:236] Engine 000: Avg prompt throughput: 2301.8 tokens/s, Avg generation throughput: 114.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 64.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60320 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:53512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:53516 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:53528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:55:41 [loggers.py:236] Engine 000: Avg prompt throughput: 2374.8 tokens/s, Avg generation throughput: 109.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 64.3%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:33952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:33960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:55:51 [loggers.py:236] Engine 000: Avg prompt throughput: 561.6 tokens/s, Avg generation throughput: 174.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 64.4%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:33970 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:39974 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:39982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:39992 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:56:01 [loggers.py:236] Engine 000: Avg prompt throughput: 2976.8 tokens/s, Avg generation throughput: 105.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 64.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:39998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:38894 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:56:11 [loggers.py:236] Engine 000: Avg prompt throughput: 2057.7 tokens/s, Avg generation throughput: 101.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 64.9%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:38902 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60512 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60522 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60534 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:56:21 [loggers.py:236] Engine 000: Avg prompt throughput: 2406.4 tokens/s, Avg generation throughput: 93.3 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 64.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40622 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:56:31 [loggers.py:236] Engine 000: Avg prompt throughput: 1841.0 tokens/s, Avg generation throughput: 105.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 64.8%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40648 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45400 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45402 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:56:41 [loggers.py:236] Engine 000: Avg prompt throughput: 3314.9 tokens/s, Avg generation throughput: 100.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 65.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34150 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34154 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:56:51 [loggers.py:236] Engine 000: Avg prompt throughput: 3446.8 tokens/s, Avg generation throughput: 104.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.4%, Prefix cache hit rate: 65.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34162 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:56940 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:56952 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:57:01 [loggers.py:236] Engine 000: Avg prompt throughput: 1836.1 tokens/s, Avg generation throughput: 107.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 65.8%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:56954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:36210 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:36214 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:36226 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:57:11 [loggers.py:236] Engine 000: Avg prompt throughput: 2312.8 tokens/s, Avg generation throughput: 86.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 66.1%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:36238 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46100 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46118 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:57:21 [loggers.py:236] Engine 000: Avg prompt throughput: 2492.3 tokens/s, Avg generation throughput: 131.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 66.3%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46134 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:55432 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:57:31 [loggers.py:236] Engine 000: Avg prompt throughput: 2566.7 tokens/s, Avg generation throughput: 116.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.0%, Prefix cache hit rate: 66.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:55442 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:56154 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:56170 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:56182 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:57:42 [loggers.py:236] Engine 000: Avg prompt throughput: 935.6 tokens/s, Avg generation throughput: 138.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 66.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:56196 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:57:52 [loggers.py:236] Engine 000: Avg prompt throughput: 2939.2 tokens/s, Avg generation throughput: 83.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 66.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:46588 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48108 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:58:02 [loggers.py:236] Engine 000: Avg prompt throughput: 1821.7 tokens/s, Avg generation throughput: 77.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 65.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48118 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60486 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60492 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:58:12 [loggers.py:236] Engine 000: Avg prompt throughput: 1358.0 tokens/s, Avg generation throughput: 155.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 65.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50558 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50574 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50580 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:58:23 [loggers.py:236] Engine 000: Avg prompt throughput: 2441.1 tokens/s, Avg generation throughput: 75.5 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 65.5%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:42982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:42998 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:58:33 [loggers.py:236] Engine 000: Avg prompt throughput: 4357.5 tokens/s, Avg generation throughput: 95.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 66.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43002 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:54810 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:54814 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:59944 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:58:43 [loggers.py:236] Engine 000: Avg prompt throughput: 3614.3 tokens/s, Avg generation throughput: 106.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 66.4%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:59946 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:59950 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:58:53 [loggers.py:236] Engine 000: Avg prompt throughput: 2510.9 tokens/s, Avg generation throughput: 62.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 66.8%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:59956 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43876 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43882 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:59:03 [loggers.py:236] Engine 000: Avg prompt throughput: 3782.7 tokens/s, Avg generation throughput: 108.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 67.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44054 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44068 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44080 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:59:13 [loggers.py:236] Engine 000: Avg prompt throughput: 3850.5 tokens/s, Avg generation throughput: 73.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 67.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:56448 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:56464 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:59:23 [loggers.py:236] Engine 000: Avg prompt throughput: 3986.4 tokens/s, Avg generation throughput: 97.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 68.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:56478 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:55886 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:55888 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34616 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:59:33 [loggers.py:236] Engine 000: Avg prompt throughput: 4564.3 tokens/s, Avg generation throughput: 90.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 68.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34624 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34630 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:59:43 [loggers.py:236] Engine 000: Avg prompt throughput: 4190.2 tokens/s, Avg generation throughput: 72.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 69.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34646 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37558 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 20:59:53 [loggers.py:236] Engine 000: Avg prompt throughput: 4971.7 tokens/s, Avg generation throughput: 109.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.3%, Prefix cache hit rate: 69.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:53562 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:53566 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:53576 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:00:03 [loggers.py:236] Engine 000: Avg prompt throughput: 3166.2 tokens/s, Avg generation throughput: 68.3 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 70.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44534 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44540 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:00:13 [loggers.py:236] Engine 000: Avg prompt throughput: 4463.4 tokens/s, Avg generation throughput: 97.2 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 70.5%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44542 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34756 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:34770 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:00:23 [loggers.py:236] Engine 000: Avg prompt throughput: 2379.0 tokens/s, Avg generation throughput: 102.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 70.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:52966 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:52968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:52974 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:00:33 [loggers.py:236] Engine 000: Avg prompt throughput: 2875.1 tokens/s, Avg generation throughput: 101.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.1%, Prefix cache hit rate: 70.8%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48518 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48534 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:00:43 [loggers.py:236] Engine 000: Avg prompt throughput: 3545.2 tokens/s, Avg generation throughput: 101.7 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 71.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37652 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37656 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:00:53 [loggers.py:236] Engine 000: Avg prompt throughput: 557.5 tokens/s, Avg generation throughput: 173.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 71.1%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48696 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48708 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48720 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48722 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:53496 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:01:03 [loggers.py:236] Engine 000: Avg prompt throughput: 3005.7 tokens/s, Avg generation throughput: 97.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 71.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:53502 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:53516 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:53526 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:01:13 [loggers.py:236] Engine 000: Avg prompt throughput: 3177.7 tokens/s, Avg generation throughput: 73.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 71.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:59138 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:59148 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40054 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:01:23 [loggers.py:236] Engine 000: Avg prompt throughput: 3905.5 tokens/s, Avg generation throughput: 111.6 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 71.3%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40066 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40388 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:01:33 [loggers.py:236] Engine 000: Avg prompt throughput: 4545.2 tokens/s, Avg generation throughput: 63.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 2.2%, Prefix cache hit rate: 71.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40398 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40408 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40412 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40414 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40418 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:47568 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:01:43 [loggers.py:236] Engine 000: Avg prompt throughput: 3569.5 tokens/s, Avg generation throughput: 154.9 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 71.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:47570 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:47584 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:47596 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:47602 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:47614 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:41506 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:01:53 [loggers.py:236] Engine 000: Avg prompt throughput: 15822.7 tokens/s, Avg generation throughput: 135.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 3.6%, Prefix cache hit rate: 72.5%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:41516 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:41528 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:41538 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:41554 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:41560 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50732 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:02:03 [loggers.py:236] Engine 000: Avg prompt throughput: 23609.1 tokens/s, Avg generation throughput: 121.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 5.5%, Prefix cache hit rate: 73.9%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50734 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50738 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50746 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50754 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50766 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:02:14 [loggers.py:236] Engine 000: Avg prompt throughput: 10016.1 tokens/s, Avg generation throughput: 91.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 74.5%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43084 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43112 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:02:24 [loggers.py:236] Engine 000: Avg prompt throughput: 3692.6 tokens/s, Avg generation throughput: 83.8 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 74.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43124 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43954 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43966 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57776 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:02:34 [loggers.py:236] Engine 000: Avg prompt throughput: 1502.0 tokens/s, Avg generation throughput: 151.4 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 74.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:57784 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:02:44 [loggers.py:236] Engine 000: Avg prompt throughput: 1760.9 tokens/s, Avg generation throughput: 94.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 74.4%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48070 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48072 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48084 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48098 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:47140 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:02:54 [loggers.py:236] Engine 000: Avg prompt throughput: 4030.8 tokens/s, Avg generation throughput: 83.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 74.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:47152 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:03:04 [loggers.py:236] Engine 000: Avg prompt throughput: 2427.9 tokens/s, Avg generation throughput: 114.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 74.4%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44860 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44876 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44878 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:03:14 [loggers.py:236] Engine 000: Avg prompt throughput: 1845.5 tokens/s, Avg generation throughput: 87.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 74.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:44892 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:58480 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:58482 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:38834 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:38838 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:03:24 [loggers.py:236] Engine 000: Avg prompt throughput: 1536.7 tokens/s, Avg generation throughput: 164.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 74.2%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:38852 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:38862 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:38864 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:03:35 [loggers.py:236] Engine 000: Avg prompt throughput: 2311.0 tokens/s, Avg generation throughput: 63.8 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 74.0%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40194 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:40200 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45960 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:03:45 [loggers.py:236] Engine 000: Avg prompt throughput: 2715.6 tokens/s, Avg generation throughput: 147.5 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.6%, Prefix cache hit rate: 73.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45968 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:45982 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:03:55 [loggers.py:236] Engine 000: Avg prompt throughput: 1742.0 tokens/s, Avg generation throughput: 107.6 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 73.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50110 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50116 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:04:05 [loggers.py:236] Engine 000: Avg prompt throughput: 553.5 tokens/s, Avg generation throughput: 173.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.2%, Prefix cache hit rate: 73.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37906 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37918 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:04:15 [loggers.py:236] Engine 000: Avg prompt throughput: 1639.6 tokens/s, Avg generation throughput: 85.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 73.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:37928 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50400 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50402 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:50416 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:04:25 [loggers.py:236] Engine 000: Avg prompt throughput: 2357.0 tokens/s, Avg generation throughput: 121.4 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 73.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:42630 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:42636 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:42644 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43848 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:04:35 [loggers.py:236] Engine 000: Avg prompt throughput: 1084.1 tokens/s, Avg generation throughput: 123.1 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 73.6%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43854 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:43858 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:04:45 [loggers.py:236] Engine 000: Avg prompt throughput: 1938.4 tokens/s, Avg generation throughput: 156.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.3%, Prefix cache hit rate: 73.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:52924 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:52926 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:52934 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:04:55 [loggers.py:236] Engine 000: Avg prompt throughput: 2197.6 tokens/s, Avg generation throughput: 109.1 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 73.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48272 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48286 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48300 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48312 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:05:05 [loggers.py:236] Engine 000: Avg prompt throughput: 2392.0 tokens/s, Avg generation throughput: 114.0 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.7%, Prefix cache hit rate: 73.8%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:42368 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:42384 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:42392 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:05:15 [loggers.py:236] Engine 000: Avg prompt throughput: 1823.2 tokens/s, Avg generation throughput: 91.9 tokens/s, Running: 1 reqs, Waiting: 0 reqs, GPU KV cache usage: 1.5%, Prefix cache hit rate: 73.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:42394 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:33136 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60832 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60838 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:05:25 [loggers.py:236] Engine 000: Avg prompt throughput: 1163.2 tokens/s, Avg generation throughput: 143.7 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 73.7%
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:60854 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO:     127.0.0.1:48654 - "POST /v1/chat/completions HTTP/1.1" 200 OK
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:05:35 [loggers.py:236] Engine 000: Avg prompt throughput: 1728.3 tokens/s, Avg generation throughput: 89.2 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 73.7%
[0;36m(APIServer pid=2399328)[0;0m INFO 12-15 21:05:45 [loggers.py:236] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 73.7%
slurmstepd: error: *** JOB 5458 ON gpuh02 CANCELLED AT 2025-12-15T23:32:26 DUE TO TIME LIMIT ***
