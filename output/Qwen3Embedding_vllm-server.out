========================================================
Loading environment modules...
Environment loaded.

========================================================
LLM Service Starting on Compute Node
Job ID: 5046
Compute Node Hostname: gpuh12
LLM Listening Port: 8382

--- Instructions for connecting from the LOGIN NODE ---
1. Open a NEW terminal on the login node.
2. Create an SSH tunnel with this command:
   ssh -L 8889:localhost:8382 cit_shifangzhao@gpuh12
   (You can replace 8889 with another unused port on the login node if needed)
3. Once the tunnel is active, you can interact with the LLM at:
   API Base URL: http://localhost:8889/v1
========================================================

INFO 12-11 10:47:49 [__init__.py:216] Automatically detected platform cuda.
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:48:16 [api_server.py:1839] vLLM API server version 0.11.0rc2.dev113+gf9e714813
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:48:16 [utils.py:233] non-default args: {'model_tag': '/public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-Embedding-8B', 'port': 8382, 'model': '/public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-Embedding-8B', 'trust_remote_code': True, 'max_model_len': 8192}
[1;36m(APIServer pid=764232)[0;0m WARNING 12-11 10:48:16 [__init__.py:2877] Found ulimit of 51200 and failed to automatically increase with error current limit exceeds maximum limit. This can cause fd limit errors like `OSError: [Errno 24] Too many open files`. Consider increasing with ulimit -n
[1;36m(APIServer pid=764232)[0;0m The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:48:16 [config.py:739] Found sentence-transformers modules configuration.
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:48:16 [config.py:759] Found pooling configuration.
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:48:16 [model.py:807] Resolved `--runner auto` to `--runner pooling`. Pass the value explicitly to silence this message.
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:48:16 [model.py:859] Resolved `--convert auto` to `--convert embed`. Pass the value explicitly to silence this message.
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:48:16 [model.py:551] Resolved architecture: Qwen3ForCausalLM
[1;36m(APIServer pid=764232)[0;0m `torch_dtype` is deprecated! Use `dtype` instead!
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:48:16 [model.py:1538] Using max model len 8192
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:48:16 [arg_utils.py:1577] (Enabling) chunked prefill by default
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:48:16 [arg_utils.py:1580] (Enabling) prefix caching by default
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:48:19 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-11 10:48:33 [__init__.py:216] Automatically detected platform cuda.
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:48:49 [core.py:648] Waiting for init message from front-end.
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:48:49 [core.py:78] Initializing a V1 LLM engine (v0.11.0rc2.dev113+gf9e714813) with config: model='/public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-Embedding-8B', speculative_config=None, tokenizer='/public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-Embedding-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-Embedding-8B, enable_prefix_caching=True, chunked_prefill_enabled=True, pooler_config=PoolerConfig(pooling_type='LAST', normalize=True, dimensions=None, enable_chunked_processing=None, max_embed_len=None, activation=None, logit_bias=None, softmax=None, step_tag_id=None, returned_token_ids=None), compilation_config={'level': 3, 'debug_dump_path': None, 'cache_dir': '', 'backend': '', 'custom_ops': [], 'splitting_ops': ['vllm.unified_attention', 'vllm.unified_attention_with_output', 'vllm.mamba_mixer2', 'vllm.mamba_mixer', 'vllm.short_conv', 'vllm.linear_attention', 'vllm.plamo2_mamba_mixer', 'vllm.gdn_attention', 'vllm.sparse_attn_indexer'], 'use_inductor': True, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.PIECEWISE: 1>, 'use_cudagraph': True, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [512, 504, 496, 488, 480, 472, 464, 456, 448, 440, 432, 424, 416, 408, 400, 392, 384, 376, 368, 360, 352, 344, 336, 328, 320, 312, 304, 296, 288, 280, 272, 264, 256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], 'cudagraph_copy_inputs': False, 'full_cuda_graph': False, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_capture_size': 512, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=770543)[0;0m /public_hw/home/cit_shifangzhao/miniconda3/envs/VCA/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py:60: UserWarning: Failed to get the IP address, using 0.0.0.0 by default.The value can be set by the environment variable VLLM_HOST_IP or HOST_IP.
[1;36m(EngineCore_DP0 pid=770543)[0;0m   get_ip(), get_open_port())
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:48:53 [parallel_state.py:1208] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=770543)[0;0m WARNING 12-11 10:48:53 [topk_topp_sampler.py:66] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:48:53 [gpu_model_runner.py:2707] Starting to load model /public_hw/home/cit_shifangzhao/zsf/HF/models/Qwen/Qwen3-Embedding-8B...
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:48:53 [gpu_model_runner.py:2739] Loading model from scratch...
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:48:54 [cuda.py:356] Using Flash Attention backend on V1 engine.
[1;36m(EngineCore_DP0 pid=770543)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=770543)[0;0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:04<00:13,  4.43s/it]
[1;36m(EngineCore_DP0 pid=770543)[0;0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:04<00:04,  2.10s/it]
[1;36m(EngineCore_DP0 pid=770543)[0;0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:09<00:03,  3.18s/it]
[1;36m(EngineCore_DP0 pid=770543)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.93s/it]
[1;36m(EngineCore_DP0 pid=770543)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:14<00:00,  3.61s/it]
[1;36m(EngineCore_DP0 pid=770543)[0;0m 
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:49:09 [default_loader.py:267] Loading weights took 14.59 seconds
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:49:09 [gpu_model_runner.py:2758] Model loading took 14.1062 GiB and 15.455005 seconds
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:49:20 [backends.py:548] Using cache directory: /public_hw/home/cit_shifangzhao/.cache/vllm/torch_compile_cache/a4315012b0/rank_0_0/backbone for vLLM's torch.compile
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:49:20 [backends.py:559] Dynamo bytecode transform time: 10.84 s
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:49:28 [backends.py:164] Directly load the compiled graph(s) for dynamic shape from the cache, took 6.711 s
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:49:29 [monitor.py:32] torch.compile takes 10.84 s in total
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:49:30 [gpu_worker.py:298] Available KV cache memory: 56.18 GiB
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:49:30 [kv_cache_utils.py:1087] GPU KV cache size: 409,056 tokens
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:49:30 [kv_cache_utils.py:1091] Maximum concurrency for 8,192 tokens per request: 49.93x
[1;36m(EngineCore_DP0 pid=770543)[0;0m Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   0%|          | 0/67 [00:00<?, ?it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   4%|â–         | 3/67 [00:00<00:02, 24.76it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):   9%|â–‰         | 6/67 [00:00<00:02, 26.77it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  13%|â–ˆâ–Ž        | 9/67 [00:00<00:02, 27.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  18%|â–ˆâ–Š        | 12/67 [00:00<00:01, 27.65it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  22%|â–ˆâ–ˆâ–       | 15/67 [00:00<00:01, 27.78it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  27%|â–ˆâ–ˆâ–‹       | 18/67 [00:00<00:01, 28.21it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  31%|â–ˆâ–ˆâ–ˆâ–      | 21/67 [00:00<00:01, 28.72it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 24/67 [00:00<00:01, 28.98it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 27/67 [00:00<00:01, 28.81it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 30/67 [00:01<00:01, 28.56it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 33/67 [00:01<00:01, 28.66it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 36/67 [00:01<00:01, 28.45it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 39/67 [00:01<00:01, 23.07it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 42/67 [00:01<00:01, 24.20it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 45/67 [00:01<00:00, 24.58it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 48/67 [00:01<00:00, 25.50it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 51/67 [00:01<00:00, 24.54it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 54/67 [00:02<00:00, 24.13it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 57/67 [00:02<00:00, 24.01it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 60/67 [00:02<00:00, 24.15it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 63/67 [00:02<00:00, 24.41it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 66/67 [00:02<00:00, 25.50it/s]Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 67/67 [00:02<00:00, 25.80it/s]
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:49:33 [gpu_model_runner.py:3583] Graph capturing finished in 3 secs, took 0.48 GiB
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:49:33 [core.py:211] init engine (profile, create kv cache, warmup model) took 23.89 seconds
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [loggers.py:147] Engine 000: vllm cache_config_info with initialization after num_gpu_blocks is: 25566
[1;36m(EngineCore_DP0 pid=770543)[0;0m INFO 12-11 10:49:34 [gc_utils.py:41] GC Debug Config. enabled:False,top_objects:-1
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [api_server.py:1634] Supported_tasks: ['embed']
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [api_server.py:1912] Starting vLLM API server 0 on http://0.0.0.0:8382
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:34] Available routes are:
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /openapi.json, Methods: HEAD, GET
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /docs, Methods: HEAD, GET
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /docs/oauth2-redirect, Methods: HEAD, GET
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /redoc, Methods: HEAD, GET
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /health, Methods: GET
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /load, Methods: GET
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /ping, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /ping, Methods: GET
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /tokenize, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /detokenize, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /v1/models, Methods: GET
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /version, Methods: GET
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /v1/responses, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /v1/responses/{response_id}, Methods: GET
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /v1/responses/{response_id}/cancel, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /v1/chat/completions, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /v1/completions, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /v1/embeddings, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /pooling, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /classify, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /score, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /v1/score, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /v1/audio/transcriptions, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /v1/audio/translations, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /rerank, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /v1/rerank, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /v2/rerank, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /scale_elastic_ep, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /is_scaling_elastic_ep, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /invocations, Methods: POST
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:49:34 [launcher.py:42] Route: /metrics, Methods: GET
[1;36m(APIServer pid=764232)[0;0m INFO:     Started server process [764232]
[1;36m(APIServer pid=764232)[0;0m INFO:     Waiting for application startup.
[1;36m(APIServer pid=764232)[0;0m INFO:     Application startup complete.
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:37772 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:37780 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:37784 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:37786 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:37800 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:37810 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:37816 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:37826 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:37830 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:37844 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:51:05 [loggers.py:127] Engine 000: Avg prompt throughput: 17291.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.3%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:39818 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:51:15 [loggers.py:127] Engine 000: Avg prompt throughput: 1.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.3%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:35588 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:51:25 [loggers.py:127] Engine 000: Avg prompt throughput: 2.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.3%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:49230 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:51:35 [loggers.py:127] Engine 000: Avg prompt throughput: 1.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.3%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:48266 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:51:45 [loggers.py:127] Engine 000: Avg prompt throughput: 2.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.3%
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:51:55 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.3%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:39230 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:52:05 [loggers.py:127] Engine 000: Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.3%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:50376 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:52:15 [loggers.py:127] Engine 000: Avg prompt throughput: 3.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:40542 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:52:25 [loggers.py:127] Engine 000: Avg prompt throughput: 0.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:35456 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:52:35 [loggers.py:127] Engine 000: Avg prompt throughput: 3.3 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:36434 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:52:45 [loggers.py:127] Engine 000: Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:48734 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:52:55 [loggers.py:127] Engine 000: Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:48750 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:53:05 [loggers.py:127] Engine 000: Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:35752 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:53:15 [loggers.py:127] Engine 000: Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:37996 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:53:25 [loggers.py:127] Engine 000: Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:48678 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:53:35 [loggers.py:127] Engine 000: Avg prompt throughput: 2.6 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:60830 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:43238 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:53:45 [loggers.py:127] Engine 000: Avg prompt throughput: 5.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:53:55 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.4%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:42182 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:42188 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:42192 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:54:15 [loggers.py:127] Engine 000: Avg prompt throughput: 3880.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.5%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:48950 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:36908 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:54:25 [loggers.py:127] Engine 000: Avg prompt throughput: 5.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.5%
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:54:35 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.5%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:43782 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:54:45 [loggers.py:127] Engine 000: Avg prompt throughput: 3.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.5%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:60010 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:60012 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:60016 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:51906 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:51922 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:54:55 [loggers.py:127] Engine 000: Avg prompt throughput: 25.5 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.6%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:51932 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:51942 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:51662 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:51674 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:55:05 [loggers.py:127] Engine 000: Avg prompt throughput: 36.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.8%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:51686 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:51688 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:37332 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:55:15 [loggers.py:127] Engine 000: Avg prompt throughput: 39.1 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.1%
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:37338 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO:     127.0.0.1:37352 - "POST /v1/embeddings HTTP/1.1" 200 OK
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:55:25 [loggers.py:127] Engine 000: Avg prompt throughput: 32.2 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.3%
[1;36m(APIServer pid=764232)[0;0m INFO 12-11 10:55:35 [loggers.py:127] Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 1.3%
slurmstepd: error: *** JOB 5046 ON gpuh12 CANCELLED AT 2025-12-11T16:22:03 ***
